{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d954e5-3d69-470c-b010-c7c8a4c23343",
   "metadata": {},
   "source": [
    "## 1 算法简介\n",
    "\n",
    "线性回归是⼀种 **「监督学习」(superviesed learning)** 算法，即给定⼀个训练集，去学习⼀个假设函数，⽤来尽量精确地预测每个样本对应的输出。从输出变量的离散程度来看，监督学习算法可以分为两类。线性回归属于回归算法，其输出变量 **「连续」** ；而另⼀类监督学习算法是分类算法，其输出变量离散。\n",
    "\n",
    "线性回归的假设函数为：\n",
    "\n",
    "$$h_{\\theta}(x)=\\sum_{i=1}^n \\theta_i x_i = \\theta^T x$$\n",
    "\n",
    "线性回归的代价函数为：\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{2}\\sum_{i=1}^m (h_{\\theta}(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "$m$个样本，每个样本有$n$个特征。\n",
    "\n",
    "线性回归的学习目标：通过训练集找出使代价函数最小的一组参数$\\theta$（又称最小二乘法LMS algorithm(least mean squares)）.\n",
    "\n",
    "## 2 求解方法\n",
    "\n",
    "对于线性回归代价函数的求解，有两种可选⽅法：**「梯度下降」(gradient descent)** 与 **「正规方程」(normal equations)**。\n",
    "\n",
    "### 2.1 梯度下降\n",
    "\n",
    "梯度下降是⼀种求解最优化问题的迭代⽅法，具体步骤为：\n",
    "\n",
    "1. 随机选取初始的 $\\theta$\n",
    "\n",
    "2. 不断地以梯度的方向修正 $\\theta$\n",
    "\n",
    "3. 最终使$J(\\theta)$收敛至局部最优（在最小二乘中，局部最优即全局最优）\n",
    "\n",
    "$$\\theta_j:= \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)$$\n",
    "\n",
    "$\\alpha$称为学习率(learning rate)，太小会导致收敛缓慢，太大会导致错过最优点，需要谨慎选择。\n",
    "\n",
    "对公式进一步推导（假设只有一个样本点），得到：\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j}J(\\theta)=\\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2}(h_{\\theta}(x)-y)^2$$\n",
    "\n",
    "$$=(h_{\\theta}(x)-y)\\cdot \\frac{\\partial}{\\partial \\theta_j}(\\sum_{i=0}^n \\theta_i x_i - y)$$\n",
    "\n",
    "$$=(h_{\\theta}(x) - y)x_j$$\n",
    "\n",
    "将上述结果代回公式得到：\n",
    "\n",
    "$$\\theta_j:=\\theta_j + \\alpha (y^{(i)}-h_{\\theta}(x^{(i)})) x_j^{(i)}$$\n",
    "\n",
    "注意公式中通过数学变换将减号变成了加号， 方便之后与逻辑回归的结果作比较.\n",
    "\n",
    "#### 2.1.1 分类\n",
    "\n",
    "梯度下降主要可以分为两类：**「批量梯度下降(Batch gradient descent)」** 和 **「随机梯度下降」(Stochastic gradient descent)：\n",
    "\n",
    "* 批量梯度下降：每次计算梯度都需要遍历所有的样本点，当样本量很大时， 计算速度会十分缓慢\n",
    "\n",
    "* 随机梯度下降：每次只考虑⼀个样本点，而不是所有样本点，计算速度会提高， 但是收敛过程会比较曲折， 可能无法精确收敛至最优值\n",
    "\n",
    "随机梯度下降的一种优化形式是 **「⼩批量梯度下降(Mini-batch gradient descent)」**，利用矩阵**并行运算**，一次处理小批量的样本点，有时可以比随机梯度下降速度更快。\n",
    "\n",
    "#### 2.1.2 梯度方向的选择\n",
    "\n",
    "选择梯度⽅向的原因是它是使代价函数减小（下降）最大的方向，我们可以利用柯西不等式对这一结论进行证明：\n",
    "\n",
    "当 $\\theta$ 改变一个很小的量时，利用泰勒公式，忽略一阶导数之后的项，得："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6f227-485b-4d35-8478-93209449997d",
   "metadata": {},
   "source": [
    "$$\\Delta J \\approx \\frac{\\partial J}{\\partial \\theta_0} \\Delta \\theta_0 + \\frac{\\partial J}{\\partial \\theta_1} \\Delta \\theta_1+...+\\frac{\\partial J}{\\partial \\theta_n} \\Delta \\theta_n$$\n",
    "\n",
    "定义如下变量：\n",
    "\n",
    "$$\\Delta \\theta = (\\Delta \\theta_0,\\Delta \\theta_1,...\\Delta \\theta_n)^T$$\n",
    "\n",
    "$$\\triangledown J = (\\frac{\\partial J}{\\partial \\theta_0},\\frac{\\partial J}{\\partial \\theta_1},...,\\frac{\\partial J}{\\partial \\theta_n})^T$$\n",
    "\n",
    "将其代回上式，得:\n",
    "\n",
    "$$\\Delta J \\approx \\triangledown J \\cdot \\Delta \\theta$$\n",
    "\n",
    "根据柯西不等式，有（等号当且仅当 $\\Delta \\theta$ 与$\\triangledown J$ 线性相关时成立）：\n",
    "\n",
    "$$|\\Delta J| \\approx |\\triangledown J \\cdot \\Delta \\theta| \\leq \\parallel \\triangledown J \\parallel \\cdot \\parallel \\Delta \\theta \\parallel $$\n",
    "\n",
    "因此，要使 $\\Delta J$最小，即$|\\Delta J|$ 最大且$\\Delta J < 0$，而当且仅当$\\Delta \\theta = -\\alpha \\triangledown J(\\alpha > 0)$时满足条件，即沿着梯度方向调整$\\theta$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92615d9-aa16-4765-ba80-79e4bd8eb343",
   "metadata": {},
   "source": [
    "### 2.2 正规方程\n",
    "\n",
    "我们可以通过 **「正规方程」** 直接求出 $\\theta$ 的解析解。推导过程如下：\n",
    "\n",
    "#### 2.2.1 矩阵导数\n",
    "\n",
    "对一个将$m \\times n$ 的矩阵映射至实数的函数，定义其导数为：\n",
    "\n",
    "$$\\triangledown_A f(A) = \\left[\n",
    "\\begin{matrix}\n",
    " \\frac{\\partial f}{\\partial A_{11}}   & \\cdots & \\frac{\\partial f}{\\partial A_{1n}}     \\\\\n",
    " \\vdots  & \\ddots & \\vdots \\\\\n",
    " \\frac{\\partial f}{\\partial A_{m1}}    & \\cdots & \\frac{\\partial f}{\\partial A_{mn}}    \\\\\n",
    "\\end{matrix}\n",
    "\\right]$$\n",
    "\n",
    "对一个$n \\times n$方阵，它的 **「迹」** 定义为 **「对角线元素之和」**：\n",
    "\n",
    "$$trA = \\sum_{i=1}^n A_{ii}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d2f2ee-0f4a-4954-bfa3-6ac14c82c647",
   "metadata": {},
   "source": [
    "易证明迹操作具有如下性质（各矩阵为⽅阵）：\n",
    "\n",
    "$$trAB = trBA$$\n",
    "\n",
    "$$trABC = trBCA = trCAB$$\n",
    "\n",
    "$$trABCD = trBCDA = trCDAB = tr DABC$$\n",
    "\n",
    "同样易证明如下性质($a$为实数）：\n",
    "\n",
    "$$trA = trA^T$$\n",
    "\n",
    "$$tr(A+B)=trA + trB$$\n",
    "\n",
    "$$traA = atrA$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a16fb-aba3-4733-87a2-eacdd4ced518",
   "metadata": {},
   "source": [
    "基于以上定义，可以证明一些关于矩阵导数的性质（最后一个等式只针对非奇异矩阵）：\n",
    "\n",
    "$$\\triangledown_A trAB = B^T$$\n",
    "\n",
    "$$\\triangledown_{A^T} f(A) = (\\triangledown_A f(A))^T$$\n",
    "\n",
    "$$\\triangledown_A trABA^T C = CAB + C^T A B^T$$\n",
    "\n",
    "$$\\triangledown_A|A|=|A|(A^{-1})^T$$\n",
    "\n",
    "#### 2.2.2 最小二乘重现\n",
    "\n",
    "对于训练集，可以写成如下的形式：\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
