{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.backends.cudnn.benchmark  \n",
    "\n",
    "大家在训练深度学习模型的时候，经常会使用 `GPU` 来加速网络的训练。但是说起 `torch.backends.cudnn.benchmark` 这个 `GPU` 相关的 `flag`，可能有人会感到比较陌生。在一般场景下，只要简单地在 `PyTorch` 程序开头将其值设置为 `True`，就可以大大提升卷积神经网络的运行速度。既然如此神奇，为什么 `PyTorch` 不将其默认设置为 `True`？它的适用场景是什么？为什么使用它可以提升效率？答案就在本文之中。\n",
    "\n",
    "设置 `torch.backends.cudnn.benchmark=True` 将会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速。适用场景是网络结构固定（不是动态变化的），网络的输入形状（包括 `batch size`，图片大小，输入的通道）是不变的，其实也就是一般情况下都比较适用。反之，如果卷积层的设置一直变化，将会导致程序不停地做优化，反而会耗费更多的时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cuDNN` 是英伟达专门为深度神经网络所开发出来的 `GPU` 加速库，针对卷积、池化等等常见操作做了非常多的底层优化，比一般的 `GPU` 程序要快很多。大多数主流深度学习框架都支持 `cuDNN，PyTorch` 自然也不例外。在使用 `GPU` 的时候，`PyTorch` 会默认使用 `cuDNN` 加速。但是，在使用 `cuDNN` 的时候，`torch.backends.cudnn.benchmark` 模式是为 `False`。所以就意味着，我们的程序可能还可以继续提速！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f95285c7e70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='GeForce RTX 2080 Ti', major=7, minor=5, total_memory=10988MB, multi_processor_count=68)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10988.125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0).total_memory / 1024 ** 2  #bytes to MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  torch.cuda.synchronize() \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one\n",
    "start = time.time()\n",
    "result = model(input)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "result = model(input)\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一共上述两种测试时间的方式，正确的方式是第二种.在`pytorch`里面，程序的执行都是**异步**的。如果采用第一种方式，测试的时间会很短，因为执行完`end=time.time()`程序就退出了，后台的`cu`也因为`python`的退出退出了，如果采用第二种方式，代码会同步`cu`的操作，等待`gpu`上的操作都完成了再继续成形`end = time.time()`  \n",
    "如果将第一段代码改为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "result = model(input)\n",
    "print(result)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候会发现第三段代码和第二段代码的时间是类似的，因为第三段代码会等待`gpu`上的结果执行完传给`print`函数，所以整个时间就和第二段同步的操作的时间基本上是一致的，将`print(result)`换成`result.cpu()`结果是一致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### thop(计算模型flops)  \n",
    "\n",
    "`FLOPS`：注意全大写，是`floating point operations per second`的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。  \n",
    "`FLOPs`：注意s小写，是`floating point operations`的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。网上打字很容易全小写，造成混淆，本问题针对模型，应指的是`FLOPs`。\n",
    "\n",
    "`Paper`里比较流行的单位是`GFLOPs`:`1 GFLOPs = 10^9 FLOPs = 2 * Mac`即：10亿次浮点运算  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thop\n",
      "  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/weiweia92/anaconda3/lib/python3.7/site-packages (from thop) (1.5.0)\n",
      "Requirement already satisfied: future in /home/weiweia92/anaconda3/lib/python3.7/site-packages (from torch>=1.0.0->thop) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/weiweia92/anaconda3/lib/python3.7/site-packages (from torch>=1.0.0->thop) (1.18.2)\n",
      "Installing collected packages: thop\n",
      "Successfully installed thop-0.0.31.post2005241907\n"
     ]
    }
   ],
   "source": [
    "!pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thop\n",
    "class YourModule(nn.Module):   \n",
    "input = torch.randn(1, 3, 224, 224)\n",
    "flops, params = profile(model, inputs=(input, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算FLOPs的代码和包，即傻瓜又好用  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ptflops\n",
      "  Downloading ptflops-0.6.2.tar.gz (10 kB)\n",
      "Requirement already satisfied: torch in /home/weiweia92/anaconda3/lib/python3.7/site-packages (from ptflops) (1.5.0)\n",
      "Requirement already satisfied: future in /home/weiweia92/anaconda3/lib/python3.7/site-packages (from torch->ptflops) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/weiweia92/anaconda3/lib/python3.7/site-packages (from torch->ptflops) (1.18.2)\n",
      "Building wheels for collected packages: ptflops\n",
      "  Building wheel for ptflops (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ptflops: filename=ptflops-0.6.2-py3-none-any.whl size=8529 sha256=787d3f7ba920c095253cd92220915d5a6ffc4909842d83f384be1300d128c89e\n",
      "  Stored in directory: /home/weiweia92/.cache/pip/wheels/3f/ac/b2/05c4fc4f364faad4af4130919c228cca4df1d429bcd365fcc1\n",
      "Successfully built ptflops\n",
      "Installing collected packages: ptflops\n",
      "Successfully installed ptflops-0.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  138.358 M, 100.000% Params, 15.504 GMac, 100.000% MACs, \n",
      "  (features): Sequential(\n",
      "    14.715 M, 10.635% Params, 15.38 GMac, 99.202% MACs, \n",
      "    (0): Conv2d(0.002 M, 0.001% Params, 0.09 GMac, 0.580% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(0.0 M, 0.000% Params, 0.003 GMac, 0.021% MACs, inplace=True)\n",
      "    (2): Conv2d(0.037 M, 0.027% Params, 1.853 GMac, 11.951% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(0.0 M, 0.000% Params, 0.003 GMac, 0.021% MACs, inplace=True)\n",
      "    (4): MaxPool2d(0.0 M, 0.000% Params, 0.003 GMac, 0.021% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(0.074 M, 0.053% Params, 0.926 GMac, 5.976% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(0.0 M, 0.000% Params, 0.002 GMac, 0.010% MACs, inplace=True)\n",
      "    (7): Conv2d(0.148 M, 0.107% Params, 1.851 GMac, 11.941% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(0.0 M, 0.000% Params, 0.002 GMac, 0.010% MACs, inplace=True)\n",
      "    (9): MaxPool2d(0.0 M, 0.000% Params, 0.002 GMac, 0.010% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(0.295 M, 0.213% Params, 0.926 GMac, 5.971% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(0.0 M, 0.000% Params, 0.001 GMac, 0.005% MACs, inplace=True)\n",
      "    (12): Conv2d(0.59 M, 0.426% Params, 1.85 GMac, 11.936% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(0.0 M, 0.000% Params, 0.001 GMac, 0.005% MACs, inplace=True)\n",
      "    (14): Conv2d(0.59 M, 0.426% Params, 1.85 GMac, 11.936% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(0.0 M, 0.000% Params, 0.001 GMac, 0.005% MACs, inplace=True)\n",
      "    (16): MaxPool2d(0.0 M, 0.000% Params, 0.001 GMac, 0.005% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(1.18 M, 0.853% Params, 0.925 GMac, 5.968% MACs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.003% MACs, inplace=True)\n",
      "    (19): Conv2d(2.36 M, 1.706% Params, 1.85 GMac, 11.933% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.003% MACs, inplace=True)\n",
      "    (21): Conv2d(2.36 M, 1.706% Params, 1.85 GMac, 11.933% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.003% MACs, inplace=True)\n",
      "    (23): MaxPool2d(0.0 M, 0.000% Params, 0.0 GMac, 0.003% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(2.36 M, 1.706% Params, 0.463 GMac, 2.983% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs, inplace=True)\n",
      "    (26): Conv2d(2.36 M, 1.706% Params, 0.463 GMac, 2.983% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs, inplace=True)\n",
      "    (28): Conv2d(2.36 M, 1.706% Params, 0.463 GMac, 2.983% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs, inplace=True)\n",
      "    (30): MaxPool2d(0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    123.643 M, 89.365% Params, 0.124 GMac, 0.798% MACs, \n",
      "    (0): Linear(102.765 M, 74.275% Params, 0.103 GMac, 0.663% MACs, in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, inplace=True)\n",
      "    (2): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.5, inplace=False)\n",
      "    (3): Linear(16.781 M, 12.129% Params, 0.017 GMac, 0.108% MACs, in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, inplace=True)\n",
      "    (5): Dropout(0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs, p=0.5, inplace=False)\n",
      "    (6): Linear(4.097 M, 2.961% Params, 0.004 GMac, 0.026% MACs, in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "Flops:15.5 GMac\n",
      "Params: 138.36 M\n"
     ]
    }
   ],
   "source": [
    "net = models.vgg16()\n",
    "flops, params = get_model_complexity_info(net, (3, 224,224), as_strings=True, print_per_layer_stat=True)\n",
    "print(f'Flops:{flops}')\n",
    "print('Params: '+params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
