{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd:自动微分　　\n",
    "`autograd`包是PyTorch中神经网络的核心, 它可以为基于`tensor`的的所有操作提供自动微分的功能, ??这是一个逐个运行的框架, 意味着反向传播是根据你的代码来运行的, 并且每一次的迭代运行都可能不同  \n",
    "\n",
    "### Variable  \n",
    "\n",
    "`autograd.Variable`包裹着`Tensor`, 支持几乎所有`Tensor`的操作,并附加额外的属性, 在进行操作以后, 通过调用`.backward()`来计算梯度, 通过`.data`来访问原始`raw data (tensor)`, 并将变量梯度累加到`.grad`\n",
    "\n",
    "　　`Variable` 与 `Function`互连并建立一个非循环图，编码完整的计算历史。 每个变量都有一个`.grad_fn` 属性，它引用了一个已经创建了 `Variable`的操作,如加减乘除等（除了用户创建的变量代替`creator is None` 即第一个运算节点, `.grad_fn`为空)  \n",
    "\n",
    "### Variable和Tensor  \n",
    "`Tensor`是存在`Variable`中的`.data`里的，而`cpu`和`gpu`的数据是通过 `.cpu()`和`.cuda()`来转换的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "a = Variable(torch.Tensor([1]),requires_grad=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.cpu().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Variable(torch.Tensor([1]),requires_grad=True) \n",
    "b = Variable(torch.Tensor([2]),requires_grad=True)\n",
    "c = Variable(torch.Tensor([3]),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = a + 2*b\n",
    "e = d + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([2.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad,b.grad,c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.grad #中间梯度值不保存，为空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad_fn #第一个节点的.grad_fn为空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7efbe8657a10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从反向传播中排除子图\n",
    "　　每个`Variable`都有两个属性，`requires_grad`和`volatile`, 这两个属性都可以将子图从梯度计算中排除并可以增加运算效率\n",
    "`requires_grad`：排除特定子图，不参与反向传播的计算，即不会累加到`grad`\n",
    "\n",
    "`volatile`: 推理模式, 计算图中只要有一个子图设置为`True`,　所有子图都会被设置不参与反向传播计算，`.backward()`被禁止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Variable(torch.Tensor([1]),requires_grad=False) \n",
    "b=Variable(torch.Tensor([2]),requires_grad=True)\n",
    "c=a+b\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad  # 因为a的requires_grad=False 所以不存储梯度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiweia92/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "a=Variable(torch.Tensor([1]),volatile=True) \n",
    "b=Variable(torch.Tensor([2]),requires_grad=True)\n",
    "c=a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注册钩子：\n",
    "`Variable`中的`hook`, 相当于插件，可以在既不修改主体的情况下，同时增加额外的功能挂在主体代码上，就好比一个人去打猎，他的衣服上有挂枪的扣子，他可以选择他想要带的枪．来取得不同的打猎效果．　\n",
    "    \n",
    "   因此，在`Variable`中通过`register_hook`来实现，`register_hook`的作用是，当反向传播时，你所注册的`hook`都会被调用，比如你可以定义个打印函数，每次反向传播都将`grad值`打印出来．\n",
    "   \n",
    "   需要注意的是，`register_hook`函数接收的是一个函数，这个函数有如下的形式：`hook(grad) -> Variable or None`\n",
    "也就是说，这个函数是拥有改变梯度值的威力的！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#定义一个打印函数，每次反向传播都将相应的grad打印出来\n",
    "def print_grad(grad):\n",
    "    print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn([1]), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.mean(torch.pow(y, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7efbe8663810>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.register_hook(print_grad) #将打印函数挂在变量y上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5940])\n"
     ]
    }
   ],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7efbe8669310>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那一般什么时候我们可以用到注册`hook`呢？有几种常见的情况，比如当我们想要提取中间层参数来进行可视化的时候，或者当我们想要保存中间参数变量，或者我们想要在传播过程中改变梯度值的时候． \n",
    "\n",
    "### 自定义Function \n",
    "\n",
    "在快速实现想法的过程中，创建自定义的 `Operation` 可以让我们灵活的使用 `pytorch`. 基于这个过程，你所自定义的 `Operation` 需要继承 `class autograd.Function` 类来将其添加到 `autograd` , 这样当我们调用`Operation` 时可以使用 `autograd` 来计算结果和梯度，并编码 `operation` 的历史，在定义过程中每个 `operation` 都需要实现三个方法：\n",
    "\n",
    "* `__init__（optional)`: 如果你的 `operation` 包含非 `Variable` 的参数，那么可以将其传入到`init` 并在 `operation` 中使用，如果你的 `operation` 不需要额外的参数，你可以忽略`__init__`\n",
    "* `forward()` : 这里写的是 `operation` 的逻辑代码，可以有任意数量的参数，但参数只能 是`Variable`,返回既可以是 `Variable`, 也可以是 `Variable` 的 `tuple`.\n",
    "* `backward()`:　梯度计算逻辑代码，参数个数和 `forward()` 返回个数一样，每个参数代表传回到此 `Operation` 的梯度，返回值的个数和此 `operation` 输入的个数一样，如果`operation` 不需要返回梯度，可以返回`None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：官方定义一般会用到`@staticmethod` , 同时通过调用`custom_function.apply`来实现,但也可以不加`@staticmethod`,　这样调用要`custom_function()`来实现．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class custom_add(torch.autograd.Function): \n",
    "    \"\"\"\n",
    "    forward and backward both use tensor.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, input2):\n",
    "        \"\"\"\n",
    "        input:tensor,output:tensor,ctx是可以用来在反向传播计算的存储属性的对象\n",
    "        如ctx.save_for_backward可以在backward中使用\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input,input2)\n",
    "        output = input + input2\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        \"\"\"\n",
    "      　backward,我们接受一个存储loss梯度的tensor(grad_output)\n",
    "        同时根据输入来计算应该返回的的梯度\n",
    "     　　\"\"\"\n",
    "        input1, input2=ctx.saved_tensors #在forward中存储的数据 save_for_backward\n",
    "        grad_input=grad_output.clone()\n",
    "        return grad_input,grad_input #由于input是两个输入，所以也返回两个grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_add = custom_add.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Variable(torch.Tensor([1]),requires_grad=True)\n",
    "b=Variable(torch.Tensor([2]),requires_grad=True)\n",
    "c=new_add(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.], grad_fn=<custom_addBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义`function`检查：\n",
    "\n",
    "在你完成`function`后，你可能会想要知道你的逻辑，反向传播是否有写错，你可以通过比较小数值的差分法结果来进行确认，通过`gradcheck`来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradchek takes a tuple of tensor as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "input = (Variable(torch.Tensor([2]).double(), requires_grad=True), Variable(torch.Tensor([3]).double(), requires_grad=True),)\n",
    "test = gradcheck(custom_add.apply, input, eps=1e-6, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析工具：\n",
    "\n",
    "　　如果你想查看你定义操作的时间花销，`autograd` 的 `profiler` 提供内视每个操作在`GPU`和`CPU`的花销，对于`CPU`通过`profile`, 　基于`nvprof`通过使用`emit_nvtx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(1, 1), requires_grad=True)\n",
    "with torch.autograd.profiler.profile() as prof:\n",
    "    y = x ** 2\n",
    "    y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  \n",
      "Name                                 Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls  Input Shapes                         \n",
      "-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  \n",
      "pow                                  20.45%           40.499us         20.45%           40.499us         40.499us         1                []                                   \n",
      "torch::autograd::GraphRoot           1.04%            2.060us          1.04%            2.060us          2.060us          1                []                                   \n",
      "PowBackward0                         38.66%           76.558us         38.66%           76.558us         76.558us         1                []                                   \n",
      "pow                                  20.17%           39.939us         20.17%           39.939us         39.939us         1                []                                   \n",
      "mul                                  9.06%            17.939us         9.06%            17.939us         17.939us         1                []                                   \n",
      "mul                                  1.32%            2.620us          1.32%            2.620us          2.620us          1                []                                   \n",
      "torch::autograd::AccumulateGrad      8.88%            17.579us         8.88%            17.579us         17.579us         1                []                                   \n",
      "detach                               0.43%            0.860us          0.43%            0.860us          0.860us          1                []                                   \n",
      "-----------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------  -----------------------------------  \n",
      "Self CPU time total: 198.054us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
