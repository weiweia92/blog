{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32fcb85a-c05b-434e-a69b-92a056986a07",
   "metadata": {},
   "source": [
    "## Define\n",
    "\n",
    "### Shannon Entropy\n",
    "\n",
    "Given a distributions $p$ over a given variable $X$, it is defined as $$H(p)=\\mathbb E_p[-logp]$$\n",
    "\n",
    "Concretely(具体地), for continuous case, $$H(p)=-\\int_X p(x)logp(x)dx $$\n",
    "\n",
    "and for discrete case, $$H(p)=-\\sum_{x\\in X}p(x)log p(x)$$\n",
    "\n",
    "### Cross Entropy\n",
    "\n",
    "Given two distributions $p$ and $q$ over a given variable $X$, it is defined as $$H(p,q)=\\mathbb E_p[-logq]$$\n",
    "\n",
    "Concretely, for continuous case, $$H(p,q)=-\\int_X p(x)logq(x)dx $$\n",
    "\n",
    "and for discrete case, $$H(p,q)=-\\sum_{x\\in X}p(x)log q(x)$$\n",
    "\n",
    "### Kullback-Leibler Divergence(散度)\n",
    "\n",
    "KL Divergence is also called relative entropy. Given two distributions $p$ and $q$ over a given variable $X$, it is defined as $$D_{KL}(p||q)=\\mathbb E_{p}[-log\\frac{q}{p}]$$\n",
    "\n",
    "Concretely, for continuous case, $$D_{KL}(p||q)=-\\int_X p(x)log\\frac{q(x)}{p(x)}dx$$\n",
    "\n",
    "and for discrete case, $$D_{KL}(p||q)=-\\sum_{x \\in X}p(x)log\\frac{q(x)}{p(x)}$$\n",
    "\n",
    "### Maximum Likelihood Estimation\n",
    "\n",
    "For unsupervised learning, given a dataset ${x_1, x_2, ...,x_n}$, we want to train a model with parameters $\\theta$ so that the product of the likelihood for all the samples in the dataset is maximized.\n",
    "\n",
    "We use $q_{\\theta}(x_i)$ to denote the predicted likelihood $q(x_i|\\theta)$ from the model for sample $x_i$ from the dataset. Concretely, we have the follow objective function $$argmax_{\\theta}\\prod_{i=1}^n q_{\\theta}(x_i)$$\n",
    "\n",
    "It is equivalent to optimize $$argmax_{\\theta}\\sum_{i=1}^n log q_{\\theta}(x_i)$$ or $$argmin_{\\theta}-\\sum_{i=1}^n log q_{\\theta}(x_i)$$\n",
    "\n",
    "Similarly, for supervised learning, given a dataset ${(x_1, y_1),(x_2, y_2),...,(x_n,y_n)}$, we want to optimize $$argmax_{\\theta}\\prod_{i=1}^n q_{\\theta}(y_i|x_i)$$\n",
    "\n",
    "It is equivalent to optimize $$argmax_{\\theta}\\sum_{i=1}^n log q_{\\theta}(y_i|x_i)$$ or $$argmin_{\\theta}-\\sum_{i=1}^n log q_{\\theta}(y_i|x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a4435-1cc6-4a44-9087-5e238ce45f14",
   "metadata": {},
   "source": [
    "## Relationships\n",
    "\n",
    "### Maximum Likelihood Estimation and Cross Entropy\n",
    "\n",
    "In classification problems, we set $p$ as the distribution for the ground truth label for feature $x$, and $q_{\\theta}$ as the distribution for the predicted label for feature $x$ from the model. The ground truth distribution $p(y|x_i)$ would be a one-hot encoded vector where     \n",
    "$$p(y|x_i)=\n",
    "\\begin{cases}\n",
    "1& \\text{if y=y_i}\\\\\n",
    "0& \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10226d14-50f5-486e-a449-bc2d4152bf5f",
   "metadata": {},
   "source": [
    "For sample $(x_i, y_i)$ from the dataset, the cross entropy of the ground truth distribution and the predicted label distribution is  \n",
    "\n",
    "$$H_i(p,q_{\\theta})=-\\sum_{y\\in Y}p(y|x_i)logq_{\\theta}(y|x_i)=-logq_{\\theta}(y_i|x_i)$$\n",
    "\n",
    "We use the sum of the cross entropy for all our samples from the dataset and use it as the loss function to train our model on the dataset  \n",
    "\n",
    "$$L=\\sum_{i=1}^n H_i(p,q_{\\theta})=-\\sum_{i=1}^n log q_{\\theta}(y_i|x_i)$$\n",
    "\n",
    "This loss function is sometimes called a log loss function. Thus the optimization goal is\n",
    "\n",
    "$$argmin_{\\theta} L = argmin_{\\theta} \\sum_{i=1}^n H_i(p, q_{\\theta})=argmin_{\\theta}-\\sum_{i=1}^n logq_{\\theta}(y_i|x_i)$$\n",
    "\n",
    "This is exactly the same as the optimization goal of maximum likelihood estimation. Therefore, we say optimization using log loss in the classification problems is equivalent to do maximum likelihood estimation.\n",
    "\n",
    "### Cross Entropy and KL Divergence\n",
    "\n",
    "It is not hard to derive the relationship between cross entropy and KL divergence.\n",
    "\n",
    "$$D_{KL}(p||q)=\\mathbb E_p[-log\\frac{q}{p}]=\\mathbb E_p[-logq+logp]=\\mathbb E_p[-logq]+ \\mathbb E_p[logp]$$\n",
    "$$=\\mathbb E_p[-log q]-\\mathbb E_p[-log p]=H(p,q)-H(p)$$\n",
    "\n",
    "### Optimization Using Cross Entropy or KL Divergence\n",
    "\n",
    "From the relationship between cross entropy and KL divergence, we know that\n",
    "\n",
    "$$H(p,q)-D_{KL}(p||q)+H(p)$$\n",
    "\n",
    "We could then rewrite our log loss\n",
    "\n",
    "$$L = \\sum_{i=1}^n H_i(p,q_{\\theta})=\\sum_{i=1}^n[D_{KL,i}(p||q_{\\theta})+H_i(p)]$$\n",
    "\n",
    "The optimization goal then becomes\n",
    "\n",
    "$$argmin_{\\theta}L = argmin_{\\theta} \\sum_{i=1}^n H_i(p,q_{\\theta}) = argmin_{\\theta}\\sum_{i=1}^n[D_{KL,i}(p||q_{\\theta})+H_i(p)]$$\n",
    "\n",
    "Because $H_i(p)$ is independent of $\\theta$\n",
    "\n",
    "$$argmin_{\\theta}L = argmin_{\\theta} \\sum_{i=1}^n H_i(p,q_{\\theta}) = argmin_{\\theta}\\sum_{i=1}^n[D_{KL,i}(p||q_{\\theta})$$\n",
    "\n",
    "Therefore, in classification problems, optimization using the sum of cross entropy over all the training samples is equivalent to optimization using the sum of KL divergence over all the training samples.\n",
    "\n",
    "We use cross entropy in practice because it is relatively easy to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf67a8a-bf9f-4b6b-ab5e-0a645b8f9d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
