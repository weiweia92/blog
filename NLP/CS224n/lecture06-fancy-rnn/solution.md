![](https://github.com/weiweia92/blog/blob/main/NLP/pic/Screen%20Shot%202021-07-20%20at%206.00.19%20PM.png)

`(g)` 1. Masks prevent(阻止) the decoder to output(输出 v.) <pad> padding tokens which are present in nearly every batch. Padding tokens are an artifact of the speed-up that comes from processing the data in batches (padding是为了可以批量处理,批量处理是为了加速), it's undesirable to output them because prediction ends with a special termination token. Pre-softmax value of negative infinity sets their probability to zero.         
&emsp;&emsp;&nbsp;&nbsp;2.Attention score/distributions calculates the attention probability of the target word at a certain time step in the decoder to all the source words in the encoder, while the pad token is only used for mini-batch and has no linguistic meaning. The target word does not need to be distracted(分心) attention, so you need to use masks to filter out(过滤) the pad token.
 
`(j)` Advantage:dot product attention doesn't contain any learnable parameters, therefore, it is the simplest and computationally least expensive variant.     
&emsp;&emsp;&nbsp;&nbsp;Disadvantage: it's not very expression(表现力) and only measures the degree of alignment of the decoder and encoder hidden states. Therefore force the decoder and encoder having very similar embedding spaces. It can be viewed as a form of regularization that may hurt performance for certain language pairs that are very dissimilar.       
  
&emsp;&emsp;&nbsp;&nbsp;Multiplicative attention is more expressive and allows the encoder and decoder to develop linearly dependent(线性相关) word vectore representations. Since the weight matrix doesn't have to be square(正方形) and embedding spaces can have different dimensions. Faster and more memory-saving than additive attention but there's additional computational cost compared to the dot product variant.     
  
&emsp;&emsp;&nbsp;&nbsp;Additive attention gives the decoder and encoder the most freedom to develop independent embedding spaces. The mapping has more degrees of freedom and allows for an affine(仿射) non-linear mapping between the encoder and decoder space. Additive attention requires the most computational operations of the three options. Computational cost increase in this case may be substantial(重大的) because the number of attention operations grows approximately as ![](https://latex.codecogs.com/png.image?\dpi{110}%20O(n^2)). n is the mean input and output sequence length.
