{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c335ec07-5308-49a6-980d-abb14395984a",
   "metadata": {},
   "source": [
    "## embedding\n",
    "\n",
    "词嵌入，通俗来讲就是将文字转换为一串数字。因为数字是计算机更容易识别的一种表达方式。我们词嵌入的过程相当于我们在给计算机制造出一本字典的过程，计算机可以通过这个字典来间接地识别文字。\n",
    "\n",
    "词嵌入向量的意思可以理解为：词在神经网络中的向量表示。\n",
    "\n",
    "### embedding层的理解\n",
    "\n",
    "首先，我们有一个one-hot编码的概念。假设，我们中文，一共只有10个字...只是假设啊，那么我们用0-9就可以表示完     \n",
    "比如，这十个字就是“我从哪里来，要到何处去”其分别对应“0-9”，如下：\n",
    "\n",
    "我  从  哪  里  来  要  到  何  处  去       \n",
    "0    1    2    3   4    5   6    7    8   9\n",
    "\n",
    "那么，其实我们只用一个列表就能表示所有的对话     \n",
    "如：我  从  哪  里  来  要  到  何  处  去  ——>>>[0 1 2 3 4 5 6 7 8 9]     \n",
    "或：我  从  何  处  来  要  到  哪  里  去  ——>>>[0 1 7 8 4 5 6 2 3 9]     \n",
    "\n",
    "但是，我们看看one-hot编码方式\n",
    "\n",
    "```\n",
    "# 我从哪里来，要到何处去\n",
    "[\n",
    "[1 0 0 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 0]\n",
    "[0 0 1 0 0 0 0 0 0 0]\n",
    "[0 0 0 1 0 0 0 0 0 0]\n",
    "[0 0 0 0 1 0 0 0 0 0]\n",
    "[0 0 0 0 0 1 0 0 0 0]\n",
    "[0 0 0 0 0 0 1 0 0 0]\n",
    "[0 0 0 0 0 0 0 1 0 0]\n",
    "[0 0 0 0 0 0 0 0 1 0]\n",
    "[0 0 0 0 0 0 0 0 0 1]\n",
    "]\n",
    " \n",
    "# 我从何处来，要到哪里去\n",
    "[\n",
    "[1 0 0 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 0]\n",
    "[0 0 0 0 0 0 0 1 0 0]\n",
    "[0 0 0 0 0 0 0 0 1 0]\n",
    "[0 0 0 0 1 0 0 0 0 0]\n",
    "[0 0 0 0 0 1 0 0 0 0]\n",
    "[0 0 0 0 0 0 1 0 0 0]\n",
    "[0 0 1 0 0 0 0 0 0 0]\n",
    "[0 0 0 1 0 0 0 0 0 0]\n",
    "[0 0 0 0 0 0 0 0 0 1]\n",
    "]\n",
    "```\n",
    "这样就把每一系列的文本整合成一个稀疏矩阵。那问题来了，稀疏矩阵（二维）和列表（一维）相比，有什么优势?\n",
    "\n",
    "很明显，计算简单嘛，稀疏矩阵做矩阵计算的时候，只需要把1对应位置的数相乘求和就行，也许你心算都能算出来；而一维列表，你能很快算出来？何况这个列表还是一行，如果是100行、1000行和或1000列呢？所以，one-hot编码的优势就体现出来了，计算方便快捷、表达能力强。然而，缺点也随着来了。比如：中文大大小小简体繁体常用不常用有十几万，然后一篇文章100W字，你要表示成100W X 10W的矩阵？？？这是它最明显的缺点。过于稀疏时，过度占用资源。\n",
    "\n",
    "比如：其实我们这篇文章，虽然100W字，但是其实我们整合起来，有99W字是重复的，只有1W字是完全不重复的。那我们用100W X 10W的岂不是白白浪费了99W X 10W的矩阵存储空间。那怎么办？？？\n",
    "\n",
    "这时，Embedding层横空出世。\n",
    "\n",
    "假设我们有一个2\\*6矩阵，然后乘上一个6\\*3矩阵后，变为2\\*3矩阵。先不管它什么意思，这个过程，我们把一个12个元素的矩阵变成6个元素的矩阵，直观上，大小是不是缩小了一半？\n",
    "也许你已经想到了！！！对！！！不管你想的对不对，但是embedding层，在某种程度上，就是用来降维的，降维的原理就是矩阵乘法。在卷积网络中，可以理解为特殊全连接层操作，跟1x1卷积核异曲同工！！也就是说，假如我们有一个100W X10W的矩阵，用它乘上一个10W X 20的矩阵，我们可以把它降到100W X 20，瞬间量级降了。。。10W/20=5000倍！！！\n",
    "\n",
    "这就是(embedding layer)嵌入层的一个作用——**降维**。\n",
    "\n",
    "然后中间那个10W X 20的矩阵，可以理解为查询表，也可以理解为映射表，也可以理解为过度表，whatever。接着，既然可以降维，当然也可以升维。为什么要升维？\n",
    "\n",
    "低维的数据可能包含的特征是非常笼统的，我们需要不停地拉近拉远来改变我们的感受野\n",
    "\n",
    "embedding的又一个作用体现了。对低维的数据进行升维时，可能把一些其他特征给放大了，或者把笼统的特征给分开了。同时，这个embedding是一直在学习在优化的，就使得整个拉近拉远的过程慢慢形成一个良好的观察点。\n",
    "\n",
    "回想一下为什么CNN层数越深准确率越高，卷积层卷了又卷，池化层池了又升，升了又降，全连接层连了又连。因为我们也不知道它什么时候突然就学到了某个有用特征。但是不管怎样，学习都是好事，所以让机器多卷一卷，多连一连，反正错了多少我会用交叉熵告诉你，怎么做才是对的我会用梯度下降算法告诉你，只要给你时间，你迟早会学懂。因此，理论上，只要层数深，只要参数足够，NN能拟合任何特征。总之，它类似于虚拟出一个关系对当前数据进行映射。这个东西也许一言难尽吧，但是目前各位只需要知道它有这些功能的就行了。\n",
    "\n",
    "接下来，继续假设我们有一句话，叫“公主很漂亮”，如果我们使用one-hot编码，可能得到的编码如下：\n",
    "```\n",
    "公 [0 0 0 0 1]\n",
    "主 [0 0 0 1 0]\n",
    "很 [0 0 1 0 0]\n",
    "漂 [0 1 0 0 0]\n",
    "亮 [1 0 0 0 0]\n",
    "```\n",
    "者假设咱们的词袋更大一些\n",
    "```\n",
    "公 [0 0 0 0 1 0 0 0 0 0]\n",
    "主 [0 0 0 1 0 0 0 0 0 0]\n",
    "很 [0 0 1 0 0 0 0 0 0 0]\n",
    "漂 [0 1 0 0 0 0 0 0 0 0]\n",
    "亮 [1 0 0 0 0 0 0 0 0 0]\n",
    "```\n",
    "这样的编码，最大的好处就是，不管你是什么字，我们都能在一个一维的数组里用01给你表示出来。并且不同的字绝对不一样，以致于一点重复都没有，表达本征的能力极强。但是，因为其完全独立，其劣势就出来了。无法表示其相关性。\n",
    "\n",
    "我给你举个例子，我们又有一句话“王妃很漂亮”那么在这基础上，我们可以把这句话表示为\n",
    "```\n",
    "王 [0 0 0 0 0 0 0 0 0 1]\n",
    "妃 [0 0 0 0 0 0 0 0 1 0]\n",
    "很 [0 0 1 0 0 0 0 0 0 0]\n",
    "漂 [0 1 0 0 0 0 0 0 0 0]\n",
    "亮 [1 0 0 0 0 0 0 0 0 0]\n",
    "```\n",
    "从中文表示来看，我们一下就跟感觉到，王妃跟公主其实是有很大关系的，比如：公主是皇帝的女儿，王妃是皇帝的妃子，可以从“皇帝”这个词进行关联上；公主住在宫里，王妃住在宫里，可以从“宫里”这个词关联上；公主是女的，王妃也是女的，可以从“女”这个字关联上。\n",
    "\n",
    "但是呢，我们用了one-hot编码，公主和王妃就变成了这样：\n",
    "```\n",
    "公 [0 0 0 0 1 0 0 0 0 0]\n",
    "主 [0 0 0 1 0 0 0 0 0 0]\n",
    "王 [0 0 0 0 0 0 0 0 0 1]\n",
    "妃 [0 0 0 0 0 0 0 0 1 0]\n",
    "```\n",
    "你说，你要是不看前面的中文注解，你知道这四行向量有什么内部关系吗？看不出来，那怎么办？既然，通过刚才的假设关联，我们关联出了“皇帝”、“宫里”和“女”三个词，那我们尝试这么去定义公主和王妃\n",
    "\n",
    "公主一定是皇帝的女儿，我们假设她跟皇帝的关系相似度为1.0；公主从一出生就住在宫里，直到20岁才嫁到府上，活了80岁，我们假设她跟宫里的关系相似度为0.25；公主一定是女的，跟女的关系相似度为1.0；\n",
    "\n",
    "王妃是皇帝的妃子，没有亲缘关系，但是有存在着某种关系，我们就假设她跟皇帝的关系相似度为0.6吧；妃子从20岁就住在宫里，活了80岁，我们假设她跟宫里的关系相似度为0.75；王妃一定是女的，跟女的关系相似度为1.0；\n",
    "\n",
    "于是公主王妃四个字我们可以这么表示：\n",
    "```\n",
    "皇 宫\n",
    "帝 里 女\n",
    "\n",
    "公主 [ 1.0 0.25 1.0]\n",
    "王妃 [ 0.6 0.75 1.0]\n",
    "```\n",
    "这样我们就把公主和王妃两个词，跟皇帝、宫里、女这几个字（特征）关联起来了，我们可以认为：\n",
    "\n",
    "公主=1.0 \\*皇帝 +0.25\\*宫里 +1.0\\*女    \n",
    "王妃=0.6 \\*皇帝 +0.75\\*宫里 +1.0\\*女     \n",
    "\n",
    "或者这样，我们假设每个词的每个字都是对等（注意：只是假设，为了方便解释）：\n",
    "```\n",
    "皇 宫\n",
    "帝 里 女\n",
    "公 [ 0.5 0.125 0.5]\n",
    "主 [ 0.5 0.125 0.5]\n",
    "王 [ 0.3 0.375 0.5]\n",
    "妃 [ 0.3 0.375 0.5]\n",
    "```\n",
    "这样，我们就把一些词甚至一个字，用三个特征给表征出来了。然后，我们把`皇帝`叫做特征（1），`宫里`叫做特征（2），`女`叫做特征（3），于是乎，我们就得出了公主和王妃的隐含特征关系：\n",
    "\n",
    "王妃=公主的特征（1） * 0.6 +公主的特征（2） * 3 +公主的特征（3） * 1\n",
    "\n",
    "于是乎，我们把文字的one-hot编码，从稀疏态变成了密集态(dense)，并且让相互独立向量变成了有内在联系的关系向量。\n",
    "\n",
    "所以，embedding层做了个什么呢？它把我们的稀疏矩阵，通过一些线性变换（在CNN中用全连接层进行转换，也称为查表操作），变成了一个密集矩阵，这个密集矩阵用了N（例子中N=3）个特征来表征所有的文字，在这个密集矩阵中，表象上代表着密集矩阵跟单个字的一一对应关系，实际上还蕴含了大量的字与字之间，词与词之间甚至句子与句子之间的内在关系（如：我们得出的王妃跟公主的关系）。他们之间的关系，用的是嵌入层学习来的参数进行表征。从稀疏矩阵到密集矩阵的过程，叫做embedding，很多人也把它叫做查表，因为他们之间也是一个一一映射的关系。\n",
    "\n",
    "更重要的是，这种关系在反向传播的过程中，是一直在更新的，因此能在多次epoch后，使得这个关系变的相对成熟，即：正确的表达整个语义以及各个语句之间的关系。这个成熟的关系，就是embedding层的所有权重参数。\n",
    "\n",
    "Embedding是NPL领域最重要的发明之一，他把独立的向量关联起来了。\n",
    "\n",
    "### PyTorch中的embedding\n",
    "\n",
    "输入是一个索引列表，输出是相应的词嵌入    \n",
    "```\n",
    "torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, \n",
    "                   norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)\n",
    "```\n",
    "**参数**\n",
    "* num_embeddings(int):词典的大小尺寸，比如总共出现5000个词，那就输入5000。此时index为（0-4999）\n",
    "\n",
    "* embedding_dim(int):嵌入向量的维度，即用多少维来表示一个符号。\n",
    "\n",
    "* padding_idx(int, optional):填充id.比如，输入长度为100，但是每次的句子长度并不一样，后面就需要用统一的数字填充，而这里就是指定这个数字，这样，网络在遇到填充id时，就不会计算其与其它符号的相关性。（初始化为0）\n",
    "\n",
    "* max_norm(float, optional):最大范数，如果嵌入向量的范数超过了这个界限，就要进行再归一化。\n",
    "\n",
    "* norm_type(float, optional):指定利用什么范数计算，并用于对比max_norm，默认为2范数。\n",
    "\n",
    "* scale_grad_by_freq(boolean, optional):根据单词在mini-batch中出现的频率，对梯度进行放缩。默认为False.\n",
    "\n",
    "* sparse(boolean, optional):若为True,则与权重矩阵相关的梯度转变为稀疏张量，默认为False。\n",
    "\n",
    "### torch.nn.Embedding在模型中的使用\n",
    "\n",
    "```\n",
    "embed = torch.nn.Embedding(n_vocabulary, embedding_size)\n",
    "```\n",
    "作为训练的一层，随模型训练得到适合的词向量，然后将其放进网络，那么词向量的输入应该是什么样子呢？\n",
    "\n",
    "首先我们知道肯定先要建立一个词典，建立词典的时候都会建立一个dict：word2id：存储单词到词典序号的映射。假设一个mini-batch如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53e9d5ab-146f-45f4-9d95-7a36123c4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "mini_batch = ['I am a boy.','How are you?','I am very lucky.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ecb9af-be1d-4fab-9432-e72773ec9743",
   "metadata": {},
   "source": [
    "mini_batch有三个句子，即batch_size = 3\n",
    "\n",
    "第一步首先要做的是：将句子标准化，所谓标准化，指的是：大写转小写，标点分离这部分很简单就略过。经处理后，mini-batch变为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1370ccb4-6886-42b9-a7b5-5d613e5d314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = [['i','am','a','boy','.'],['how','are','you','?'],['i','am','very','lucky','.']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7daf58-1583-463d-ad81-c8e34b86b6b7",
   "metadata": {},
   "source": [
    "还要做一步：将上面的三个list按单词数从多到少排列。标点也算单词。至于为什么，后面会说到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2444e58-7e5b-47c4-9e76-f1fc46c0605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = [['i','am','a','boy','.'],['i','am','very','lucky','.'],['how','are','you','?']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b2a2b2-713f-48a7-b167-fadf07c194a9",
   "metadata": {},
   "source": [
    "可见，每个句子的长度，即每个内层list的元素数为：5,5,4。这个长度也要记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a13c0c88-e25e-4fa7-932b-5123a82f16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [5,5,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b1fb46-8116-4df6-acb4-f319ae9c7b62",
   "metadata": {},
   "source": [
    "之后，为了能够处理，将mini_batch的单词表示转为在词典中的index序号，这就是word2id的作用。转换过程很简单，假设转换之后的结果如下所示，当然这些序号是我编的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f35a2baf-e502-4cb4-b253-bd1ead99e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = [[3,6,5,6,7],[6,4,7,9,5],[4,5,8,7]]\n",
    "\n",
    "#每个句子结尾要加EOS，假设EOS在词典中的index是1。\n",
    "mini_batch = [[3,6,5,6,7,1],[6,4,7,9,5,1],[4,5,8,7,1]]\n",
    "\n",
    "#长度更新\n",
    "lens = [6, 6, 5]\n",
    "\n",
    "#PAD过后，padding_idx=2\n",
    "mini_batch = [[3,6,5,6,7,1],[6,4,7,9,5,1],[4,5,8,7,1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a0ef9b-0957-448c-8d51-db95828b38ea",
   "metadata": {},
   "source": [
    "这样就可以直接取词向量训练了吗?\n",
    "\n",
    "不能！上面mini_batch有3个样例，RNN的每一步要输入每个样例的一个单词，一次输入batch_size个样例，所以batch要按list外层是时间步数(即序列长度)，list内层是batch_size排列。即batch的维度应该是:\n",
    "\n",
    "```\n",
    "[seq_len,batch_size]\n",
    "[seq_len,batch_size]\n",
    "[seq_len,batch_size]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "771bc71a-c14e-4256-9791-af34f06642a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 6, 5, 6, 7, 1], [6, 4, 7, 9, 5, 1], [4, 5, 8, 7, 1, 2]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0c17e76-f71f-484e-b34b-9db7c5d92c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 5, 6, 7, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4af77f0d-a461-4265-9321-ba5a95af5134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 4, 7, 9, 5, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ac181bb-f635-4c7a-b0db-bb1df99f13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "mini_batch = list(zip_longest(mini_batch[0], mini_batch[1], mini_batch[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1c8d210-6639-4fe7-b0e0-c0f62188f0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 6, 4), (6, 4, 5), (5, 7, 8), (6, 9, 7), (7, 5, 1), (1, 1, 2)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47213423-112c-4ad6-8d72-491e93168db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = torch.LongTensor(mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21c96f99-4132-4c2e-9bf3-8d39d5e0e6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 6, 4],\n",
       "        [6, 4, 5],\n",
       "        [5, 7, 8],\n",
       "        [6, 9, 7],\n",
       "        [7, 5, 1],\n",
       "        [1, 1, 2]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da57dc53-eeaa-40c1-b386-c62e0614fc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5e282-b68c-449a-a0b0-40f9e3fd160f",
   "metadata": {},
   "source": [
    "这里的mini_batch就是词向量层的输入，那么词向量层的输出是什么样呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51d6c1f8-8615-444d-b14b-c332a8c4a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = torch.nn.Embedding(13, 6)   #假设词向量维度为6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "604c208a-0984-4de6-afef-0d95a07594ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7274,  0.0855, -2.1961, -0.1330,  0.4045,  0.3604],\n",
       "         [ 0.5984,  1.0862,  0.7329, -2.5007,  0.9278, -1.7792],\n",
       "         [ 1.0848,  1.5140,  0.7713,  0.2904,  1.1941,  0.4829]],\n",
       "\n",
       "        [[ 0.5984,  1.0862,  0.7329, -2.5007,  0.9278, -1.7792],\n",
       "         [ 1.0848,  1.5140,  0.7713,  0.2904,  1.1941,  0.4829],\n",
       "         [-0.4636,  0.7657, -0.5665,  0.8889, -1.1510,  1.6418]],\n",
       "\n",
       "        [[-0.4636,  0.7657, -0.5665,  0.8889, -1.1510,  1.6418],\n",
       "         [ 2.7416,  0.5861, -1.3711,  1.0608, -0.0847,  1.6041],\n",
       "         [ 1.2153,  0.8472, -0.3552, -0.0733, -0.0279,  0.2736]],\n",
       "\n",
       "        [[ 0.5984,  1.0862,  0.7329, -2.5007,  0.9278, -1.7792],\n",
       "         [ 0.1090,  0.6214, -1.0293, -1.7593, -1.4738, -1.0516],\n",
       "         [ 2.7416,  0.5861, -1.3711,  1.0608, -0.0847,  1.6041]],\n",
       "\n",
       "        [[ 2.7416,  0.5861, -1.3711,  1.0608, -0.0847,  1.6041],\n",
       "         [-0.4636,  0.7657, -0.5665,  0.8889, -1.1510,  1.6418],\n",
       "         [-1.6617,  1.0632,  0.6106, -0.1512,  1.2257, -0.2087]],\n",
       "\n",
       "        [[-1.6617,  1.0632,  0.6106, -0.1512,  1.2257, -0.2087],\n",
       "         [-1.6617,  1.0632,  0.6106, -0.1512,  1.2257, -0.2087],\n",
       "         [-0.5588, -0.1938,  0.6078,  0.9093,  0.1140, -0.6161]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_batch = embed(mini_batch)\n",
    "embed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b3e6d2b-8cc8-4bee-823b-a519309cc51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 6])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eefae26-0cfc-4e7a-afab-60fe326cf303",
   "metadata": {},
   "source": [
    "每个时序同时读三个单词(batch_size=3)，一个单词向量形状为1\\*6，句子由6个单词组成（包括pad）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436eb96d-80e6-4b0e-8766-e97157d7594d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
