{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777d3532-675d-4c34-b198-38c3af50288e",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "用torch.nn模块搭建的模型是一个layer, 用torch.nn.function模块搭建的模型是函数。\n",
    "\n",
    "`torch.nn.lstm(input_size,hidden_size,num_layers,bias,batch_first,dropout,bidirectional)`\n",
    "\n",
    "**参数**\n",
    "\n",
    "* input_size==embedding_size\n",
    "\n",
    "* hidden_size,lstm模型参数维度\n",
    "\n",
    "* num_layers,有几层LSTM\n",
    "\n",
    "* bias: 隐层状态是否带bias，默认为true。bias是偏置值，或者偏移值。没有偏置值就是以0为中轴，或以0为起点。偏置值的作用请参考单层感知器相关结构。\n",
    "\n",
    "* batch_first: 输入输出的第一维是否为 batch_size，默认值 False。因为 Torch 中，人们习惯使用Torch中带有的dataset，dataloader向神经网络模型连续输入数据，这里面就有一个 batch_size 的参数，表示一次输入多少个数据。 在 LSTM 模型中，输入数据必须是一批数据，为了区分LSTM中的批量数据和dataloader中的批量数据是否相同意义，LSTM 模型就通过这个参数的设定来区分。 如果是相同意义的，就设置为True，如果不同意义的，设置为False。 torch.LSTM 中 batch_size 维度默认是放在第二维度，故此参数设置可以将 batch_size 放在第一维度。如：input 默认是(4,1,5)，中间的 1 是 batch_size，指定batch_first=True后就是(1,4,5)。所以，如果你的输入数据是二维数据的话，就应该将 batch_first 设置为True;\n",
    "\n",
    "* dropout: 默认值0。是否在除最后一个 RNN 层外的其他 RNN 层后面加 dropout 层。输入值是 0-1 之间的小数，表示概率。0表示0概率dripout，即不dropout\n",
    "\n",
    "* bidirectional: 是否是双向 RNN，默认为：false，若为 true，则：num_directions=2，否则为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715b0912-96d2-4992-8096-6eaee570bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#输入维度为10,隐藏层神经元个数为20,总共有2个隐藏层\n",
    "lstm = nn.LSTM(10, 20, 2)  #实例化  括号里的参数(input_size,hidden_size,num_layers)  \n",
    "\n",
    "# batch_size=3,每个句子有5个词，每个词用10维向量表示\n",
    "x = torch.randn(5, 3, 10)  # 准备输入张量,    括号里的参数(seq_len,batch,input_size)\n",
    "\n",
    "# hidden的初始状态\n",
    "h0 = torch.randn(2, 3, 20) # hidden初始状态,括号里的参数(num_layers*num_directions, batch, hidden_size)\n",
    "\n",
    "# cell的初始状态\n",
    "c0 = torch.randn(2, 3, 20) # cell初始状态,  括号里的参数(num_layers * num_directions, batch, hidden_size)\n",
    "output , (hn, cn) = lstm(x, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63be706a-b97f-4ae2-8a28-78c1667f20da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 20])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0af455e-2182-4f08-a63e-1812f40d9ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 20])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba3cb7d7-84a7-4d8b-95b3-f831f4e87de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 20])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909bd97-f98d-4b22-bec3-12e30c65c8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
