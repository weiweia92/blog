{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6e1f5d-62ae-49d8-ab59-2b6e65ec02de",
   "metadata": {},
   "source": [
    "## CLASS torch.nn.Module\n",
    "\n",
    "Base class for all neural network modules. Your models should alse subclass this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d1d4b2-cd64-4e74-8571-7c2745d94b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(conv1(x))\n",
    "        return F.relu(conv2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24821b3f-e565-401f-8e04-7b4a0ca83546",
   "metadata": {},
   "source": [
    "## CLASS torch.nn.Sequential(\\*args)\n",
    "\n",
    "A sequential container.Modules will be added to it in the order they are passed in(传入) the constructor. Alternatively, an ordered dict of modules can also be passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d481a9b0-1c0b-48a9-bdf2-677106f9ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "# Example of using Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 20, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(20, 64, 5),\n",
    "    nn.ReLU()\n",
    "    )\n",
    "# Example of using Sequential with OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "    ('ReLU1', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "    ('ReLU2', nn.ReLU())\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815e57dc-2b71-403e-b6d7-4685a215101e",
   "metadata": {},
   "source": [
    "nn.Sequential里面的模块按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小时一致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15436550-c758-41e2-aacf-6e958a64c170",
   "metadata": {},
   "source": [
    "## CLASS torch.nn.ModuleList(modules=None)\n",
    "\n",
    "nn.ModuleList，它是一个储存不同 module，并自动将每个 module 的 parameters 添加到网络之中的容器。你可以把任意 nn.Module 的子类 (比如 nn.Conv2d, nn.Linear 之类的) 加到这个 list 里面，方法和 Python 自带的 list 一样，无非是 extend，append 等操作。但不同于一般的 list，加入到 nn.ModuleList 里面的 module 是会自动注册到整个网络上的，同时 module 的 parameters 也会自动添加到整个网络中。若使用python的list，则会出问题。下面看一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20b0a229-98aa-4a5b-bcfd-8bbe7de207e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_modlist(\n",
      "  (modlist): ModuleList(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class net_modlist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_modlist, self).__init__()\n",
    "        self.modlist = nn.ModuleList([\n",
    "                       nn.Conv2d(1, 20, 5),\n",
    "                       nn.ReLU(),\n",
    "                        nn.Conv2d(20, 64, 5),\n",
    "                        nn.ReLU()\n",
    "                        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "net_modlist = net_modlist()\n",
    "print(net_modlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24c511d5-2e77-4ccd-aea7-f7cbe7ee9aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([20, 1, 5, 5])\n",
      "<class 'torch.Tensor'> torch.Size([20])\n",
      "<class 'torch.Tensor'> torch.Size([64, 20, 5, 5])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for param in net_modlist.parameters():\n",
    "    print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e8d09c8-b785-4661-a92d-54fddefcd8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_modlist(\n",
      "  (modlist): ModuleList(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class net_modlist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_modlist, self).__init__()\n",
    "        self.modlist = nn.ModuleList()\n",
    "        self.modlist.append(nn.Conv2d(1, 20, 5))\n",
    "        self.modlist.append(nn.ReLU())\n",
    "        self.modlist.append(nn.Conv2d(20, 64, 5))\n",
    "        self.modlist.append(nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n",
    "    \n",
    "net_modlist = net_modlist()\n",
    "print(net_modlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c18b4e3-b42e-45f0-a8a4-35950041a6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([20, 1, 5, 5])\n",
      "<class 'torch.Tensor'> torch.Size([20])\n",
      "<class 'torch.Tensor'> torch.Size([64, 20, 5, 5])\n",
      "<class 'torch.Tensor'> torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for param in net_modlist.parameters():\n",
    "    print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301caa90-da10-4e79-875f-311462f06872",
   "metadata": {},
   "source": [
    "接下来看看另一个作为对比的网络，它使用 Python 自带的 list："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf79bd0-ca9d-4bb6-b8d7-d7b6d9a67c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_modlist()\n"
     ]
    }
   ],
   "source": [
    "class net_modlist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_modlist, self).__init__()\n",
    "        self.modlist = [\n",
    "                       nn.Conv2d(1, 20, 5),\n",
    "                       nn.ReLU(),\n",
    "                        nn.Conv2d(20, 64, 5),\n",
    "                        nn.ReLU()\n",
    "                        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "net_modlist = net_modlist()\n",
    "print(net_modlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b27e46f-af34-4736-8308-58fa58e5b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net_modlist.parameters():\n",
    "    print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535657f8-1e5a-4ac0-9706-94dc6301698b",
   "metadata": {},
   "source": [
    "显然，使用 Python 的 list 添加的卷积层和它们的 parameters 并没有自动注册到我们的网络中。当然，我们还是可以使用 forward 来计算输出结果。但是如果用其实例化的网络进行训练的时候，因为这些层的parameters不在整个网络之中，所以其网络参数也不会被更新，也就是无法训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de37dfe6-71d7-41a9-aeea-3c09aca37334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.linear for i in range(10)])\n",
    "\n",
    "    # ModuleList can act as an iterable, or be indexed using ints\n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = self.linears[i // 2](x) + l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69127252-81ca-4ddb-a544-03f59239565d",
   "metadata": {},
   "source": [
    "## nn.Sequential 与 nn.ModuleList的区别\n",
    "\n",
    "* 不同点1：nn.Sequential内部实现了forward函数，因此可以不用写forward函数。而nn.ModuleList则没有实现内部forward函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54dba39a-c514-4698-b635-9feac719de7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 内部已经实现了forward函数，所以不用写def forward()\n",
    "seq = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f504fc03-4479-4c6d-8057-7a4b986a8c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 12, 12])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(16, 1, 20, 20)\n",
    "seq(input).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15851af-1dd6-4f76-9c78-8ec0a0a415c8",
   "metadata": {},
   "source": [
    "但如果是继承nn.Module类的话，就要写forward函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5adac040-6e29-463b-bbc2-688a9dc705c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "class net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net1, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "                        nn.Conv2d(1, 20, 5),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(20, 64, 5),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "    \n",
    "    # 注意：按照下面这种利用for循环的方式也可以得到同样的结果\n",
    "    # def forward(self, x):\n",
    "    #     for s in self.seq:\n",
    "    #         x = s(x)\n",
    "    #     return x\n",
    "    \n",
    "input = torch.randn(16, 1, 20, 20)\n",
    "net1 = net1()\n",
    "print(net1(input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7abec-17c8-4be3-a544-47bb23f85fe2",
   "metadata": {},
   "source": [
    "而对于nn.ModuleList:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80e52573-3c6a-4ade-bcff-791dcd66b8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#例1：若按照下面这么写，则会产生错误\n",
    "modlist = nn.ModuleList([\n",
    "         nn.Conv2d(1, 20, 5),\n",
    "         nn.ReLU(),\n",
    "         nn.Conv2d(20, 64, 5),\n",
    "         nn.ReLU()\n",
    "         ])\n",
    "print(modlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d668b03a-b6fb-4fde-a54e-d1a97a40e4be",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-807530dbec2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "input = torch.randn(16, 1, 20, 20)\n",
    "print(modlist(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99a9f5ec-305e-4d7c-b5e4-a2f410cf75ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "# 更改：加写了forward函数\n",
    "class net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net2, self).__init__()\n",
    "        self.modlist = nn.ModuleList([\n",
    "                       nn.Conv2d(1, 20, 5),\n",
    "                       nn.ReLU(),\n",
    "                        nn.Conv2d(20, 64, 5),\n",
    "                        nn.ReLU()\n",
    "                        ])\n",
    "        \n",
    "    #这里若按照这种写法则会报NotImplementedError错\n",
    "    #def forward(self, x):\n",
    "    #    return self.modlist(x)\n",
    "\n",
    "    #注意：只能按照下面利用for循环的方式\n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n",
    "    \n",
    "input = torch.randn(16, 1, 20, 20)\n",
    "net2 = net2()\n",
    "print(net2(input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82351a37-1186-4281-a111-ec6f045f542d",
   "metadata": {},
   "source": [
    "如果完全直接用 `nn.Sequential`，确实是可以的，但这么做的代价就是失去了部分灵活性，不能自己去定制 forward 函数里面的内容了。\n",
    "\n",
    "一般情况下 `nn.Sequential` 的用法是来组成卷积块 (block)，然后像拼积木一样把不同的 block 拼成整个网络，让代码更简洁，更加结构化。\n",
    "\n",
    "* 不同点2：nn.Sequential可以使用OrderedDict对每层进行命名，上面已经阐述过了；\n",
    "\n",
    "* 不同点3：nn.Sequential里面的模块按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。而nn.ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序可言。\n",
    "\n",
    "见下面代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e51cde16-6b49-473d-a5a9-50dc19d07351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net3(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=30, bias=True)\n",
      "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net3, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,20), nn.Linear(20,30), nn.Linear(5,10)])\n",
    "    def forward(self, x):\n",
    "        x = self.linears[2](x)\n",
    "        x = self.linears[0](x)\n",
    "        x = self.linears[1](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "net3 = net3()\n",
    "print(net3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed123d9b-3b50-4f1b-b4e6-1baeb65f2211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 30])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(32, 5)\n",
    "print(net3(input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9da1b0-c792-43d0-9fa7-424e945850fc",
   "metadata": {},
   "source": [
    "根据 net3 的结果，可以看出来这个 `ModuleList` 里面的顺序不能决定什么，网络的执行顺序是根据 `forward` 函数来决定的。若将`forward`函数中几行代码互换，使输入输出之间的大小不一致，则程序会报错。此外，为了使代码具有更高的可读性，最好把`ModuleList`和`forward`中的顺序保持一致。\n",
    "\n",
    "* 不同点4：有时候网络中有很多相似或者重复的层，我们一般会考虑用for循环来创建他们，而不是一行一行地写\n",
    "\n",
    "比如：`layer=[nn.Linear(10, 10) for i in range(5)]`\n",
    "\n",
    "那么这里我们使用`ModuleList`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2802c882-5887-4d81-84c7-fde4b19e85bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net4(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net4, self).__init__()\n",
    "        layers = [nn.Linear(10, 10) for i in range(5)]\n",
    "        self.linears = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "net = net4()\n",
    "print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
