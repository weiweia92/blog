{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2921420-1d29-4c3b-b756-d3ad365b10e4",
   "metadata": {},
   "source": [
    "## Keyword Extraction with BERT\n",
    "\n",
    "With methods such as Rake and YAKE! we already have easy-to-use packages that can be used to extract keywords and keyphrases. However these models typically work based on the statistical properties of a text and not so much on semantic similarity.\n",
    "\n",
    "In comes BERT. BERT is a bi-directional transformer model that allows us to transform phrases and documents to vectors that capture their meaning.\n",
    "\n",
    "Although there are many great papers and solutions out there that use BERT-embeddings, I could not find a simple and easy-to-use BERT-based solution. Instead, I decide to create KeyBERT a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings.\n",
    "\n",
    "KeyBERT is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and keyphrases that are most similar to a document. This is a tutorial on how to use BERT to create your own keyword extraction model.\n",
    "\n",
    "### 1. Data\n",
    "\n",
    "For this tutorial, we are going to be using a document about supervised machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210a4f70-d565-47cf-b2a6-c93fcb383b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input to an output based \n",
    "         on example input-output pairs.[1] It infers a function \n",
    "         from labeled training data consisting of a set of \n",
    "         training examples.[2] In supervised learning, each \n",
    "         example is a pair consisting of an input object \n",
    "         (typically a vector) and a desired output value (also \n",
    "         called the supervisory signal). A supervised learning \n",
    "         algorithm analyzes the training data and produces an \n",
    "         inferred function, which can be used for mapping new \n",
    "         examples. An optimal scenario will allow for the algorithm \n",
    "         to correctly determine the class labels for unseen \n",
    "         instances. This requires the learning algorithm to  \n",
    "         generalize from the training data to unseen situations \n",
    "         in a 'reasonable' way (see inductive bias).\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e525ce-0e8c-419a-89be-408fb12995ed",
   "metadata": {},
   "source": [
    "I believe that using a document about a topic that the readers know quite a bit about helps you understand if the resulting keyphrases are of quality.\n",
    "\n",
    "### 2. Candidate Keywords/Keyphrases\n",
    "\n",
    "We start by creating a list of candidate keywords or keyphrases from a document. Although many focus on noun phrases, we are going to keep it simple by using Scikit-Learns `CountVectorizer`. This allows us to specify the length of the keywords and make them into keyphrases. It also is a nice method for quickly removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "558b6044-8c33-4f40-b806-69d2a4e3a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (3, 3)  #(3,3)\n",
    "stop_words = 'english'\n",
    "\n",
    "# Extract candicate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "880d36a1-db51-4e76-af32-bcc53cae1114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates) #72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e228d-0d69-4d49-8fdb-482d5959f4c1",
   "metadata": {},
   "source": [
    "We can use `n_gram_range` to change the size of the resulting candidates. For example, if we would set it to (3, 3) then the resulting candidates would phrases that include 3 keywords.\n",
    "\n",
    "Then, the variable `candidates` is simply a list of strings that includes our candidate keywords/keyphrases.\n",
    "\n",
    "**NOTE**: You can play around with `n_gram_range` to create different lengths of keyphrases. Then, you might not want to remove stop_words as they can tie(扎，绑，系) longer keyphrases together.\n",
    "\n",
    "### 3. Embeddings\n",
    "\n",
    "Next, we convert both the document as well as the candidate keywords/keyphrases to numerical data. We use **BERT** for this purpose as it has shown great results for both similarity- and paraphrasing(释义) tasks.\n",
    "\n",
    "There are many methods for generating the BERT embeddings, such as Flair, Hugginface Transformers, and now even spaCy with their 3.0 release! However, I prefer to use the `sentence-transformers` package as it allows me to quickly create high-quality embeddings that work quite well for sentence- and document-level embeddings.\n",
    "\n",
    "We install the package with `pip install sentence-transformers`. If you run into issues installing this package, then it might be helpful to install Pytorch first.\n",
    "\n",
    "Now, we are going to run the following code to transform our document and candidates into vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2b468ef-e69d-427e-ab52-c818cffb15b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c48853c-a93b-4222-8d45-ba56e740fca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_embeddings.shape #(72, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4493f54-790e-4061-8e2c-22b5c2cf946c",
   "metadata": {},
   "source": [
    "We are **Distilbert** as it has shown great performance in similarity tasks, which is what we are aiming for with keyword/keyphrase extraction!\n",
    "\n",
    "Since transformer models have a token limit, you might run into some errors when inputting large documents. In that case, you could consider splitting up your document into paragraphs and mean pooling (taking the average of) the resulting vectors.\n",
    "\n",
    "**NOTE**: There are many pre-trained BERT-based models(https://github.com/UKPLab/sentence-transformers) that you can use for keyword extraction. However, I would advise you to use either `distilbert — base-nli-stsb-mean-tokens` or `xlm-r-distilroberta-base-paraphase-v1` as they have shown great performance in **semantic similarity** and **paraphrase identification** respectively.\n",
    "\n",
    "### 4. Cosine Similarity\n",
    "\n",
    "In the final step, we want to find the candidates that are most similar to the document. We assume that the most similar candidates to the document are good keywords/keyphrases for representing the document.\n",
    "\n",
    "To calculate the similarity between candidates and the document, we will be using the cosine similarity between vectors as it performs quite well in high-dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6ea9745-40c2-4688-b64b-a0a3c965deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24067fc1-b9bb-4a24-82fe-e364f6c8b6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mapping', 'class', 'training', 'algorithm', 'learning']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbcb485-9189-4f63-97e1-0d894c909519",
   "metadata": {},
   "source": [
    "Now, let us take a look at what happens if we change the `n_gram_range` to (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f7e3c34-d936-4a22-b824-ca20e6ed6650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm analyzes training',\n",
       " 'learning algorithm generalize',\n",
       " 'learning machine learning',\n",
       " 'learning algorithm analyzes',\n",
       " 'algorithm generalize training']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803462d5-2ff6-4d78-8146-b941b6390e0b",
   "metadata": {},
   "source": [
    "It seems that we get keyphrases instead of keywords now! These keyphrases, by themselves, seem to nicely represent the document. However, I am not happy that all keyphrases are so similar to each other.\n",
    "\n",
    "To solve this issue, let us take a look at the **diversification(多样化)** of our results.\n",
    "\n",
    "### 5. Diversification\n",
    "\n",
    "There is a reason why similar results are returned… they best represent the document! If we were to diversify(多样化) the keywords/keyphrases then they are less likely to represent the document well as a collective(集体).\n",
    "\n",
    "Thus, the diversification of our results requires a delicate(巧妙的精美的) balance between the accuracy of keywords/keyphrases and the diversity between them.\n",
    "\n",
    "There are two algorithms that we will be using to diversify our results:\n",
    "\n",
    "- **max Sum Similarity**\n",
    "\n",
    "- **Maximal Marginal Relevance(最大边际相关性)**\n",
    "\n",
    "#### Max Sum Similarity\n",
    "\n",
    "The maximum sum distance between pairs of data is defined as the pairs of data for which the distance between them is maximized. In our case, we want to maximize the candidate similarity to the document whilst(同时) minimizing the similarity between candidates.\n",
    "\n",
    "To do this, we select the top 20 keywords/keyphrases, and from those 20, select the 5 that are the least similar to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7389f461-c1c0-4582-bb58-dd9deda3a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "    \n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "    \n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)),top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i!=j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "            \n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6394ecf-3df2-4650-9271-91bf6b6870c9",
   "metadata": {},
   "source": [
    "If you set a low `nr_candidates`, then our results seem to be very similar to our original cosine similarity method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa4fa944-571c-40a9-bf2f-d25a326dbb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm analyzes training',\n",
       " 'learning algorithm generalize',\n",
       " 'learning machine learning',\n",
       " 'learning algorithm analyzes',\n",
       " 'algorithm generalize training']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba07c269-2702-4841-936c-6c75aa3ca55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['requires learning algorithm',\n",
       " 'signal supervised learning',\n",
       " 'learning function maps',\n",
       " 'algorithm analyzes training',\n",
       " 'learning machine learning']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e250b2-1a69-4b77-9d1e-ea62b7cb8c4a",
   "metadata": {},
   "source": [
    "However, a relatively high `nr_candidates` will create more diverse keyphrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38b0fadc-c9e5-442b-baf9-8d64f2a3e1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['set training examples',\n",
       " 'generalize training data',\n",
       " 'requires learning algorithm',\n",
       " 'supervised learning algorithm',\n",
       " 'learning machine learning']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcefdde-fb03-4917-bcca-3576f9ebedf1",
   "metadata": {},
   "source": [
    "As mentioned before, there is a tradeoff between accuracy and diversity that you must keep in mind. If you increase the `nr_candidates`, then there is a good chance you get very diverse keywords but that are not very good representations of the document.\n",
    "\n",
    "I would advise you to keep `nr_candidates` less than 20% of the total number of unique words in your document.\n",
    "\n",
    "#### Maximal Marginal Relevance\n",
    "\n",
    "The final method for diversifying our results is **Maximal Marginal Relevance (MMR)**. MMR tries to minimize redundancy and maximize the diversity of results in text summarization tasks. Fortunately, a keyword extraction algorithm called **EmbedRank**(https://arxiv.org/pdf/1801.04470.pdf) has implemented a version of MMR that allows us to use it for diversifying our keywords/keyphrases.\n",
    "\n",
    "We start by selecting the keyword/keyphrase that is the most similar to the document. Then, we iteratively select new candidates that are both similar to the document and not similar to the already selected keywords/keyphrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf7f80be-24bd-439c-a5f8-035472cb5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "    \n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "    \n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "    \n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d671612-cec2-43df-a928-ef0aa316a9dc",
   "metadata": {},
   "source": [
    "If we set a relatively **low diversity**, then our results seem to be very similar to our original cosine similarity method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1d009e7-702d-40b7-b73b-ccf932b8c30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm analyzes training',\n",
       " 'learning algorithm generalize',\n",
       " 'learning machine learning',\n",
       " 'learning algorithm analyzes',\n",
       " 'algorithm generalize training']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "524f93c7-11dd-4089-9566-241c796a5426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm generalize training',\n",
       " 'supervised learning algorithm',\n",
       " 'learning machine learning',\n",
       " 'learning algorithm analyzes',\n",
       " 'learning algorithm generalize']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e56070-496c-4d52-8f7b-7ba9dce7b5c1",
   "metadata": {},
   "source": [
    "However, a relatively **high diversity** score will create very diverse keyphrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e791dfe4-117f-4aac-b3e7-4e08511d0a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm analyzes training',\n",
       " 'learning algorithm generalize',\n",
       " 'learning machine learning',\n",
       " 'learning algorithm analyzes',\n",
       " 'algorithm generalize training']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e401826c-f2a9-411f-8232-c62b9208eadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm generalize training',\n",
       " 'labels unseen instances',\n",
       " 'new examples optimal',\n",
       " 'determine class labels',\n",
       " 'supervised learning algorithm']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
