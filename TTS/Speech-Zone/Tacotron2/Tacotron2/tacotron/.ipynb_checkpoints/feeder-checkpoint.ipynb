{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaca817-5f28-44bf-88ec-f5be53e418f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from infolog import log\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tacotron.utils.text import text_to_sequence\n",
    "\n",
    "_batches_per_group = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe4b36-c9f0-4c46-b312-8de0332a64ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feeder:\n",
    "    \"\"\"\n",
    "    Feeds batches of data into queue on a background thread.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, coordinator, metadata_filename, hparams):\n",
    "        super(Feeder, self).__init__()\n",
    "        self._coord = coordinator\n",
    "        self._hparams = hparams\n",
    "        self._cleaner_names = [x.strip() for x in hparams.cleaners.split(',')]\n",
    "        self._train_offset = 0\n",
    "        self._test_offset = 0\n",
    "        \n",
    "        # Load metadata\n",
    "        self._mel_dir = os.path.join(os.path.dirname(metadata_filename), 'mels')\n",
    "        self._linear_dir = os.path.join(os.path.dirname(metadata_filename), 'linear')\n",
    "        with open(metadata_filename, encoding='utf-8') as f:\n",
    "            self._metadata = [line.strip().split('|') for line in f]\n",
    "            frame_shift_ms = hparams.hop_size / hparams.sample_rate\n",
    "            hours = sum([int(x[4]) for x in self._metadata]) * frame_shift_ms / (3600)\n",
    "            log('Loaded metadata for {} examples ({:.2f} hours)'.format(len(self._metadata), hours))\n",
    "\n",
    "        #Train test split\n",
    "        if hparams.tacotron_test_size is None:\n",
    "            assert hparams.tacotron_test_batches is not None\n",
    "\n",
    "        test_size = (hparams.tacotron_test_size if hparams.tacotron_test_size is not None \n",
    "                     else hparams.tacotron_test_batches * hparams.tacotron_batch_size)\n",
    "        indices = np.arange(len(self._metadata))\n",
    "        train_indices, test_indices = train_test_split(indices,test_size=test_size, \n",
    "                                                       random_state=hparams.tacotron_data_random_state)\n",
    "\n",
    "        #Make sure test_indices is a multiple of batch_size else round down\n",
    "        len_test_indices = self._round_down(len(test_indices), hparams.tacotron_batch_size)\n",
    "        extra_test = test_indices[len_test_indices:]\n",
    "        test_indices = test_indices[:len_test_indices]\n",
    "        train_indices = np.concatenate([train_indices, extra_test]) \n",
    "        #所有测试集的数据除以batch_size后剩余的数据，放到训练集里\n",
    "\n",
    "        self._train_meta = list(np.array(self._metadata)[train_indices])\n",
    "        self._test_meta = list(np.array(self._metadata)[test_indices])\n",
    "\n",
    "        self.test_steps = len(self._test_meta) // hparams.tacotron_batch_size\n",
    "\n",
    "        if hparams.tacotron_test_size is None:\n",
    "            assert hparams.tacotron_test_batches == self.test_steps\n",
    "\n",
    "        #pad input sequences with the <pad_token> 0 ( _ )\n",
    "        self._pad = 0\n",
    "        #explicitely setting the padding to a value that doesn't originally exist in the spectogram\n",
    "        #to avoid any possible conflicts, without affecting the output range of the model too much\n",
    "        if hparams.symmetric_mels:\n",
    "            self._target_pad = -hparams.max_abs_value\n",
    "        else:\n",
    "            self._target_pad = 0.\n",
    "        #Mark finished sequences with 1s\n",
    "        self._token_pad = 1.\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            # Create placeholders for inputs and targets. Don't specify batch size because we want\n",
    "            # to be able to feed different batch sizes at eval time.\n",
    "            self._placeholders = [\n",
    "            tf.placeholder(tf.int32, shape=(None, None), name='inputs'),\n",
    "            tf.placeholder(tf.int32, shape=(None, ), name='input_lengths'),\n",
    "            tf.placeholder(tf.float32, shape=(None, None, hparams.num_mels), name='mel_targets'),\n",
    "            tf.placeholder(tf.float32, shape=(None, None), name='token_targets'),\n",
    "            tf.placeholder(tf.float32, shape=(None, None, hparams.num_freq), name='linear_targets'),\n",
    "            tf.placeholder(tf.int32, shape=(None, ), name='targets_lengths'),\n",
    "            tf.placeholder(tf.int32, shape=(hparams.tacotron_num_gpus, None), name='split_infos'),\n",
    "            ]\n",
    "\n",
    "            # Create queue for buffering data\n",
    "            queue = tf.FIFOQueue(8, [tf.int32, tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32], name='input_queue')\n",
    "            self._enqueue_op = queue.enqueue(self._placeholders)\n",
    "            self.inputs, self.input_lengths, self.mel_targets, self.token_targets, self.linear_targets, self.targets_lengths, self.split_infos = queue.dequeue()\n",
    "\n",
    "            self.inputs.set_shape(self._placeholders[0].shape)\n",
    "            self.input_lengths.set_shape(self._placeholders[1].shape)\n",
    "            self.mel_targets.set_shape(self._placeholders[2].shape)\n",
    "            self.token_targets.set_shape(self._placeholders[3].shape)\n",
    "            self.linear_targets.set_shape(self._placeholders[4].shape)\n",
    "            self.targets_lengths.set_shape(self._placeholders[5].shape)\n",
    "            self.split_infos.set_shape(self._placeholders[6].shape)\n",
    "\n",
    "            # Create eval queue for buffering eval data\n",
    "            eval_queue = tf.FIFOQueue(1, [tf.int32, tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32], name='eval_queue')\n",
    "            self._eval_enqueue_op = eval_queue.enqueue(self._placeholders)\n",
    "            self.eval_inputs, self.eval_input_lengths, self.eval_mel_targets, self.eval_token_targets, \\\n",
    "                self.eval_linear_targets, self.eval_targets_lengths, self.eval_split_infos = eval_queue.dequeue()\n",
    "\n",
    "            self.eval_inputs.set_shape(self._placeholders[0].shape)\n",
    "            self.eval_input_lengths.set_shape(self._placeholders[1].shape)\n",
    "            self.eval_mel_targets.set_shape(self._placeholders[2].shape)\n",
    "            self.eval_token_targets.set_shape(self._placeholders[3].shape)\n",
    "            self.eval_linear_targets.set_shape(self._placeholders[4].shape)\n",
    "            self.eval_targets_lengths.set_shape(self._placeholders[5].shape)\n",
    "            self.eval_split_infos.set_shape(self._placeholders[6].shape)\n",
    "\n",
    "    def start_threads(self, session):\n",
    "        self._session = session\n",
    "        thread = threading.Thread(name='background', target=self._enqueue_next_train_group)\n",
    "        thread.daemon = True #Thread will close when parent quits\n",
    "        thread.start()\n",
    "\n",
    "        thread = threading.Thread(name='background', target=self._enqueue_next_test_group)\n",
    "        thread.daemon = True #Thread will close when parent quits\n",
    "        thread.start()\n",
    "\n",
    "    def _get_test_groups(self):\n",
    "        meta = self._test_meta[self._test_offset]\n",
    "        self._test_offset += 1\n",
    "\n",
    "        text = meta[5]\n",
    "\n",
    "        input_data = np.asarray(text_to_sequence(text, self._cleaner_names), dtype=np.int32)\n",
    "        mel_target = np.load(os.path.join(self._mel_dir, meta[1]))\n",
    "        #Create parallel sequences containing zeros to represent a non finished sequence\n",
    "        token_target = np.asarray([0.] * (len(mel_target) - 1))\n",
    "        linear_target = np.load(os.path.join(self._linear_dir, meta[2]))\n",
    "        return (input_data, mel_target, token_target, linear_target, len(mel_target))\n",
    "\n",
    "    def make_test_batches(self):\n",
    "        start = time.time()\n",
    "\n",
    "        # Read a group of examples\n",
    "        n = self._hparams.tacotron_batch_size\n",
    "        r = self._hparams.outputs_per_step\n",
    "\n",
    "        #Test on entire test set\n",
    "        examples = [self._get_test_groups() for i in range(len(self._test_meta))]\n",
    "\n",
    "        # Bucket examples based on similar output sequence length for efficiency\n",
    "        examples.sort(key=lambda x: x[-1])\n",
    "        batches = [examples[i: i+n] for i in range(0, len(examples), n)]\n",
    "        np.random.shuffle(batches)\n",
    "\n",
    "        log('\\nGenerated {} test batches of size {} in {:.3f} sec'.format(len(batches), n, time.time() - start))\n",
    "        return batches, r\n",
    "\n",
    "    def _enqueue_next_train_group(self):\n",
    "        while not self._coord.should_stop():\n",
    "            start = time.time()\n",
    "\n",
    "            # Read a group of examples\n",
    "            n = self._hparams.tacotron_batch_size\n",
    "            r = self._hparams.outputs_per_step\n",
    "            examples = [self._get_next_example() for i in range(n * _batches_per_group)] # _batches_per_group=64\n",
    "\n",
    "            # Bucket examples based on similar output sequence length for efficiency\n",
    "            examples.sort(key=lambda x: x[-1])\n",
    "            batches = [examples[i: i+n] for i in range(0, len(examples), n)]\n",
    "            np.random.shuffle(batches)\n",
    "\n",
    "            log('\\nGenerated {} train batches of size {} in {:.3f} sec'.format(len(batches), n, time.time() - start))\n",
    "            for batch in batches:\n",
    "                feed_dict = dict(zip(self._placeholders, self._prepare_batch(batch, r)))\n",
    "                self._session.run(self._enqueue_op, feed_dict=feed_dict)\n",
    "\n",
    "    def _enqueue_next_test_group(self):\n",
    "        #Create test batches once and evaluate on them for all test steps\n",
    "        test_batches, r = self.make_test_batches()\n",
    "        while not self._coord.should_stop():\n",
    "            for batch in test_batches:\n",
    "                feed_dict = dict(zip(self._placeholders, self._prepare_batch(batch, r)))\n",
    "                self._session.run(self._eval_enqueue_op, feed_dict=feed_dict)\n",
    "\n",
    "    def _get_next_example(self):\n",
    "        \"\"\"Gets a single example (input, mel_target, token_target, linear_target, mel_length) from_ disk\n",
    "        \"\"\"\n",
    "        if self._train_offset >= len(self._train_meta):\n",
    "            self._train_offset = 0\n",
    "            np.random.shuffle(self._train_meta)\n",
    "\n",
    "        meta = self._train_meta[self._train_offset]\n",
    "        self._train_offset += 1\n",
    "\n",
    "        text = meta[5]\n",
    "\n",
    "        input_data = np.asarray(text_to_sequence(text, self._cleaner_names), dtype=np.int32)\n",
    "        mel_target = np.load(os.path.join(self._mel_dir, meta[1]))\n",
    "        #Create parallel sequences containing zeros to represent a non finished sequence\n",
    "        token_target = np.asarray([0.] * (len(mel_target) - 1))\n",
    "        linear_target = np.load(os.path.join(self._linear_dir, meta[2]))\n",
    "        return (input_data, mel_target, token_target, linear_target, len(mel_target))\n",
    "\n",
    "    def _prepare_batch(self, batches, outputs_per_step):\n",
    "        assert 0 == len(batches) % self._hparams.tacotron_num_gpus\n",
    "        size_per_device = int(len(batches) / self._hparams.tacotron_num_gpus)\n",
    "        np.random.shuffle(batches)\n",
    "\n",
    "        inputs = None\n",
    "        mel_targets = None\n",
    "        token_targets = None\n",
    "        linear_targets = None\n",
    "        targets_lengths = None\n",
    "        split_infos = []\n",
    "\n",
    "        targets_lengths = np.asarray([x[-1] for x in batches], dtype=np.int32) #Used to mask loss\n",
    "        input_lengths = np.asarray([len(x[0]) for x in batches], dtype=np.int32)\n",
    "\n",
    "        #Produce inputs/targets of variables lengths for different GPUs\n",
    "        for i in range(self._hparams.tacotron_num_gpus):\n",
    "            batch = batches[size_per_device * i: size_per_device * (i + 1)]\n",
    "            input_cur_device, input_max_len = self._prepare_inputs([x[0] for x in batch])\n",
    "            inputs = np.concatenate((inputs, input_cur_device), axis=1) if inputs is not None else input_cur_device\n",
    "            mel_target_cur_device, mel_target_max_len = self._prepare_targets([x[1] for x in batch], outputs_per_step)\n",
    "            mel_targets = np.concatenate(( mel_targets, mel_target_cur_device), axis=1) if mel_targets is not None else mel_target_cur_device\n",
    "\n",
    "            #Pad sequences with 1 to infer that the sequence is done\n",
    "            token_target_cur_device, token_target_max_len = self._prepare_token_targets([x[2] for x in batch], outputs_per_step)\n",
    "            token_targets = np.concatenate((token_targets, token_target_cur_device),axis=1) if token_targets is not None else token_target_cur_device\n",
    "            linear_targets_cur_device, linear_target_max_len = self._prepare_targets([x[3] for x in batch], outputs_per_step)\n",
    "            linear_targets = np.concatenate((linear_targets, linear_targets_cur_device), axis=1) if linear_targets is not None else linear_targets_cur_device\n",
    "            split_infos.append([input_max_len, mel_target_max_len, token_target_max_len, linear_target_max_len])\n",
    "\n",
    "        split_infos = np.asarray(split_infos, dtype=np.int32)\n",
    "        return (inputs, input_lengths, mel_targets, token_targets, linear_targets, targets_lengths, split_infos)\n",
    "\n",
    "    def _prepare_inputs(self, inputs):\n",
    "        max_len = max([len(x) for x in inputs])\n",
    "        return np.stack([self._pad_input(x, max_len) for x in inputs]), max_len\n",
    "\n",
    "    def _prepare_targets(self, targets, alignment):\n",
    "        max_len = max([len(t) for t in targets])\n",
    "        data_len = self._round_up(max_len, alignment)\n",
    "        return np.stack([self._pad_target(t, data_len) for t in targets]), data_len\n",
    "\n",
    "    def _prepare_token_targets(self, targets, alignment):\n",
    "        max_len = max([len(t) for t in targets]) + 1\n",
    "        data_len = self._round_up(max_len, alignment)\n",
    "        return np.stack([self._pad_token_target(t, data_len) for t in targets]), data_len\n",
    "\n",
    "    def _pad_input(self, x, length):\n",
    "        return np.pad(x, (0, length - x.shape[0]), mode='constant', constant_values=self._pad)\n",
    "\n",
    "    def _pad_target(self, t, length):\n",
    "        return np.pad(t, [(0, length - t.shape[0]), (0, 0)], mode='constant', constant_values=self._target_pad)\n",
    "\n",
    "    def _pad_token_target(self, t, length):\n",
    "        return np.pad(t, (0, length - t.shape[0]), mode='constant', constant_values=self._token_pad)\n",
    "\n",
    "    def _round_up(self, x, multiple):\n",
    "        remainder = x % multiple\n",
    "        return x if remainder == 0 else x + multiple - remainder\n",
    "\n",
    "    def _round_down(self, x, multiple):\n",
    "        remainder = x % multiple\n",
    "        return x if remainder == 0 else x - remainder\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
