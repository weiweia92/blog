{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d7d12-46d9-4ee2-a4bf-86afc3506848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "import infolog\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import audio\n",
    "from hparams import hparams_debug_string\n",
    "from tacotron.feeder import Feeder\n",
    "from tacotron.models import create_model\n",
    "from tacotron.utils import ValueWindow, plot\n",
    "from tacotron.utils.text import sequence_to_text\n",
    "from tacotron.utils.symbols import symbols\n",
    "from tqdm import tqdm\n",
    "\n",
    "log = infolog.log\n",
    "\n",
    "def time_string():\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f271c2-2e60-42bd-b2bc-938056d0c153",
   "metadata": {},
   "source": [
    "### Tensorboard projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4d676-bdc0-4e36-a6a5-5b582076a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embedding_stats(summary_writer, embedding_names, paths_to_meta, checkpoint_path):\n",
    "    #Create tensorboard projector\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    config.model_checkpoint_path = checkpoint_path\n",
    "\n",
    "    for embedding_name, path_to_meta in zip(embedding_names, paths_to_meta):\n",
    "        #Initialize config\n",
    "        embedding = config.embeddings.add()\n",
    "        #Specifiy the embedding variable and the metadata\n",
    "        embedding.tensor_name = embedding_name\n",
    "        embedding.metadata_path = path_to_meta\n",
    "    \n",
    "    #Project the embeddings to space dimensions for visualization\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "def add_train_stats(model, hparams):\n",
    "    with tf.variable_scope('stats') as scope:\n",
    "        for i in range(hparams.tacotron_num_gpus):\n",
    "            tf.summary.histogram(f'mel_outputs {i}', model.tower_mel_outputs[i])\n",
    "            tf.summary.histogram(f'mel_targets {i}', model.tower_mel_targets[i])\n",
    "        tf.summary.scalar('before_loss', model.before_loss)\n",
    "        tf.summary.scalar('after_loss', model.after_loss)\n",
    "\n",
    "        if hparams.predict_linear:\n",
    "            tf.summary.scalar('linear_loss', model.linear_loss)\n",
    "            for i in range(hparams.tacotron_num_gpus):\n",
    "                tf.summary.histogram(f'linear_outputs {i}', model.tower_linear_outputs[i])\n",
    "                tf.summary.histogram(f'linear_targets {i}', model.tower_linear_targets[i])\n",
    "\n",
    "        tf.summary.scalar('regularization_loss', model.regularization_loss)\n",
    "        tf.summary.scalar('stop_token_loss', model.stop_token_loss)\n",
    "        tf.summary.scalar('loss', model.loss)\n",
    "        tf.summary.scalar('learning_rate', model.learning_rate) #Control learning rate decay speed\n",
    "        if hparams.tacotron_teacher_forcing_mode == 'scheduled':\n",
    "            tf.summary.scalar('teacher_forcing_ratio', model.ratio) #Control teacher forcing ratio decay when mode = 'scheduled'\n",
    "        gradient_norms = [tf.norm(grad) for grad in model.gradients]\n",
    "        tf.summary.histogram('gradient_norm', gradient_norms)\n",
    "        tf.summary.scalar('max_gradient_norm', tf.reduce_max(gradient_norms)) #visualize gradients (in case of explosion)\n",
    "        return tf.summary.merge_all()\n",
    "    \n",
    "def add_eval_stats(summary_writer, step, linear_loss, before_loss, after_loss, stop_token_loss, loss):\n",
    "    values = [\n",
    "    tf.Summary.Value(tag='Tacotron_eval_model/eval_stats/eval_before_loss', simple_value=before_loss),\n",
    "    tf.Summary.Value(tag='Tacotron_eval_model/eval_stats/eval_after_loss', simple_value=after_loss),\n",
    "    tf.Summary.Value(tag='Tacotron_eval_model/eval_stats/stop_token_loss', simple_value=stop_token_loss),\n",
    "    tf.Summary.Value(tag='Tacotron_eval_model/eval_stats/eval_loss', simple_value=loss),\n",
    "    ]\n",
    "    if linear_loss is not None:\n",
    "        values.append(tf.Summary.Value(tag='Tacotron_eval_model/eval_stats/eval_linear_loss', simple_value=linear_loss))\n",
    "    test_summary = tf.Summary(value=values)\n",
    "    summary_writer.add_summary(test_summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21a9b8-e8b2-4ff1-9f1e-9b549889e5cf",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d0715-1c6a-4a4c-87de-667ae69811bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_dir, args, hparams):\n",
    "    save_dir = os.path.join(log_dir, 'taco_pretrained')\n",
    "    plot_dir = os.path.join(log_dir, 'plots')\n",
    "    wav_dir = os.path.join(log_dir, 'wavs')\n",
    "    mel_dir = os.path.join(log_dir, 'mel-spectrograms')\n",
    "    eval_dir = os.path.join(log_dir, 'eval-dir')\n",
    "    eval_plot_dir = os.path.join(eval_dir, 'plots')\n",
    "    eval_wav_dir = os.path.join(eval_dir, 'wavs')\n",
    "    tensorboard_dir = os.path.join(log_dir, 'tacotron_events')\n",
    "    meta_folder = os.path.join(log_dir, 'metas')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    os.makedirs(wav_dir, exist_ok=True)\n",
    "    os.makedirs(mel_dir, exist_ok=True)\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    os.makedirs(eval_plot_dir, exist_ok=True)\n",
    "    os.makedirs(eval_wav_dir, exist_ok=True)\n",
    "    os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "    os.makedirs(meta_folder, exist_ok=True)\n",
    "\n",
    "    checkpoint_path = os.path.join(save_dir, 'tacotron_model.ckpt')\n",
    "    input_path = os.path.join(args.base_dir, args.tacotron_input)\n",
    "\n",
    "    if hparams.predict_linear:\n",
    "        linear_dir = os.path.join(log_dir, 'linear-spectrograms')\n",
    "        os.makedirs(linear_dir, exist_ok=True)\n",
    "\n",
    "    log('Checkpoint path: {}'.format(checkpoint_path))\n",
    "    log('Loading training data from: {}'.format(input_path))\n",
    "    log('Using model: {}'.format(args.model))\n",
    "    log(hparams_debug_string())\n",
    "\n",
    "    #Start by setting a seed for repeatability\n",
    "    tf.set_random_seed(hparams.tacotron_random_seed)\n",
    "\n",
    "    #Set up data feeder\n",
    "    coord = tf.train.Coordinator()\n",
    "    with tf.variable_scope('datafeeder') as scope:\n",
    "        feeder = Feeder(coord, input_path, hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd19429-2b04-48e4-9262-bb84f4425186",
   "metadata": {},
   "source": [
    "Feeder 类具体内容前往feeder.ipynb查看 http://localhost:8888/lab/tree/TTS/Speech-Zone/Tacotron2/Tacotron2/tacotron/feeder.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5bbd0-7bc6-4df0-8c85-16d8ef9a732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Set up model:\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    model, stats = model_train_mode(args, feeder, hparams, global_step)\n",
    "    eval_model = model_test_mode(args, feeder, hparams, global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e6f22-cc73-4cc8-b584-0dc8b4655380",
   "metadata": {},
   "source": [
    "其中`model_train_mode`,`model_text_mode`在train.py中有定义，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb760c59-3d87-4254-85d7-ee755a1a7a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_mode(args, feeder, hparams, global_step):\n",
    "    with tf.variable_scope('Tacotron_model', reuse=tf.AUTO_REUSE) as scope:\n",
    "        model_name = None\n",
    "        if args.model == 'Tacotron-2':\n",
    "            model_name = 'Tacotron'\n",
    "        model = create_model(model_name or args.model, hparams)\n",
    "        if hparams.predict_linear:\n",
    "            model.initialize(feeder.inputs, feeder.input_lengths, feeder.mel_targets, feeder.token_targets, linear_targets=feeder.linear_targets,\n",
    "                             targets_lengths=feeder.targets_lengths, global_step=global_step,is_training=True, split_infos=feeder.split_infos)\n",
    "        else:\n",
    "            model.initialize(feeder.inputs, feeder.input_lengths, feeder.mel_targets, feeder.token_targets,\n",
    "                             targets_lengths=feeder.targets_lengths, global_step=global_step,is_training=True, split_infos=feeder.split_infos)\n",
    "        model.add_loss()\n",
    "        model.add_optimizer(global_step)\n",
    "        stats = add_train_stats(model, hparams)\n",
    "        return model, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe6ebc-3422-4d92-84a6-1d7130b2fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test_mode(args, feeder, hparams, global_step):\n",
    "    with tf.variable_scope('Tacotron_model', reuse=tf.AUTO_REUSE) as scope:\n",
    "        model_name = None\n",
    "        if args.model == 'Tacotron-2':\n",
    "            model_name = 'Tacotron'\n",
    "        model = create_model(model_name or args.model, hparams)\n",
    "        if hparams.predict_linear:\n",
    "            model.initialize(feeder.eval_inputs, feeder.eval_input_lengths, feeder.eval_mel_targets, feeder.eval_token_targets,linear_targets=feeder.eval_linear_targets, \n",
    "                             targets_lengths=feeder.eval_targets_lengths, global_step=global_step,is_training=False, \n",
    "                             is_evaluating=True, split_infos=feeder.eval_split_infos)\n",
    "        else:\n",
    "            model.initialize(feeder.eval_inputs, feeder.eval_input_lengths, feeder.eval_mel_targets, feeder.eval_token_targets,\n",
    "                             targets_lengths=feeder.eval_targets_lengths, global_step=global_step, is_training=False, \n",
    "                             is_evaluating=True, split_infos=feeder.eval_split_infos)\n",
    "        model.add_loss()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a514f77-5b19-4c20-83f8-9c2827290d2c",
   "metadata": {},
   "source": [
    "`create_model`在 `models/__init__.py`有定义。  http://localhost:8888/lab/tree/TTS/Speech-Zone/Tacotron2/Tacotron2/tacotron/models/__init__.py\n",
    "\n",
    "而`create_model`又调用了 Tacotron类， 此类在`models/tacotron.ipynb`有讲解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200cb2e-be44-479f-960d-18b914bc8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tacotron_train(args, log_dir, hparams):\n",
    "    return train(log_dir, args, hparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
