{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981c28b-dcca-4f29-b66b-cf8c55635cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Attention file for location based attention 基于位置的注意力机制(compatible with tensorflow attention wrapper)\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.seq2seq.python.ops.attention_wrapper import BahdanauAttention\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.python.ops import array_ops, math_ops, nn_ops, variable_scope\n",
    "\n",
    "#From https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\n",
    "def _compute_attention(attention_mechanism, cell_output, attention_state, \n",
    "                       attention_layer, prev_max_attentions):\n",
    "    \"\"\"Computes the attention and alignments for a given attention_mechanism.\"\"\"\n",
    "\talignments, next_attention_state, max_attentions = attention_mechanism(\n",
    "\t\tcell_output, state=attention_state, prev_max_attentions=prev_max_attentions)\n",
    "\n",
    "\t# Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]\n",
    "\texpanded_alignments = array_ops.expand_dims(alignments, 1)\n",
    "\t# Context is the inner product of alignments and values along the\n",
    "\t# memory time dimension.\n",
    "\t# alignments shape is\n",
    "\t#   [batch_size, 1, memory_time]\n",
    "\t# attention_mechanism.values shape is\n",
    "\t#   [batch_size, memory_time, memory_size]\n",
    "\t# the batched matmul is over memory_time, so the output shape is\n",
    "\t#   [batch_size, 1, memory_size].\n",
    "\t# we then squeeze out the singleton dim.\n",
    "\tcontext = math_ops.matmul(expanded_alignments, attention_mechanism.values)\n",
    "\tcontext = array_ops.squeeze(context, [1])\n",
    "\n",
    "\tif attention_layer is not None:\n",
    "\t\tattention = attention_layer(array_ops.concat([cell_output, context], 1))\n",
    "\telse:\n",
    "\t\tattention = context\n",
    "\n",
    "\treturn attention, alignments, next_attention_state, max_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe4660-7003-4afc-82f7-c0902244f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _location_sensitive_score(W_query, W_fil, W_keys):\n",
    "    \"\"\"Impelements Bahdanau-style (cumulative) scoring function.\n",
    "    This attention is described in:\n",
    "        J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based models for speech recognition,” in Ad-\n",
    "        vances in Neural Information Processing Systems, 2015, pp.577–585.\n",
    "#############################################################################\n",
    "\t\t\t  hybrid attention (content-based + location-based)\n",
    "\t\t\t\t\t\t\t   f = F * α_{i-1}\n",
    "\t   energy = dot(v_a, tanh(W_keys(h_enc) + W_query(h_dec) + W_fil(f) + b_a))\n",
    "\t#############################################################################\n",
    "\tArgs:\n",
    "\t\tW_query: Tensor, shape '[batch_size, 1, attention_dim]' to compare to location features.\n",
    "\t\tW_location: processed previous alignments into location features, shape '[batch_size, max_time, attention_dim]'\n",
    "\t\tW_keys: Tensor, shape '[batch_size, max_time, attention_dim]', typically the encoder outputs.\n",
    "\tReturns:\n",
    "\t\tA '[batch_size, max_time]' attention score (energy)\n",
    "\t\"\"\"\n",
    "\t# Get the number of hidden units from the trailing dimension of keys\n",
    "\tdtype = W_query.dtype\n",
    "\tnum_units = W_keys.shape[-1].value or array_ops.shape(W_keys)[-1]\n",
    "\n",
    "\tv_a = tf.get_variable(\n",
    "\t\t'attention_variable_projection', shape=[num_units], dtype=dtype,\n",
    "\t\tinitializer=tf.contrib.layers.xavier_initializer())\n",
    "\tb_a = tf.get_variable(\n",
    "\t\t'attention_bias', shape=[num_units], dtype=dtype,\n",
    "\t\tinitializer=tf.zeros_initializer())\n",
    "\n",
    "\treturn tf.reduce_sum(v_a * tf.tanh(W_keys + W_query + W_fil + b_a), [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf67bd-224d-464a-8822-88847381f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationSensitiveAttention(BahdanauAttention):\n",
    "    \"\"\"Impelements Bahdanau-style (cumulative) scoring function.\n",
    "    Usually referred to as \"hybrid\" attention (content-based + location-based)\n",
    "    Extends the additive attention described in:\n",
    "    \"D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in Proceedings of ICLR, 2015.\"\n",
    "    to use previous alignments as additional location features.\n",
    "    This attention is described in:\n",
    "    J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based models for speech recognition,” in Ad-\n",
    "    vances in Neural Information Processing Systems, 2015, pp.577–585.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_units, memory, hparams, is_training, mask_encoder=True, memory_sequence_length=None, \n",
    "                 smoothing=False,cumulate_weights=True, name='LocationSensitiveAttention'):\n",
    "        \"\"\"Construct the Attention mechanism.\n",
    "        Args:\n",
    "            num_units: The depth of the query mechanism.\n",
    "            memory: The memory to query; usually the output of an RNN encoder.  This\n",
    "                tensor should be shaped `[batch_size, max_time, ...]`.\n",
    "            mask_encoder (optional): Boolean, whether to mask encoder paddings.\n",
    "            memory_sequence_length (optional): Sequence lengths for the batch entries\n",
    "                in memory.  If provided, the memory tensor rows are masked with zeros\n",
    "                for values past the respective sequence lengths. Only relevant if mask_encoder = True.\n",
    "            smoothing (optional): Boolean. Determines which normalization function to use.\n",
    "                Default normalization function (probablity_fn) is softmax. If smoothing is\n",
    "                enabled, we replace softmax with:\n",
    "                        a_{i, j} = sigmoid(e_{i, j}) / sum_j(sigmoid(e_{i, j}))\n",
    "                Introduced in:\n",
    "                    J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based models for speech recognition,” \n",
    "                    in Advances in Neural Information Processing Systems, 2015, pp.577–585.\n",
    "                This is mainly used if the model wants to attend to multiple input parts\n",
    "                at the same decoding step. We probably won't be using it since multiple sound\n",
    "                frames may depend on the same character/phone, probably not the way around.\n",
    "                Note:\n",
    "                    We still keep it implemented in case we want to test it. They used it in the\n",
    "                    paper in the context of speech recognition, where one phoneme may depend on\n",
    "                    multiple subsequent sound frames.\n",
    "            name: Name to use when creating ops.\n",
    "        \"\"\"\n",
    "\t\t#Create normalization function\n",
    "\t\t#Setting it to None defaults in using softmax\n",
    "\t\tnormalization_function = _smoothing_normalization if (smoothing == True) else None\n",
    "\t\tmemory_length = memory_sequence_length if (mask_encoder==True) else None\n",
    "\t\tsuper(LocationSensitiveAttention, self).__init__(\n",
    "\t\t\t\tnum_units=num_units,\n",
    "\t\t\t\tmemory=memory,\n",
    "\t\t\t\tmemory_sequence_length=memory_length,\n",
    "\t\t\t\tprobability_fn=normalization_function,\n",
    "\t\t\t\tname=name)\n",
    "\n",
    "\t\tself.location_convolution = tf.layers.Conv1D(filters=hparams.attention_filters,\n",
    "\t\t\tkernel_size=hparams.attention_kernel, padding='same', use_bias=True,\n",
    "\t\t\tbias_initializer=tf.zeros_initializer(), name='location_features_convolution')\n",
    "\t\tself.location_layer = tf.layers.Dense(units=num_units, use_bias=False,\n",
    "\t\t\tdtype=tf.float32, name='location_features_layer')\n",
    "\t\tself._cumulate = cumulate_weights\n",
    "\t\tself.synthesis_constraint = hparams.synthesis_constraint and not is_training\n",
    "\t\tself.attention_win_size = tf.convert_to_tensor(hparams.attention_win_size, dtype=tf.int32)\n",
    "\t\tself.constraint_type = hparams.synthesis_constraint_type\n",
    "\n",
    "\tdef __call__(self, query, state, prev_max_attentions):\n",
    "\t\t\"\"\"Score the query based on the keys and values.\n",
    "\t\tArgs:\n",
    "\t\t\tquery: Tensor of dtype matching `self.values` and shape\n",
    "\t\t\t\t`[batch_size, query_depth]`.\n",
    "\t\t\tstate (previous alignments): Tensor of dtype matching `self.values` and shape\n",
    "\t\t\t\t`[batch_size, alignments_size]`\n",
    "\t\t\t\t(`alignments_size` is memory's `max_time`).\n",
    "\t\tReturns:\n",
    "\t\t\talignments: Tensor of dtype matching `self.values` and shape\n",
    "\t\t\t\t`[batch_size, alignments_size]` (`alignments_size` is memory's\n",
    "\t\t\t\t`max_time`).\n",
    "\t\t\"\"\"\n",
    "\t\tprevious_alignments = state\n",
    "\t\twith variable_scope.variable_scope(None, \"Location_Sensitive_Attention\", [query]):\n",
    "\n",
    "\t\t\t# processed_query shape [batch_size, query_depth] -> [batch_size, attention_dim]\n",
    "\t\t\tprocessed_query = self.query_layer(query) if self.query_layer else query\n",
    "\t\t\t# -> [batch_size, 1, attention_dim]\n",
    "\t\t\tprocessed_query = tf.expand_dims(processed_query, 1)\n",
    "\n",
    "\t\t\t# processed_location_features shape [batch_size, max_time, attention dimension]\n",
    "\t\t\t# [batch_size, max_time] -> [batch_size, max_time, 1]\n",
    "\t\t\texpanded_alignments = tf.expand_dims(previous_alignments, axis=2)\n",
    "\t\t\t# location features [batch_size, max_time, filters]\n",
    "\t\t\tf = self.location_convolution(expanded_alignments)\n",
    "\t\t\t# Projected location features [batch_size, max_time, attention_dim]\n",
    "\t\t\tprocessed_location_features = self.location_layer(f)\n",
    "\n",
    "\t\t\t# energy shape [batch_size, max_time]\n",
    "\t\t\tenergy = _location_sensitive_score(processed_query, processed_location_features, self.keys)\n",
    "\n",
    "\t\tif self.synthesis_constraint:\n",
    "\t\t\tTx = tf.shape(energy)[-1]\n",
    "\t\t\t# prev_max_attentions = tf.squeeze(prev_max_attentions, [-1])\n",
    "\t\t\tif self.constraint_type == 'monotonic':\n",
    "\t\t\t\tkey_masks = tf.sequence_mask(prev_max_attentions, Tx)\n",
    "\t\t\t\treverse_masks = tf.sequence_mask(Tx - self.attention_win_size - prev_max_attentions, Tx)[:, ::-1]\n",
    "\t\t\telse:\n",
    "\t\t\t\tassert self.constraint_type == 'window'\n",
    "\t\t\t\tkey_masks = tf.sequence_mask(prev_max_attentions - (self.attention_win_size // 2 + (self.attention_win_size % 2 != 0)), Tx)\n",
    "\t\t\t\treverse_masks = tf.sequence_mask(Tx - (self.attention_win_size // 2) - prev_max_attentions, Tx)[:, ::-1]\n",
    "\t\t\t\n",
    "\t\t\tmasks = tf.logical_or(key_masks, reverse_masks)\n",
    "\t\t\tpaddings = tf.ones_like(energy) * (-2 ** 32 + 1)  # (N, Ty/r, Tx)\n",
    "\t\t\tenergy = tf.where(tf.equal(masks, False), energy, paddings)\n",
    "\n",
    "\t\t# alignments shape = energy shape = [batch_size, max_time]\n",
    "\t\talignments = self._probability_fn(energy, previous_alignments)\n",
    "\t\tmax_attentions = tf.argmax(alignments, -1, output_type=tf.int32) # (N, Ty/r)\n",
    "\n",
    "\t\t# Cumulate alignments\n",
    "\t\tif self._cumulate:\n",
    "\t\t\tnext_state = alignments + previous_alignments\n",
    "\t\telse:\n",
    "\t\t\tnext_state = alignments\n",
    "\n",
    "\t\treturn alignments, next_state, max_attentions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
