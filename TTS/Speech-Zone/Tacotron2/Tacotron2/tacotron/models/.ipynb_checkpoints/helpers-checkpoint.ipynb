{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1aba8f-f4f6-49e4-be37-ddf3ed85c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TacoTrainingHelper(Helper):\n",
    "    def __init__(self, batch_size, targets, hparams, gta, evaluating, global_step):\n",
    "        # inputs is [N, T_in], targets is [N, T_out, D]\n",
    "        with tf.name_scope('TacoTrainingHelper'):\n",
    "            self._batch_size = batch_size\n",
    "            self._output_dim = hparams.num_mels\n",
    "            self._reduction_factor = hparams.outputs_per_step\n",
    "            self._ratio = tf.convert_to_tensor(hparams.tacotron_teacher_forcing_ratio)\n",
    "            self.gta = gta\n",
    "            self.eval = evaluating\n",
    "            self._hparams = hparams\n",
    "            self.global_step = global_step\n",
    "\n",
    "            r = self._reduction_factor\n",
    "            # Feed every r-th target frame as input\n",
    "            self._targets = targets[:, r-1::r, :]\n",
    "\n",
    "            #Maximal sequence length\n",
    "            self._lengths = tf.tile([tf.shape(self._targets)[1]], [self._batch_size])\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def token_output_size(self):\n",
    "        return self._reduction_factor\n",
    "\n",
    "    @property\n",
    "    def sample_ids_shape(self):\n",
    "        return tf.TensorShape([])\n",
    "\n",
    "    @property\n",
    "    def sample_ids_dtype(self):\n",
    "        return np.int32\n",
    "\n",
    "    def initialize(self, name=None):\n",
    "        #Compute teacher forcing ratio for this global step.\n",
    "        #In GTA mode, override teacher forcing scheme to work with full teacher forcing\n",
    "        if self.gta:\n",
    "            self._ratio = tf.convert_to_tensor(1.) #Force GTA model to always feed ground-truth\n",
    "        elif self.eval and self._hparams.tacotron_natural_eval:\n",
    "            self._ratio = tf.convert_to_tensor(0.) #Force eval model to always feed predictions\n",
    "        else:\n",
    "            if self._hparams.tacotron_teacher_forcing_mode == 'scheduled':\n",
    "                self._ratio = _teacher_forcing_ratio_decay(self._hparams.tacotron_teacher_forcing_init_ratio,\n",
    "                    self.global_step, self._hparams)\n",
    "\n",
    "        return (tf.tile([False], [self._batch_size]), _go_frames(self._batch_size, self._output_dim))\n",
    "\n",
    "    def sample(self, time, outputs, state, name=None):\n",
    "        return tf.tile([0], [self._batch_size])  # Return all 0; we ignore them\n",
    "\n",
    "    def next_inputs(self, time, outputs, state, sample_ids, stop_token_prediction, name=None):\n",
    "        with tf.name_scope(name or 'TacoTrainingHelper'):\n",
    "            #synthesis stop (we let the model see paddings as we mask them when computing loss functions)\n",
    "            finished = (time + 1 >= self._lengths)\n",
    "\n",
    "            #Pick previous outputs randomly with respect to teacher forcing ratio\n",
    "            next_inputs = tf.cond(\n",
    "                tf.less(tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32), self._ratio),\n",
    "                lambda: self._targets[:, time, :], #Teacher-forcing: return true frame\n",
    "                lambda: outputs[:,-self._output_dim:])\n",
    "\n",
    "            #Pass on state\n",
    "            next_state = state\n",
    "            return (finished, next_inputs, next_state)\n",
    "\n",
    "\n",
    "def _go_frames(batch_size, output_dim):\n",
    "    '''Returns all-zero <GO> frames for a given batch size and output dimension'''\n",
    "    return tf.tile([[0.0]], [batch_size, output_dim])\n",
    "\n",
    "def _teacher_forcing_ratio_decay(init_tfr, global_step, hparams):\n",
    "        #################################################################\n",
    "        # Narrow Cosine Decay:\n",
    "\n",
    "        # Phase 1: tfr = init\n",
    "        # We only start learning rate decay after 10k steps\n",
    "\n",
    "        # Phase 2: tfr in ]init, final[\n",
    "        # decay reach minimal value at step ~40k\n",
    "\n",
    "        # Phase 3: tfr = final\n",
    "        # clip by minimal teacher forcing ratio value (step >~ 40k)\n",
    "        #################################################################\n",
    "        #Pick final teacher forcing rate value\n",
    "        if hparams.tacotron_teacher_forcing_final_ratio is not None:\n",
    "            alpha = float(hparams.tacotron_teacher_forcing_final_ratio / hparams.tacotron_teacher_forcing_init_ratio)\n",
    "\n",
    "        else:\n",
    "            assert hparams.tacotron_teacher_forcing_decay_alpha is not None\n",
    "            alpha = hparams.tacotron_teacher_forcing_decay_alpha\n",
    "\n",
    "        #Compute natural cosine decay\n",
    "        tfr = tf.train.cosine_decay(init_tfr,\n",
    "            global_step=global_step - hparams.tacotron_teacher_forcing_start_decay, #tfr ~= init at step 10k\n",
    "            decay_steps=hparams.tacotron_teacher_forcing_decay_steps, #tfr ~= final at step ~40k\n",
    "            alpha=alpha, #tfr = alpha% of init_tfr as final value\n",
    "            name='tfr_cosine_decay')\n",
    "\n",
    "        #force teacher forcing ratio to take initial value when global step < start decay step.\n",
    "        narrow_tfr = tf.cond(\n",
    "            tf.less(global_step, tf.convert_to_tensor(hparams.tacotron_teacher_forcing_start_decay)),\n",
    "            lambda: tf.convert_to_tensor(init_tfr),\n",
    "            lambda: tfr)\n",
    "\n",
    "        return narrow_tfr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
