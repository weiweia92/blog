## WaveRNN

å°†å£°å­¦ç‰¹å¾è½¬æ¢ä¸ºè¯­éŸ³æ³¢å½¢çš„è¿™ç±»æ¨¡å‹é€šå¸¸è¢«ç§°ä½œvocoderï¼Œä¸€èˆ¬æ¥è¯´ï¼Œç›®å‰çš„ç«¯åˆ°ç«¯è¯­éŸ³åˆæˆé¦–å…ˆç”±å£°å­¦æ¨¡å‹ç”Ÿæˆå£°å­¦ç‰¹å¾ï¼Œæ¯”å¦‚æ¢…å°”é¢‘è°±ã€çº¿æ€§è°±ç­‰ï¼Œå†ç”±å£°ç å™¨è½¬æ¢ä¸ºæœ€ç»ˆçš„è¯­éŸ³ã€‚ç”Ÿæˆè¯­éŸ³ä¸­çš„éŸµå¾‹ã€è¡¨ç°åŠ›ç­‰ç”±å£°å­¦æ¨¡å‹å†³å®šï¼Œè€Œæ¸…æ™°åº¦åˆ™ç”±å£°ç å™¨å†³å®šï¼Œå£°ç å™¨é™åˆ¶äº†æœ€ç»ˆåˆæˆè¯­éŸ³çš„éŸ³è´¨ï¼ŒåŒæ—¶ä¹Ÿæ˜¯æ•´ä¸ªè¯­éŸ³åˆæˆæ¨¡å‹çš„è®¡ç®—ç“¶é¢ˆã€‚åºåˆ—æ¨¡å‹åœ¨æ–‡å­—å’Œè¯­éŸ³é¢†åŸŸçš„ä»»åŠ¡ä¸­éƒ½æœ‰è¾ƒå¥½çš„ç»“æœï¼Œä½†æ˜¯å¦‚ä½•æ›´å¿«é€Ÿåœ°è¿›è¡Œé‡‡æ ·ä»ç„¶æ˜¯ä¸€ä¸ªæ¯”è¾ƒå›°éš¾çš„é—®é¢˜ã€‚  

WaveRNNæ˜¯WaveNetçš„è¿›åŒ–ç‰ˆã€‚wavenet is a generative model operating directly on the raw audio waveform. the joint probability of a waveform $ X={x_1, ...,x_T}$ is factorised as a product of conditional probabilities as follows:  

$$ p(X)=\prod _{t=1}^{T}p(x_t|x_1, ...,x_{t-1})$$

This structure lets the models allot(åˆ†é…) significant capacity to estimate each conditional factor, makes them robust during training and easy to evaluate. The ordering encoded in the structure also makes the sampling process strictly serial: a sample can be generated only after samples on which it depends have been produced in accordance withæ ¹æ® the ordering. The sampling process is slow and impractical to use these models to generate high-dimensional data like speech and video. Our goal is to increase the efficiency of sampling from sequential models without compromising their quality.  

### åºåˆ—æ¨¡å‹çš„ç”Ÿæˆè€—æ—¶åˆ†æ

$$ T(u)=|u|\sum_{i=1}^N(c(op_i)+d(op_i))$$ 
$ T(u)$ å‘éŸ³éœ€è¦çš„æ—¶é—´ï¼Œå…¶ä¸­ $u$ è¡¨ç¤ºåºåˆ—é•¿åº¦ä¸€å…±æœ‰ $ |u|$ä¸ªsampleï¼Œ$N$ è¡¨ç¤ºç½‘ç»œå±‚æ•°ï¼Œ$c(op_i)$ è¡¨ç¤ºæ¯ä¸€å±‚çš„è®¡ç®—æ—¶é—´ï¼ˆwide layer or a large number of parameters,åˆ™è®¡ç®—æ—¶é—´é•¿ï¼‰, $d(op_i)$ è¡¨ç¤ºç¡¬ä»¶æ‰§è¡Œç¨‹åºçš„overheadæ—¶é—´ï¼ŒåŒ…å«äº†è°ƒç”¨ç¨‹åºï¼Œæå–å¯¹åº”å‚æ•°ä¹‹ç±»çš„æ—¶é—´ã€‚è¦æƒ³è¯­éŸ³ç”Ÿæˆçš„å¿«ï¼Œä¸Šé¢çš„æ¯ä¸ªå‚æ•°éƒ½è¦å°½é‡å°ã€‚å¯¹äºæ³¢å½¢é‡å»ºè€Œè¨€ï¼Œåºåˆ—é•¿åº¦|ğ‘¢|æœ¬èº«å°±å¾ˆå¤§ï¼Œç‰¹åˆ«å¯¹äºé«˜è´¨é‡çš„è¯­éŸ³è¿›è¡Œé‡å»ºæ—¶ï¼Œæ¯ç§’åŒ…æ‹¬24000ä¸ª16bitä½æ·±çš„æ ·æœ¬ç‚¹ã€‚å› æ­¤ä¸ºäº†æé«˜æ•ˆç‡ï¼Œå¯ä»¥é€šè¿‡ï¼š  

**1. Reduce N**:The WaveRNN model is a single-layer RNN with a dualåŒ softmax layer that is designed to efficiently predict 16-bit raw audio samples.The WaveRNN achieves this performance by requiring just N = 5 matrix-vector products in sequence for each 16-bit sample; however, WaveNet that has 30 residual blocks of two layers each requiring a series of N = 30 âˆ— 2 = 60 matrix-vector products.  

**2. Reduce $c(op_i)$ --Sparse WaveRNN**:reducing the number of parameters in the network by weight pruning to sparsify the weights on WaveRNN. For a fixed parameter count, we discover that large sparse WaveRNNs significantly outperform small dense WaveRNNs  

**3. Reduce $d(op_i)$**: We sidestepå›é¿ the overhead by implementing custom GPU operations for the sampling process.é’ˆå¯¹æ‰€è¿›è¡Œå®éªŒçš„GPUå‹å·(P100)è®¾è®¡äº†hidden unitsçš„ä¸ªæ•°896ï¼Œä½¿å¾—æ‰€æœ‰å‚æ•°éƒ½å¯ä»¥loadåˆ°GPUçš„å¯„å­˜å™¨ä¸­ï¼Œä»è€Œå¤§å¹…åº¦å‡å°‘äº†overhead æ—¶é—´(d)ï¼Œæ¨¡å‹çš„é€Ÿåº¦è¾¾åˆ°äº†96000 samples/secondã€‚  

**4. Propose a generation process based on subscaling--subscale WaveRNN**  
$|u|$å›ºç„¶ä¸èƒ½å‡å°ï¼ŒA tensor of scale $L$ is folded into $B$ sub-tensors of scale $L/B$. The $B$ sub-tensors are generated in order, each conditioned on the previous sub-tensors. Subscaling lets us generate multiple samples at once in a batch. å®è·µä¸­ï¼Œæ¯ä¸ªå­å‘é‡çš„ç”Ÿæˆä»…éœ€è¦å¾ˆå°çš„ä¸‹æ–‡ä¿¡æ¯ï¼Œä¸éœ€è¦ä¾èµ–é¥è¿œçš„æœªæ¥ä¿¡æ¯ï¼Œå› æ­¤ä¸‹ä¸€ä¸ªå­å‘é‡çš„ç”Ÿæˆï¼Œå¯ä»¥åœ¨ä¸Šä¸€ä¸ªå­å‘é‡ç”Ÿæˆè¿‡ç¨‹å¼€å§‹çš„ä¸ä¹…ä¹‹åå°±è¿›è¡Œã€‚å®éªŒä¸­ï¼Œsubscale WaveRNNèƒ½å¤Ÿåœ¨æ¯ä¸€æ­¥äº§ç”Ÿğµ=16ä¸ªæ ·æœ¬ï¼Œè€Œä¸ä¼šæŸå¤±éŸ³é¢‘æ¸…æ™°åº¦ã€‚

#### 1.WaveRNN structure

![](pic/1615974203821-24f3a7b5-d26d-46a6-87aa-67ab74c69f5b.png)

æ¯ä¸ªéŸ³é¢‘æ ·æœ¬ç‚¹è¦ç”Ÿæˆ16bitï¼Œå¦‚æœç›´æ¥ä½¿ç”¨softmaxéœ€è¦ä¸ªæ ‡ç­¾ï¼ŒWe split the state of the RNN in two parts that predict respectively the 8 coarse (or more significant) bits  and the 8 fine (or least significant) bits  of the 16-bit audio sample. ä»è€Œå°†è¾“å‡ºç©ºé—´å‹ç¼©åˆ°$2^8=256$ä¸ªå€¼ä¸Šã€‚

The overall computation in the WaveRNN is as follows:

$$ x_t = [c_{t-1},f_{t-1},c_t]$$
$$ u_t=\sigma(R_uh_{t-1}+I_u^*x_t)$$
$$ r_t=\sigma(R_rh_{t-1}+I_r^*x_t)$$
$$ e_t=\tau(r_t)\circ(R_eh_{t-1}+I_e^*x_t)$$
$$ h_t=u_t \circ h_{t-1}+(1-u_t) \circ e_t$$
$$ y_c,y_f=\text{split}(h_t)$$
$$ p(c_t)=\text{softmax}(O_2relu(O_1y_c))$$
$$ p(f_t)=\text{softmax}(O_4relu(O_3y_f))$$

ç”±äº$f_t$ çš„å€¼ä¾èµ–äº $c_t$ï¼Œå› æ­¤éœ€è¦å…ˆè®¡ç®— $c_t$ï¼Œæ³¨æ„åˆ°å…¬å¼ç¬¬1è¡Œä¸­æ—¢ä½œä¸ºè¾“å…¥ï¼Œå…¬å¼ç¬¬7è¡Œåˆä½œä¸ºäº†è¾“å‡ºã€‚è¿™å°±æ˜¯å¸¦æœ‰$âˆ—$çš„è¿™äº›çŸ©é˜µçš„å±è”½ä½œç”¨äº†ï¼ŒçŸ©é˜µğ¼å…¶å®æ˜¯ä¸€ä¸ªæ©è”½çŸ©é˜µï¼ˆmask matrixï¼‰ï¼Œç”¨äºåˆ‡æ–­è¾“å…¥ä¸­çš„ $c_t$ å’Œè¾“å‡ºä¸­çš„ $c_t$ çš„ä¸€åˆ‡è¿æ¥ï¼Œæœ€ç»ˆä½œä¸ºè¾“å…¥çš„ç²—8ä½ $c_t$ä»…ä»…ä¸ç»†8ä½ $f_t$ çš„çŠ¶æ€ $u_t,r_t,e_t,h_t$ ç›¸è¿æ¥ï¼Œè€Œä¸ä¼šåœ¨æ±‚ $c_t$ æ—¶ï¼Œå°† $c_t$ ä½œä¸ºè¾“å…¥ï¼Œå› æ­¤ä¸Šå¼ä¸­é™¤äº†æœ€åä¸¤ä¸ªä¹‹å¤–ï¼Œå…¶å®éƒ½éœ€è¦è®¡ç®—ä¸¤éã€‚

ä¸Šå›¾ä¸­ï¼Œè¾“å…¥é—¨é¦–å…ˆä»è¾“å‡ºåˆ†å¸ƒ $p(c_t)$ ä¸­é‡‡æ ·å‡º $c_t$ï¼Œ$c_t$ è¢«é‡‡æ ·å‡ºæ¥ä¹‹åï¼Œ$f_t$ æ‰èƒ½è¢«è®¡ç®—å’Œé‡‡æ ·å‡ºæ¥ã€‚æ³¨æ„ï¼Œä¸Šå›¾ä¸­çš„ $c_t$ å¹¶æ²¡æœ‰ä½œä¸º $p(c_t)$ çš„è¾“å…¥

### 2.Sparse WaveRNN

#### 2.1 Weight Sparisification Method

ä¸ºäº†é™ä½æ¨¡å‹å¤§å°å’Œæé«˜ç”Ÿæˆé€Ÿç‡ï¼Œå¿…é¡»å‡å°æ¨¡å‹çš„å‚æ•°é‡ã€‚é€šè¿‡å®éªŒå‘ç°ï¼Œåœ¨åŒæ ·å‚æ•°é‡çš„æƒ…å†µä¸‹ï¼Œå¤§è€Œå‚æ•°â€œç¨€ç–â€çš„æ¨¡å‹è¦æ¯”å°è€Œå‚æ•°â€œå¯†é›†â€çš„æ¨¡å‹æ•ˆæœå¥½å¾—å¤šã€‚æ‰€è°“å‚æ•°â€œç¨€ç–â€æ˜¯æŒ‡åœ¨å‚æ•°çŸ©é˜µä¸­ï¼Œå€¼ä¸º0çš„å‚æ•°æ¯”è¾ƒå¤šï¼ŒWaveRNNé‡‡ç”¨weight pruningæ–¹æ³•å‡å°‘æ¨¡å‹ä¸­çš„éé›¶æƒé‡ã€‚å…·ä½“çš„åšæ³•æ˜¯ï¼Œå¯¹æ¯ä¸€ä¸ªå‚æ•°çŸ©é˜µéƒ½ç»´æŠ¤ä¸€ä¸ªbinary maskçŸ©é˜µï¼Œå¼€å§‹æ—¶äºŒå…ƒæ©è”½çŸ©é˜µçš„å…ƒç´ å€¼å…¨ä¸º1ï¼Œæ¯è®­ç»ƒä¸€æ®µæ—¶é—´(500steps)ï¼Œå°±ä¼šå¯¹å‚æ•°çŸ©é˜µçš„å…ƒç´ è¿›è¡Œæ’åºï¼Œå°†å‚æ•°çŸ©é˜µå€¼æœ€å°çš„ğ‘˜ä¸ªå…ƒç´ å¯¹åº”çš„maskç½®ä¸º0ã€‚ğ‘˜çš„è®¡ç®—éœ€è¦è€ƒè™‘æƒ³è¦çš„ç¨€ç–åº¦ğ‘å’Œå‚æ•°çŸ©é˜µä¸­çš„å…ƒç´ æ€»é‡ï¼Œå› æ­¤æƒ³è¦è®¡ç®—ä¸€ä¸ªæ¯”ä¾‹ğ‘§ï¼Œè¿™ä¸ªæ¯”ä¾‹ğ‘§ä¹˜ä¸Šå‚æ•°æ€»é‡å°±æ˜¯ğ‘˜çš„å€¼ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»0å¼€å§‹é€æ¸å¢åŠ åˆ°ç›®æ ‡ç¨€ç–åº¦ğ‘ï¼Œæ¯”ä¾‹ç³»æ•°ğ‘§çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š  
$$ z=Z(1-(1-\frac{t-t_0}{S})^3)$$ 
å…¶ä¸­ï¼Œğ‘¡ä¸ºæ­¤æ—¶çš„è®­ç»ƒæ­¥æ•°ï¼Œ$t_0$ ä¸ºå‚æ•°è£å‰ªå¼€å§‹æ—¶çš„è®­ç»ƒæ­¥æ•°ï¼Œğ‘†æ˜¯æ€»çš„è£å‰ªæ­¥æ•°ã€‚åœ¨è¯¥æ–‡çš„å®éªŒä¸­ï¼Œè®¾ç½® $ğ‘¡_0=1000,ğ‘†=200ğ‘˜,$ å…±è®­ç»ƒ500ğ‘˜æ­¥ã€‚æ–‡ä¸­ä½¿ç”¨è¯¥ç­–ç•¥ç¨€ç–GRUå•å…ƒçš„3ä¸ªé—¨çŸ©é˜µã€‚  
 
#### 2.2 Structured Sparsity

æˆ‘ä»¬æ¢ç´¢ç»“æ„åŒ–ç¨€ç–æ˜¯ä¸ºäº†å‡å°‘å†…å­˜å¼€é”€,å¯ä»¥é€šè¿‡ç¼–ç ç¨€ç–çŸ©é˜µçš„æ–¹æ³•æé«˜è®¡ç®—æ•ˆç‡ã€‚ä½œè€…ä½¿ç”¨4x4 blockä½œä¸ºå•ä½æ¥å‹ç¼©çŸ©é˜µï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒåŸæœ‰çš„å‚æ•°çŸ©é˜µè¢«å‹ç¼©ä¸ºå¤šä¸ª4x4çŸ©é˜µï¼Œè¿™æ ·binary maskçš„å¤§å°å°±å¯ä»¥ç¼©å°ä¸ºåŸæ¥çš„åå…­åˆ†ä¹‹ä¸€ï¼Œä¸”ä¸ä¼šæŸå¤±è¡¨ç°ã€‚ä½œè€…å°è¯•çš„å¦ä¸€ä¸ªç»“æ„æ˜¯16x1 blockï¼Œè¿™ç§å‹ç¼©æ–¹æ³•åªéœ€è¦ä¸€ä¸ªdot productå°±å¯ä»¥å¾—åˆ°activation valueï¼Œå› æ­¤åœ¨æ•ˆç‡ä¸Šè¡¨ç°æ›´å¥½ã€‚  

#### 2.3 Sparse WaveRNN Sampling on Mobile CPU

weights converted 16-bit floating point to 32-bit floating point.The low memory overhead afforded by small blocks allows the sparse matrix-vector products to match the performance of dense matrix-vector products with the same parameter count.  

### 3.Subscale WaveRNN

![](pic/1616038144148-d423497a-7494-4b19-adb0-d79a1e803f8a.png)

å¦‚æœå¯ä»¥ä¸€æ¬¡åŒæ—¶ç”Ÿæˆå¤šä¸ªæ ·æœ¬ç‚¹ï¼Œå°±å¯ä»¥â€œå‡å°â€åºåˆ—é•¿åº¦ï¼Œå°±æ˜¯ä¸€ä¸ªå¹¶è¡Œç”Ÿæˆçš„æ¦‚å¿µã€‚è¿™é‡Œä¸€æ¬¡ç”Ÿæˆğµä¸ªæ ·æœ¬ç‚¹ï¼Œå› æ­¤åºåˆ—é•¿åº¦å°±â€œç¼©å°â€äº†ğµå€  

$$ T(u)=\frac{|u|}{B}\sum_{i=1}^N(c(op_i^B)+d(op_i^B))$$ 

![](pic/1616047307199-72b49409-1267-460f-9b88-2aec2fb12516.png)

**Step 1**: é¦–å…ˆ æŠŠä¸€ä¸ªå°ºåº¦ä¸ºLçš„å¼ é‡æŠ˜å æˆL/B å¤§å°çš„B ä¸ªå¼ é‡ã€‚ ä»¥ä¸Šå›¾ä¸ºä¾‹ï¼Œ å°±æ˜¯æŠŠæ•°åˆ—ï¼Œ 1ï¼Œ2ï¼Œ3ï¼Œ4,...128 è¿™ä¹ˆé•¿çš„æ•°åˆ—æŠ˜å æˆäº†8 ä»½,æ¯ä¸€ä»½éƒ½æ˜¯16 è¿™ä¹ˆé•¿çš„å¼ é‡ã€‚ è¿™é‡Œå«å­è§„æ¨¡ç”Ÿæˆçš„åŸå› æ˜¯æœ¬æ¥ç”Ÿæˆçš„æ•°å­—æ˜¯1,2,3,4,5,6... è¿™äº›ã€‚ä½†æ˜¯æŠ˜å ä»¥åç”Ÿæˆçš„æ˜¯1,9,17,25,è¿™æ ·çš„æ•°ç»„ï¼Œ ç›¸å½“äºæŠŠåŸæ•°åˆ—é™é‡‡æ ·ï¼Œå³sub-scale sampling.  

**Step 2**: è§£é‡Šäº†å£°éŸ³ç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥åŠç›¸å¯¹çš„ä¾èµ–å…³ç³»(conditioning)ã€‚è¿™é‡Œå‡è®¾æ¯ä¸ªå­å¼ é‡çš„ç”Ÿæˆéœ€è¦è¾ƒå°çš„å‰ç»æ€§ã€‚ é¦–å…ˆç”Ÿæˆçš„æ˜¯ç¬¬å…«è¡Œï¼Œ å³ 1ï¼Œ9ï¼Œ17ï¼Œ25ï¼Œ33 ...è¿™åˆ—æ•°ç»„ã€‚ è¿™åˆ—æ•°ç»„ä¼šå…ˆç”Ÿæˆï¼Œ ç„¶åç”Ÿæˆçš„æ˜¯ç¬¬ä¸ƒè¡Œæ•°ç»„ï¼Œ ä¾æ¬¡ç”Ÿæˆã€‚ ä½†æ˜¯ç¬¬ä¸ƒè¡Œæ•°ç»„çš„ç”Ÿæˆä¸ä»…è€ƒè™‘ç¬¬å…«è¡Œå’Œç¬¬ä¸ƒè¡Œå½“å‰æ—¶é—´å·²ç»ç”Ÿæˆçš„æ•°ç»„ï¼Œè¿˜è¦çœ‹ç¬¬å…«è¡Œæ¨ªè½´æœªæ¥ç”Ÿæˆçš„æ•°ç»„ã€‚ æŒ‰ç…§åŸè®ºæ–‡ï¼Œ è¿™é‡Œä¸»è¦è§£é‡Šç¬¬äº”è¡Œçº¢è‰²æ•°å­—ï¼Œ76 çš„ç”Ÿæˆã€‚ 76 çš„ç”Ÿæˆï¼Œ éœ€è¦è€ƒè™‘å½“å‰æ—¶åˆ»ä¹‹å‰çš„æ‰€æœ‰æ•°å­—ï¼Œ å³ç¬¬å…«è¡Œï¼Œ ç¬¬ä¸ƒè¡Œï¼Œ ç¬¬å…­è¡Œï¼Œ ç¬¬äº”è¡Œï¼Œ 1-9åˆ—è“è‰²æ ‡æ³¨çš„æ•°å€¼ï¼Œè¿˜è¦è€ƒè™‘æœªæ¥ï¼Œ 10-13åˆ—äº§ç”Ÿçš„æ•°å€¼ï¼Œ å³å‰ç»æ€§ä¸ºF=4.  

**Step 3**:Â  è§£é‡Šäº†å¹¶è¡Œç”Ÿæˆçš„è¿‡ç¨‹ã€‚ ä¾ç…§ä¸Šå›¾ï¼Œ å°±æ˜¯çº¢è‰²æ ‡æ³¨çš„37ï¼Œ 76ï¼Œ 107 æ˜¯å¯ä»¥åŒæ—¶å¹¶åˆ—ç”Ÿæˆã€‚ä»–çš„å‰ç»æ€§éƒ½æ˜¯4ï¼Œ å¦‚step 2 ä¸­çš„è§£é‡Šã€‚  

**Step 4**: æœ€åä¸€æ­¥å°±æ˜¯æŠŠç”Ÿæˆçš„çŸ©é˜µï¼Œå†å±•å¼€å˜æˆåŸæ¥çš„æ•°åˆ—ï¼Œ 1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œ5...è¿™æ ·çš„è¯å°±å®ç°äº†é«˜åº¦å¹¶è¡Œå¿«é€Ÿç”Ÿæˆè¯­éŸ³çš„ç›®çš„ã€‚æŒ‰ç…§ä¸Šå›¾çš„è§£é‡Šï¼Œå¦‚æœæƒ³ç”Ÿæˆ 24kHz çš„å£°éŸ³ï¼Œ æ¯ä¸€ä¸ªå­è§„æ¨¡çš„ç”Ÿæˆé€Ÿåº¦å°±æ˜¯24/16= 1.5kHz, å¦‚æœB = 16.