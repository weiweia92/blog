{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ec6a54-f823-4494-8d8d-a28de9d539cf",
   "metadata": {},
   "source": [
    "## Tensorflow 中如何指定GPU设备、分配显存？\n",
    "\n",
    "Tensorflow 如何为运算(operations)分配设备(GPU, CPU),如何手动改变设备分配编排？\n",
    "\n",
    "### 指定使用哪块GPU卡\n",
    "\n",
    "#### Tensorflow默认分配\n",
    "\n",
    "如果TensorFlow中的一个运算(operations)同时具有CPU、GPU两种实现，TensorFlow将会优先为其分配GPU设备。\n",
    "\n",
    "#### tf.device(device_name)设置\n",
    "\n",
    "通过设置tf.device(device_name)，指定使用哪个设备。当指定的设备不存在时，可通过设置allow_soft_placement =True来让TensorFlow自动选择一个可用的设备来运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50318bd-1650-4ecc-a986-20ddedd74901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "with tf.device('/device:GPU:2'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    \n",
    "c = tf.matmul(a, b) # 未指定设备，将有限在GPU上执行\n",
    "\n",
    "# Creates a session with log_device_placement set to True.\n",
    "# 当指定的设备不存在时，可通过设置allow_soft_placement =True来让TensorFlow自动选择一个可用的设备来运行。\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True，allow_soft_placement=True))\n",
    "\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e99614-6161-42a0-92da-9c19da49796d",
   "metadata": {},
   "source": [
    "#### 在终端执行程序时指定GPU\n",
    "\n",
    "`CUDA_VISIBLE_DEVICES=1   python  your_file.py`\n",
    "\n",
    "这样在跑网络之前，告诉程序只能看到1号GPU，其他的GPU它不可见\n",
    "\n",
    "可用的形式如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a687c-ab3a-4863-99e0-a1feec1da4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=1           Only device 1 will be seen\n",
    "CUDA_VISIBLE_DEVICES=0,1         Devices 0 and 1 will be visible\n",
    "CUDA_VISIBLE_DEVICES=\"0,1\"       Same as above, quotation marks are optional\n",
    "CUDA_VISIBLE_DEVICES=0,2,3       Devices 0, 2, 3 will be visible; device 1 is masked\n",
    "CUDA_VISIBLE_DEVICES=\"\"          No GPU will be visible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b591c8-6299-4e10-a162-b647f5b857b7",
   "metadata": {},
   "source": [
    "#### 在python 代码中指定GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41a580-9170-44d9-9c0f-ebc0ef55b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1793eb2-8bed-480d-88c8-7b22eefca139",
   "metadata": {},
   "source": [
    "### 设置最小的GPU使用量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c330646-d67b-49d8-8bbd-f2242223635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式一：\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config, ...) # 使用allow_growth option，刚一开始分配少量的GPU容量，然后按需慢慢的增加。注意不会释放内存，因为会导致更严重的内存碎片\n",
    "\n",
    "# 方式二：\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5239535-1e7b-4045-9108-052465284641",
   "metadata": {},
   "source": [
    "### 设置定量的GPU使用量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346164d-6fcd-4668-8508-7780df766564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式一：\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4 # 仅分配每块GPU卡40%的显存供使用，避免资源被独占\n",
    "session = tf.Session(config=config, ...)\n",
    "\n",
    "# 方式二：\n",
    "gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "session = tf.Session(config=config, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08aee72-5973-4c66-92f2-57e8976fc33f",
   "metadata": {},
   "source": [
    "### 查看设备GPU的可用状态\n",
    "\n",
    "#### shell命令方式\n",
    "\n",
    "`nvidia-smi`: NVIDIA System Management Interface\n",
    "\n",
    "#### python接口方式\n",
    "\n",
    "基于C语言的底层函数库，提供python封装的接口(http://pypi.python.org/pypi/nvidia-ml-py/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261a68f-c2a5-451a-b9f5-0d3391ad6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nvmlDeviceGetCount()\n",
    "except NVMLError as error:\n",
    "    print(error)\n",
    "    \n",
    ">>> nvmlInit()\n",
    ">>> handle = nvmlDeviceGetHandleByIndex(0)\n",
    ">>> (current, pending) = nvmlDeviceGetEccMode(handle)\n",
    "\n",
    ">>> info = nvmlDeviceGetMemoryInfo(handle)\n",
    ">>> print \"Total memory:\", info.total\n",
    "Total memory: 5636292608\n",
    ">>> print \"Free memory:\", info.free\n",
    "Free memory: 5578420224\n",
    ">>> print \"Used memory:\", info.used\n",
    "Used memory: 57872384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b856aa-e6a5-4464-a41a-48f2054fd1ea",
   "metadata": {},
   "source": [
    "### 小结\n",
    "\n",
    "通过with语句指定使用哪个GPU，存在一些问题：\n",
    "\n",
    "* 在写训练脚本时怎么知道哪个GPU是空闲可用的？\n",
    "\n",
    "* 同组的人做实验和我冲突怎么办？\n",
    "\n",
    "* 将来某个时刻运行这个脚本的时候是不是还要根据情况修改？\n",
    "\n",
    "* 同行用我的代码复现实验，GPU配置环境不一样，他们甚至可能没有GPU，又要改代码？\n",
    "\n",
    "通过nvidia-smi（ NVIDIA System Management Interface）命令可以查询显卡信息，查看GPU显存、温度、功率使用，然后选择合适的GPU。每次训练前执行这个命令，再与团队保持良好的沟通可以解决上述1、2两个问题，但是3、4两个问题还是不好解决。\n",
    "\n",
    "因此，**需要一种解决方案，能够实现不修改脚本、不需要和组员沟通，自动选择空闲GPU设备。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
