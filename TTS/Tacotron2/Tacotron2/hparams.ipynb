{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc70b8df-4452-45fc-a9cd-bc258c337574",
   "metadata": {},
   "source": [
    "代码来自 https://github.com/Rayhane-mamah/Tacotron-2/blob/master/hparams.py\n",
    "\n",
    "## Detail explanation for hyper-parameters of Tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770debe9-a261-4092-ae9c-0ef55aa546ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "hparams = tf.contrib.training.HParams(\n",
    "        # Comma-separated list of cleaners to run on text prior to training and eval. For non-English\n",
    "        # text, you may want to use \"basic_cleaners\" or \"transliteration_cleaners\".)\n",
    "        cleaners='english_cleaners',"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad47469-9a7d-4a5e-abde-512fba15a228",
   "metadata": {},
   "source": [
    "If you only have 1 GPU or want to use only one GPU, please set num_gpus=0 and specify the GPU idx on run. example:\n",
    "\n",
    "Expample 1 GPU of index 2 (train on \"/gpu2\" only): `CUDA_VISIBLE_DEVICES=2 python train.py --model='Tacotron' --hparams='tacotron_gpu_start_idx=2'`\n",
    "\n",
    "If you want to train on multiple GPUs, simply specify the number of GPUs available, and the idx of the first GPU to use. example:\n",
    "\n",
    "Example 4 GPUs starting from index 0 (train on \"/gpu0\"->\"/gpu3\"):`python train.py --model='Tacotron' --hparams='tacotron_num_gpus=4, tacotron_gpu_start_idx=0'`\n",
    "\n",
    "The hparams arguments can be directly modified on this hparams.py file instead of being specified on run if preferred!\n",
    "\n",
    "If one wants to train both Tacotron and WaveNet in parallel (provided WaveNet will be trained on True mel spectrograms), one needs to specify different GPU idxes.\n",
    "\n",
    "Example Tacotron+WaveNet on a machine with 4 or more GPUs. Two GPUs for each model: \n",
    "\n",
    "`CUDA_VISIBLE_DEVICES=0,1 python train.py --model='Tacotron' --hparams='tacotron_num_gpus=2'`\n",
    "\n",
    "`Cuda_VISIBLE_DEVICES=2,3 python train.py --model='WaveNet' --hparams='wavenet_num_gpus=2'`\n",
    "\n",
    "**IMPORTANT NOTES**\n",
    "\n",
    "The Multi-GPU performance highly depends on your hardware and optimal parameters change between rigs. Default are optimized for servers. If using N GPUs, please multiply the `tacotron_batch_size` by N below in the hparams! (`tacotron_batch_size = 32 * N`). Never use lower batch size than 32 on a single GPU!\n",
    "\n",
    "Same applies for Wavenet: `wavenet_batch_size = 8 * N`(`wavenet_batch_size` can be smaller than 8 if GPU is having OOM, minimum 2)\n",
    "\n",
    "Please also apply the synthesis batch size modification likewise. (if N GPUs are used for synthesis, minimal batch size must be N, minimum of 1 sample per GPU)\n",
    "\n",
    "We did not add an automatic multi-GPU batch size computation to avoid confusion in the user's mind and to provide more control to the user for resources related decisions.\n",
    "\n",
    "**Acknowledgement:**\n",
    "\n",
    "Many thanks to @MlWoo for his awesome work on multi-GPU Tacotron which showed to work a little faster than the original pipeline for a single GPU as well. Great work!\n",
    "\n",
    "Hardware setup: Default supposes user has only one GPU: \"/gpu:0\" (Both Tacotron and WaveNet can be trained on multi-GPU: data parallelization)     \n",
    "Synthesis also uses the following hardware parameters for multi-GPU parallel synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1c8aa-24a5-4f0d-8716-279bac9a7608",
   "metadata": {},
   "outputs": [],
   "source": [
    "tacotron_num_gpus = 1, #Determines the number of gpus in use for Tacotron training.\n",
    "wavenet_num_gpus = 1,  #Determines the number of gpus in use for WaveNet training.\n",
    "split_on_cpu = True,   #Determines whether to split data on CPU or on first GPU. This is automatically True when more than 1 GPU is used. \n",
    "#(Recommend: False on slow CPUs/Disks, True otherwise for small speed boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca350217-a9af-4716-9614-b12ce8d3cd3e",
   "metadata": {},
   "source": [
    "### Audio parameters\n",
    "\n",
    "Audio parameters are the most important parameters to tune when using this work on your personal data. Below are the beginner steps to adapt this work to your personal data:\n",
    "\n",
    "1. Determine my data sample rate: First you need to determine your audio sample_rate (how many samples are in a second of audio). This can be done using sox: `sox --i <filename>`\n",
    "    (For this small tuto, I will consider 24kHz (24000 Hz), and defaults are 22050Hz, so there are plenty of examples to refer to)\n",
    "    \n",
    "2. set sample_rate parameter to your data correct sample rate\n",
    "\n",
    "3. Fix win_size and hop_size accordingly: (Supposing you will follow our advice: 50ms window_size, and 12.5ms frame_shift(hop_size))\n",
    "     \n",
    "     - win_size = 0.05 * sample_rate. In the tuto example, 0.05 * 24000 = 1200\n",
    "     \n",
    "     - hop_size = 0.25 * win_size. Also equal to 0.0125 * sample_rate. In the tuto example, 0.25 * 1200 = 0.0125 * 24000 = 300 (Can set frame_shift_ms=12.5 instead)\n",
    "\n",
    "4. Fix n_fft, num_freq and upsample_scales parameters accordingly.\n",
    "     \n",
    "     - n_fft can be either equal to win_size or the first power of 2 that comes after win_size. I usually recommend using the latter to be more consistent with signal processing friends. No big difference to be seen however. For the tuto example: n_fft = 2048 = $2^{11}$       \n",
    "     \n",
    "     - num_freq = (n_fft / 2) + 1. For the tuto example: num_freq = 2048 / 2 + 1 = 1024 + 1 = 1025.        \n",
    "     \n",
    "     - For WaveNet, upsample_scales products must be equal to hop_size. For the tuto example: upsample_scales=[15, 20] where 15 * 20 = 300 it is also possible to use upsample_scales=[3, 4, 5, 5] instead. One must only keep in mind that upsample_kernel_size[0] = 2\\*upsample_scales[0] so the training segments should be long enough (2.8~3x upsample_scales[0] * hop_size or longer) so that the first kernel size can see the middle  of the samples efficiently. The length of WaveNet training segments is under the parameter \"max_time_steps\".\n",
    "     \n",
    "     - Finally comes the silence trimming. This very much data dependent, so I suggest trying preprocessing (or part of it, ctrl-C to stop), then use the .ipynb provided in the repo to listen to some inverted mel/linear spectrograms. That will first give you some idea about your above parameters, and it will also give you an idea about trimming. If silences persist, try reducing trim_top_db slowly. If samples are trimmed mid words, try increasing it.      \n",
    "     \n",
    "     - If audio quality is too metallic or fragmented (or if linear spectrogram plots are showing black silent regions on top), then restart from step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8add0-1b9c-4039-a9be-b23bfeb5a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mels = 80,   #Number of mel-spectrogram channels and local conditioning dimensionality\n",
    "num_freq = 1025, # (= n_fft / 2 + 1) only used when adding linear spectrograms post processing network\n",
    "rescale = True,  #Whether to rescale audio prior to preprocessing\n",
    "rescaling_max = 0.999, #Rescaling value\n",
    "\n",
    "#train samples of lengths between 3sec and 14sec are more than enough to make a model capable of generating consistent(持久) speech.\n",
    "clip_mels_length = True, #For cases of OOM (Not really recommended, only use if facing unsolvable OOM errors, also consider clipping your samples to smaller chunks)\n",
    "max_mel_frames = 900,    #Only relevant when clip_mels_length = True, please only use after trying output_per_steps=3 and still getting OOM errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb452c9c-8c2a-4c16-939f-a6b0a299b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LWS (https://github.com/Jonathan-LeRoux/lws) for STFT and phase reconstruction  Fast spectrogram phase recovery using Local Weighted Sums (LWS)\n",
    "# It's preferred to set True to use with https://github.com/r9y9/wavenet_vocoder\n",
    "# Does not work if n_ffit is not multiple of hop_size!!\n",
    "use_lws=False,       #Only used to set as True if using WaveNet, no difference in performance is observed in either cases.\n",
    "silence_threshold=2, #silence threshold used for sound trimming for wavenet preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9590e-89f0-4c1f-a740-80e13424382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mel spectrogram\n",
    "n_fft = 2048, #Extra window size is filled with 0 paddings to match this parameter\n",
    "hop_size = 275, #For 22050Hz, 275 ~= 12.5 ms (0.0125 * sample_rate)\n",
    "win_size = 1100, #For 22050Hz, 1100 ~= 50 ms (If None, win_size = n_fft) (0.05 * sample_rate)\n",
    "sample_rate = 22050, #22050 Hz (corresponding to ljspeech dataset) (sox --i <filename>)\n",
    "frame_shift_ms = None, #Can replace hop_size parameter. (Recommended: 12.5)\n",
    "magnitude_power = 2., #The power of the spectrogram magnitude (1. for energy, 2. for power)\n",
    "\n",
    "#M-AILABS (and other datasets) trim params (there parameters are usually correct for any data, but definitely must be tuned for specific speakers)\n",
    "trim_silence = True,  #Whether to clip silence in Audio (at beginning and end of audio only, not the middle)\n",
    "trim_fft_size = 2048, #Trimming window size\n",
    "trim_hop_size = 512,  #Trimmin hop length\n",
    "trim_top_db = 40,     #Trimming db difference from reference db (smaller==harder trim.)\n",
    "\n",
    "#Mel and Linear spectrograms normalization/scaling and clipping\n",
    "signal_normalization = True, #Whether to normalize mel spectrograms to some predefined(预定) range (following below parameters)\n",
    "allow_clipping_in_normalization = True, #Only relevant if mel_normalization = True\n",
    "symmetric_mels = True, #Whether to scale the data to be symmetric around 0. (还将输出范围乘以 2，收敛更快更干净)\n",
    "max_abs_value = 4.,    #max absolute value of data. If symmetric, data will be [-max, max] else [0, max] (Must not be too big to avoid gradient explosion)\n",
    "\n",
    "normalize_for_wavenet = True, #whether to rescale to [0, 1] for wavenet. (better audio quality)\n",
    "clip_for_wavenet = True,      #whether to clip [-max, max] before training/synthesizing with wavenet (better audio quality)\n",
    "wavenet_pad_sides = 1,        #Can be 1 or 2. 1 for pad right only, 2 for both sides padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52a088-a22a-4fd5-b5d4-debb4ae57e4d",
   "metadata": {},
   "source": [
    "Contribution by @begeekmyfriend\n",
    "\n",
    "Spectrogram Pre-Emphasis (Lfilter: Reduce spectrogram noise and helps model certitude levels. Also allows for better G&L phase reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd8df4-c059-4aa6-b594-f56499ba4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "preemphasize = True, #whether to apply filter\n",
    "preemphasis = 0.97,  #filter coefficient\n",
    "\n",
    "#Griffin Lim\n",
    "power = 1.5, #Only used in G&L inversion, usually values between 1.2 and 1.5 are a good choice.\n",
    "griffin_lim_iters = 60, #Number of G&L iterations, typically 30 is enough but we use 60 to ensure convergence.\n",
    "GL_on_GPU = True,       #Whether to use G&L GPU version as part of tensorflow graph. (Usually much faster than CPU but slightly worse quality too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24868221-7afd-4c22-be78-39b166d39f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tacotron\n",
    "#Model general type\n",
    "outputs_per_step = 1,    #number of frames to generate at each decoding step (increase to speed up computation and allows for higher batch size, decreases G&L audio quality)\n",
    "stop_at_any = True,      #Determines whether the decoder should stop when predicting <stop> to any frame or to all of them (True works pretty well)\n",
    "batch_norm_position = 'after', #Can be in ('before', 'after'). Determines whether we use batch norm before or after the activation function (relu). Matter for debate.\n",
    "clip_outputs = True,     #Whether to clip spectrograms to T2_output_range (even in loss computation). ie: Don't penalize model for exceeding output range and bring back to borders.\n",
    "lower_bound_decay = 0.1, #Small regularizer for noise synthesis by adding small range of penalty for silence regions. Set to 0 to clip in Tacotron range.\n",
    "\n",
    "#Input parameters\n",
    "embedding_dim = 512,     #dimension of embedding space\n",
    "\n",
    "#Encoder parameters\n",
    "enc_conv_num_layers = 3,  #number of encoder convolutional layers\n",
    "enc_conv_kernel_size = (5, ), #size of encoder convolution filters for each layer\n",
    "enc_conv_channels = 512,   #number of encoder convolutions filters for each layer\n",
    "encoder_lstm_units = 256,  #number of lstm units for each direction (forward and backward)\n",
    "\n",
    "#Attention mechanism\n",
    "smoothing = False,         #Whether to smooth the attention normalization function\n",
    "attention_dim = 128,       #dimension of attention space\n",
    "attention_filters = 32,    #number of attention convolution filters\n",
    "attention_kernel = (31, ), #kernel size of attention convolution\n",
    "cumulative_weights = True, #Whether to cumulate (sum) all previous attention weights or simply feed previous weights (Recommended: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a88fb4-8cf2-4b8e-9f6b-8d9a849bd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention synthesis constraints\n",
    "#\"Monotonic\" constraint forces the model to only look at the forwards attention_win_size steps.\n",
    "#\"Window\" allows the model to look at attention_win_size neighbors, both forward and backward steps.\n",
    "synthesis_constraint = False,  #Whether to use attention windows constraints in synthesis only (Useful for long utterances synthesis)\n",
    "synthesis_constraint_type = 'window', #can be in ('window', 'monotonic'). \n",
    "attention_win_size = 7, #Side of the window. Current step does not count. If mode is window and attention_win_size is not pair, the 1 extra is provided to backward part of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ecc0c5-38ee-4542-b927-346376789ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "prenet_layers = [256, 256], #number of layers and number of units of prenet\n",
    "decoder_layers = 2,         #number of decoder lstm layers\n",
    "decoder_lstm_units = 1024,  #number of decoder lstm units on each layer\n",
    "max_iters = 10000,          #Max decoder steps during inference (Just for safety from infinite loop cases)\n",
    "\n",
    "#Residual postnet\n",
    "postnet_num_layers = 5,      #number of postnet convolutional layers\n",
    "postnet_kernel_size = (5, ), #size of postnet convolution filters for each layer\n",
    "postnet_channels = 512,      #number of postnet convolution filters for each layer\n",
    "\n",
    "#CBHG mel->linear postnet\n",
    "cbhg_kernels = 8,                #All kernel sizes from 1 to cbhg_kernels will be used in the convolution bank of CBHG to act as \"K-grams\"\n",
    "cbhg_conv_channels = 128,        #Channels of the convolution bank\n",
    "cbhg_pool_size = 2,              #pooling size of the CBHG\n",
    "cbhg_projection = 256,           #projection channels of the CBHG (1st projection, 2nd is automatically set to num_mels)\n",
    "cbhg_projection_kernel_size = 3, #kernel_size of the CBHG projections\n",
    "cbhg_highwaynet_layers = 4,      #Number of HighwayNet layers\n",
    "cbhg_highway_units = 128,        #Number of units used in HighwayNet fully connected layers\n",
    "cbhg_rnn_units = 128,            #Number of GRU units used in bidirectional RNN of CBHG block. CBHG output is 2x rnn_units in shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944cd8d-770e-4545-b49a-144e279b7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss params\n",
    "mask_encoder = True,          #whether to mask encoder padding while computing attention. Set to True for better prosody but slower convergence.\n",
    "mask_decoder = False,         #Whether to use loss mask for padded sequences (if False, <stop_token> loss function will not be weighted, else recommended pos_weight = 20)\n",
    "cross_entropy_pos_weight = 1, #Use class weights to reduce the stop token classes imbalance (by adding more penalty on False Negatives (FN)) (1 = disabled)\n",
    "predict_linear = True,        #Whether to add a post-processing network to the Tacotron to predict linear spectrograms (True mode Not tested!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d26ccd4-ae27-4a4a-8082-a0a1e890d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tacotron Training\n",
    "#Reproduction seeds\n",
    "tacotron_random_seed = 5339,         #Determines initial graph and operations (i.e: model) random state for reproducibility\n",
    "tacotron_data_random_state = 1234,   #random state for train test split repeatability\n",
    "\n",
    "#performance parameters\n",
    "tacotron_swap_with_cpu = False,      #Whether to use cpu as support to gpu for decoder computation (Not recommended: may cause major slowdowns! Only use when critical!)\n",
    "\n",
    "#train/test split ratios, mini-batches sizes\n",
    "tacotron_batch_size = 32,            #number of training samples on each training steps\n",
    "#Tacotron Batch synthesis supports ~16x the training batch size (no gradients during testing). \n",
    "#Training Tacotron with unmasked paddings makes it aware of them, which makes synthesis times different from training. We thus recommend masking the encoder.\n",
    "tacotron_synthesis_batch_size = 1,   #DO NOT MAKE THIS BIGGER THAN 1 IF YOU DIDN'T TRAIN TACOTRON WITH \"mask_encoder=True\"!!\n",
    "tacotron_test_size = 0.05,           #% of data to keep as test data, if None, tacotron_test_batches must be not None. (5% is enough to have a good idea about overfit)\n",
    "tacotron_test_batches = None,        #number of test batches.\n",
    "\n",
    "#Learning rate schedule\n",
    "tacotron_decay_learning_rate = True,      #boolean, determines if the learning rate will follow an exponential decay\n",
    "tacotron_start_decay = 40000,             #Step at which learning decay starts\n",
    "tacotron_decay_steps = 18000,             #Determines the learning rate decay slope (UNDER TEST)\n",
    "tacotron_decay_rate = 0.5,                #learning rate decay rate (UNDER TEST)\n",
    "tacotron_initial_learning_rate = 1e-3,    #starting learning rate\n",
    "tacotron_final_learning_rate = 1e-4,      #minimal learning rate\n",
    "\n",
    "#Optimization parameters\n",
    "tacotron_adam_beta1 = 0.9,                #AdamOptimizer beta1 parameter\n",
    "tacotron_adam_beta2 = 0.999,              #AdamOptimizer beta2 parameter\n",
    "tacotron_adam_epsilon = 1e-6,             #AdamOptimizer Epsilon parameter\n",
    "\n",
    "#Regularization parameters\n",
    "tacotron_reg_weight = 1e-6,               #regularization weight (for L2 regularization)\n",
    "tacotron_scale_regularization = False,    #Whether to rescale regularization weight to adapt for outputs range (used when reg_weight is high and biasing the model)\n",
    "tacotron_zoneout_rate = 0.1,              #zoneout rate for all LSTM cells in the network\n",
    "tacotron_dropout_rate = 0.5,              #dropout rate for all convolutional layers + prenet\n",
    "tacotron_clip_gradients = True,           #whether to clip gradients\n",
    "\n",
    "#Evaluation parameters\n",
    "#Whether to use 100% natural eval (to evaluate Curriculum Learning performance) or with same teacher-forcing ratio as in training (just for overfit)\n",
    "tacotron_natural_eval = False,            \n",
    "#Decoder RNN learning can take be done in one of two ways:\n",
    "#Teacher Forcing: vanilla teacher forcing (usually with ratio = 1). mode='constant'\n",
    "#Scheduled Sampling Scheme: From Teacher-Forcing to sampling from previous outputs is function of global step. (teacher forcing ratio decay) mode='scheduled'\n",
    "#The second approach is inspired by:\n",
    "#Bengio et al. 2015: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.\n",
    "#Can be found under: https://arxiv.org/pdf/1506.03099.pdf\n",
    "tacotron_teacher_forcing_mode = 'constant', #Can be ('constant' or 'scheduled'). 'scheduled' mode applies a cosine teacher forcing ratio decay. (Preference: scheduled)\n",
    "tacotron_teacher_forcing_ratio = 1.,        #Value from [0., 1.], 0.=0%, 1.=100%, determines the % of times we force next decoder inputs, Only relevant if mode='constant'\n",
    "tacotron_teacher_forcing_init_ratio = 1.,   #initial teacher forcing ratio. Relevant if mode='scheduled'\n",
    "tacotron_teacher_forcing_final_ratio = 0.,  #final teacher forcing ratio. (Set None to use alpha instead) Relevant if mode='scheduled'\n",
    "tacotron_teacher_forcing_start_decay = 10000, #starting point of teacher forcing ratio decay. Relevant if mode='scheduled'\n",
    "tacotron_teacher_forcing_decay_steps = 40000, #Determines the teacher forcing ratio decay slope. Relevant if mode='scheduled'\n",
    "tacotron_teacher_forcing_decay_alpha = None, #teacher forcing ratio decay rate. Defines the final tfr as a ratio of initial tfr. Relevant if mode='scheduled'\n",
    "\n",
    "#Speaker adaptation parameters\n",
    "tacotron_fine_tuning = False,              #Set to True to freeze encoder and only keep training pretrained decoder. Used for speaker adaptation with small data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8916949-9223-43be-88f4-ffbb317cde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hparams_debug_string():\n",
    "    values = hparams.values()\n",
    "    hp = ['  %s: %s' % (name, values[name]) for name in sorted(values) if name != 'sentences']\n",
    "    return 'Hyperparameters:\\n' + '\\n'.join(hp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
