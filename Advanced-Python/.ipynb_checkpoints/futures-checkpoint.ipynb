{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b53a28-4207-4d86-a0d1-62365c490f40",
   "metadata": {},
   "source": [
    "### CPU-bound(cpu密集型)\n",
    "指I/O在很短的时间就可以完成，CPU需要大量的计算和处理，特点是CPU占用率相当高\n",
    "### I/O-bound(IO密集型)\n",
    "指的是系统运作大部分的状况是CPU在等I/O（硬盘/内存/网络）的读/写操作，CPU占用率仍然较低。  \n",
    "例如：文件处理程序，网络爬虫程序，读写数据库程序  \n",
    "多线程、多进程、多协程的对比  \n",
    "### 1.多进程 multiprocessing\n",
    "优点：可以利用多核CPU并行运算  \n",
    "缺点：占用资源最多、可启动数目比线程少  \n",
    "适用于：CPU密集型计算  \n",
    "### 2.多线程thread(threading)\n",
    "**优点**：相比进程，更轻量级、占用资源少  \n",
    "**缺点**：  \n",
    "相比进程：多线程只能并发执行，不能利用多CPU（GIL）  \n",
    "相比协程：启动数目有限制，占用内存资源，有线程切换开销  \n",
    "适用于：I/O密集型计算、同时运行的任务数目要求不多  \n",
    "### 3.多协程Coroutine(asyncio)\n",
    "优点：内存开销最少，启动协程数量最多\n",
    "缺点：支持的库有限制(aiohttp vs requests)、代码实现复杂\n",
    "适用于：IO密集型计算，需要超多任务运行、但有现成库支持的场景\n",
    "### Python 慢的两大原因\n",
    "1.动态类型语言（要一直检查变量类型），边解释边执行  \n",
    "2.GIL（Global Interpreter Lock）：无法利用多核CPU并发执行  \n",
    "GIL:是计算机程序设计语言解释器用于同步线程的一种机制，它使得任何时刻仅有一个线程在执行。即便在多核处理器上，使用GIL的解释器也只允许同一时间执行一个线程。\n",
    "由于GIL的存在，即时电脑有多核CPU，单个时刻也只能使用1个，相比于并发加速的C++/JAVA所以慢。  \n",
    "GIL的好处：解决了多线程之间数据完整性和状态同步问题。简化了python对共享资源的管理    \n",
    "**怎样规避GIL带来的限制？**  \n",
    "1.多线程`threadin` 机制依然有用，用于I/O密集型计算  \n",
    "因为在I/O期间，线程会释放GIL，实现CPU和IO的并行，因此多线程用于IO密集型计算依然可以大幅度提升速度。但在多线程用于CPU密集型计算时，只会拖慢速度，因为只有一个CPU在运行，同时进行多线程的切换，多线程的切换会带来开销  \n",
    "2.使用`multiprocessing` 的多进程机制实现了并行计算，利用多核CPU优势，为了应对GIL问题，`Python`提供了`multiprocessing`模块  \n",
    "相比于`C/C++/JAVA`，`python`确实慢，在一些特殊场景下，`Python`比`C++`慢100～200倍，由于速度慢的原因，很多公司的基础框架代码依然用C/C++开发。比如各大公司阿里/腾讯/快手的推荐引擎、搜索引擎、存储引擎等底层对性能要求高的模块。\n",
    "### 多线程数据通信的`queue.Queue ` \n",
    "`queue.Queue`可以用于多线程之间、线程安全的数据通信，线程安全是指多个线程并发同时的访问数据不会产生冲突。  \n",
    "```\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "import time\n",
    "import queue\n",
    "\n",
    "urls = [f\"https://www.cnblogs.com/#p{page}\" for page in range(1, 50+1)]\n",
    "\n",
    "def craw(url):\n",
    "  \tr = requests.get(url)\n",
    "  \treturn r.text\n",
    "\n",
    "def parse(html):\n",
    "      soup = BeautifulSoup(html,'lxml')\n",
    "      links = soup.find_all('a', class_='post-item-title')\n",
    "      return [(link['href'], link.get_text()) for link in links]\n",
    "\n",
    "def do_craw(url_queue:queue.Queue, html_queue:queue.Queue):\n",
    "  \twhile True:\n",
    "        url = url_queue.get()  #get url from url_queue\n",
    "        html = craw(url)\n",
    "        html_queue.put(html)   #put html in html_queue\n",
    "        print(threading.current_thread().name, f'craw {url}', 'url_queue.size=', url_queue.qsize())\n",
    "\n",
    "def do_parse(html_queue:queue.Queue, fout):\n",
    "  \twhile True:\n",
    "        html = html_queue.get()\n",
    "        results = parse(html)\n",
    "        for result in results:\n",
    "          fout.write(str(result)+'\\n')\n",
    "        print(threading.current_thread().name, f'results.size',len(results),\n",
    "              'html_queue.size=', html_queue.qsize())\n",
    "\n",
    "def single_thread():\n",
    "  \tfor url in urls:\n",
    "    \tcraw(url)\n",
    "    \n",
    "def multi_thread():\n",
    "  \tthreads = []\n",
    "      for url in urls:\n",
    "        threads.append(threading.Thread(target=craw, args=(url,)))\n",
    "      for thread in threads:\n",
    "        thread.start()\n",
    "      for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "      url_queue = queue.Queue()\n",
    "      html_queue = queue.Queue()\n",
    "      for url in urls:\n",
    "        url.queue.put(url)\n",
    "\n",
    "  #\t启动3个生产者线程\n",
    "  \tfor idx in range(3):\n",
    "        t = threading.Thread(target=do_craw, args=(url_queue,html_queue),name=f'craw{idx}')\n",
    "        t.start()\n",
    "\n",
    "      fout = open('data.txt', 'w')\n",
    "      #启动2个消费者线程\n",
    "      for idx in range(2):\n",
    "            t = threading.Thread(target=do_parse, args=(html_queue,fout),name=f'parse{idx}')\n",
    "            t.start()\n",
    "\n",
    "      start = time.time()\n",
    "      single_thread()\n",
    "      end = time.time()\n",
    "      print('single thread cost:', end-start, 'second')\n",
    "\n",
    "      start = time.time()\n",
    "      multi_thread()\n",
    "      end = time.time()\n",
    "      print('multi thread cost:', end-start, 'second')\n",
    "```\n",
    "**线程安全** ：指某个函数、函数库在多线程环境中被调用时，能够正确处理多个线程之间的共享变量，使程序功能正常完成  \n",
    "**线程不安全**：由于线程的执行随时会发生切换，就造成了不可预料的结果，出现线程不安全。  \n",
    "```\n",
    "import threading\n",
    "import time\n",
    "\n",
    "lock = threading.Lock()\n",
    "class Account:\n",
    "  \tdef __init__(self.balance):\n",
    "    \tself.balance = balance\n",
    "    \n",
    "def draw(account, amount):\n",
    "  \twith lock:\n",
    "    \tif account.balance >= amount:\n",
    "      \ttime.sleep(0.1)\n",
    "      \tprint(threading.current_thread().name, '取钱成功')\n",
    "      \taccount.balance -= amount\n",
    "      \tprint(threading.current_thread().name, '余额', account.balance)\n",
    "    else:\n",
    "      \tprint(threading.current_thread().name, '取钱失败，余额不足')\n",
    "      \n",
    "if __name__=='__main__':\n",
    "  \taccount = Account(1000)\n",
    "  \tta = threading.Thread(name='ta', target=draw, args=(account, 800))\n",
    "  \ttb = threading.Thread(name='tb', target=draw, args=(account, 800))\n",
    "```\n",
    "### 线程池\n",
    "**1.提升性能**：因为减去了大量新建、终止线程的开销，重用了线程资源  \n",
    "**2.使用场景**：适合处理突发性大量请求和需要大量线程完成任务、但实际任务处理时间较短  \n",
    "**3.防御功能**：能有效避免系统因为创建线程过多，而导致系统负荷过大响应变慢等问题  \n",
    "**4.代码优势**：使用线程池的语法比自己新建线程执行线程更加简洁  \n",
    "```\n",
    "import concurrent.futures\n",
    "#craw\n",
    "with concurrent.futures.TheadPoolExecutor() as pool:\n",
    "  \thtmls = pool.map(craw, urls)\n",
    "  \thtmls = list(zip(urls, htmls))\n",
    "  \tfor url, html in htmls:\n",
    "    \tprint(url, len(html))\n",
    "print('craw over')\n",
    "\n",
    "#parse\n",
    "with concurrent.futures.TheadPoolExecutor() as pool:\n",
    "  \tfutures = {}\n",
    "  \tfor url, html in htmls:\n",
    "    \tfuturn = pool.submit(parse, html)\n",
    "    \tfutures[future] = url\n",
    "    \n",
    "  \t#for future, url in futures.items():\n",
    "  \t#  print(url, future.result())\n",
    "  \tfor future in concurrent.futures.as_completed(futures):\n",
    "    \turl = futures[future]\n",
    "    \tprint(url, future.result())\n",
    "```\n",
    "**有了多线程threading,为什么还要用多进程multiprocessing**  \n",
    "multiprocessing模块就是python为了解决GIL缺陷引入的一个模块，原理是用多进程在多CPU上并行执行。  \n",
    "### 多进程\n",
    "```\n",
    "#判断一个数是否为素数\n",
    "import math\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "PRIMES = [2389432] * 100\n",
    "def is_prime(n):\n",
    "  \tif n < 2:\n",
    "    \treturn False\n",
    "  \tif n == 2:\n",
    "    \treturn True\n",
    "  \tif n % 2 == 0:\n",
    "    \treturn False\n",
    "  \tsqrt_n = int(math.floor(math.sqrt(n)))\n",
    "  \tfor i in range(3, sqrt_n + 1, 2):\n",
    "    \tif n % i == 0:\n",
    "      \treturn False\n",
    "  \treturn True\n",
    "\n",
    "def single_thread():\n",
    "  \tfor number in PRIMES:\n",
    "    \tis_prime(number)\n",
    "    \n",
    "def multi_thread():\n",
    "  \twith ThreadPoolExecutor() as pool:\n",
    "    \tpool.map(is_prime, PRIMES)\n",
    "def multi_process():\n",
    "  \twith ProcessPoolExecutor() as pool:\n",
    "    \tpool.map(is_prime, PRIMES)\n",
    "\n",
    "if __name__=='__main__':\n",
    "  \tpass\n",
    "```\n",
    "### python异步IO实现并发爬虫\n",
    "```\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "async def async_craw(url):\n",
    "  \tprint('craw url:', url)\n",
    "  \tasync with aiohttp.ClientSession() as session:\n",
    "    \tasync with session.get(url) as resp:\n",
    "      \tresult = await resp.text()\n",
    "      \tprint(f'craw url:{url},{len(result)}')\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "urls = [f\"https://www.cnblogs.com/#p{page}\" for page in range(1, 50+1)]\n",
    "tasks = [loop.creat_task(async_craw(url)) for url in urls]\n",
    "loop.run_until_complete(asyncio.wait(tasks))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f86ede0-0de7-4347-9ddc-0a924472df30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
