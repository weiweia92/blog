# 算法工程师面试

## 深度学习(CV)

![](pic/cv.png)

- **什么样的数据集不适合深度学习**

1.数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。  
2.数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音素组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

## 分类模型评估方法

### 1. **Accuracy作为指标有哪些局限性？**

当正负样本极度不均衡时存在问题！比如，正样本有99%时，分类器只要将所有样本划分为正样本就可以达到99%的准确率。但显然这个分类器是存在问题的。

当正负样本不均衡时，常用的评价指标为ROC曲线和PR曲线。

### 2. ROC曲线和PR曲线各是什么？

ROC:由 FPR(False positive rate)和TPR(True positive rate)为横纵坐标的曲线  

FPR:FP/(FP+TN)误分为正样本的负样本占所有实际负样本的比例  
TPR:TP/(TP+FN)正确分类的正样本占所有实际正样本的比例  

True Positive, TP: 正确的正样本(预测为正样本，实际也为正样本)  
False Positive, FP:错误的正样本(预测为正样本，实际为负样本)  
True Negative, TN: 正确的负样本(预测为负样本，实际也为负样本)  
False Negative, FN: 错误的负样本(预测为负样本，实际为正样本)  

PR:Precision-Recall 曲线，x:Recall，y:Precision  

Precision=TP/(TP+FP), 正确的正样本占所有预测正样本的比例  
Recall=TP/(TP+FN)， 正确的正样本占所有实际正样本的比例  

ROC越靠左上角效果越好，AUC为横纵坐标围成的面积，越大越好.  

- **编程实现AUC的计算，并指出复杂度？**

```
from sklearn.metrics import roc_auc_score
auc_score = roc_auc_score(y_truth,y_pred)
```

- **AUC指标有什么特点？放缩结果对AUC是否有影响？**

AUC指的是ROC曲线下的面积，介于0和1之间。AUC作为数值可以直观的评价分类器的好坏，值越大越好。放缩结果不影响AUC

### 3.F1 Score

$$\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}$$

- **余弦距离与欧式距离有什么特点？**

### 目标检测(object detection)模型评估方法

- **mAP**  

当pre-bbox和groundtruth bbox的IOU大于某一阈值(通常为0.5)，则认为该预测正确。对每个类别，我们画出它的precision-recall曲线，平均准确率是曲线下的面积。之后再对所有类别的平均准确率求平均，即可得到mAP，其取值为[0, 100%]。


### 5. 什么是偏差和方差？

Bias是用所有可能的训练数据集训练出的所有模型的输出的平均值与真实模型的输出值之间的差异。
Variance是不同的训练数据集训练出的模型输出值之间的差异。

### 6. 什么是过拟合？深度学习解决过拟合的方法有哪些？

过拟合表现在训练好的模型在训练集上效果很好，但是在测试集上效果差。也就是说模型的泛化能力弱.  
过拟合主要两个原因造成，数据太少和模型太复杂  

解决过拟合方法: early stopping，L1 regularization，L2 regularization, dropout          
![](pic/L1.png)            
参数的稀疏，在一定程度实现了特征的选择.  

L2 regularization  
![](pic/L2.png) 

dropout:通过修改隐藏层神经元的个数来防止网络的过拟合，也就是通过修改深度网络本身。在每一批次数据被训练时，Dropout按照给定的概率P随机剔除一些神经元，只有没有被剔除神经元的参数被更新。每一批次数据，由于随机性剔除神经元，使得网络具有一定的稀疏性，从而能减轻了不同特征之间的协同效应。而且由于每次被剔除的神经元不同，所以整个网络神经元的参数也只是部分被更新，消除减弱了神经元间的联合适应性，增强了神经网络的泛化能力和鲁棒性。**Dropout只在训练时使用，作为一个超参数，然而在测试集时，并不能使用**

### 7. dropout原理

Hinton认为过拟合可以通过阻止某些特征的协同作用来缓解．  
模型平均,防止参数过分依赖训练数据，增加参数对数据集的泛化能力  
1.取平均的作用(Bagging通过几个模型降低泛化误差的技术): 先回到正常的模型（没有dropout），我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络（随机删掉一半隐藏神经元导致网络结构已经不同)，整个dropout过程就相当于 对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。  
bagging与dropout的不同点:Bagging所有模型都是独立的(模型之间的参数不会有相互的影响).Dropout所有模型共享参数(每个子模型继承父神经网络的不同子集)

2.减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 （这些特征在其它的神经元的随机子集中也存在）。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。（这个角度看 dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高）

### 8. 解决欠拟合的方法有哪些？

通过增加网络复杂度或者在模型中增加特征  

### 9. 优化方法

**简述了解的优化器，发展综述？**

`BGD,SGD`,动量优化法:`Monentum,NAG(Nesterov accelerated gradient)`,自适应学习率优化法:`Adagrad,RMSprop,AdaDelta,Adam`

### 10. 激活函数总结  

![](pic/activationfunction.png)
![](pic/activationFunction1.png)

深度网络中激活函数的选择对训练过程和任务性能有很大影响。目前，最成功、使用最广泛的激活函数是修正线性单元（Rectified Linear Unit，**ReLU x = max(0,x)**）。尽管出现了很多修正 ReLU 的激活函数，但是无一可以真正替代它。我们提出了一种新型激活函数 **Swish：f(x) = x · sigmoid(x), 其中sigmoid(x)=1/(1+exp(-x))** 。我们在多个难度较高的数据集上进行实验，证明 Swish 在深层模型上的效果优于 ReLU。例如，仅仅使用 Swish 单元替换 ReLU 就能把 Mobile NASNetA 在 ImageNet 上的 top-1 分类准确率提高 0.9%，Inception-ResNet-v 的分类准确率提高 0.6%。Swish 的简洁性及其与 ReLU 的相似性使从业者可以在神经网络中使用 Swish 单元替换 ReLU。不过在 Reddit 论坛上，该激活函数的性能与优点还是有些争议的，有的开发者发现该激活函数很多情况下可以比标准的 ReLU 获得更高的性能，而有些开发者则认为 Swish 激活函数并没有什么新意，我们应该关注于更加基础的研究。  

### 11. swish函数图像:  

![](pic/swish.png)
![](pic/swish1.png)
![](pic/swish2.png)
![](hpic/swish3.png)

我们的实验证明 Swish 在多个深度模型上的性能持续优于或与 ReLU 函数相匹配。由于训练会受多种因素的影响，我们很难证明为什么一个激活函数会优于另一个。但是我们认为 Swish 无上界有下界、非单调且平滑的特性都是优势  

### 12. Mish  

ReLU和Mish的对比，Mish的梯度更平滑  

**Mish=x * tanh(ln(1+e^x))**  

**为什么Mish表现这么好?**  

上无边界:避免了在训练时优于梯度为0导致的饱和。  
有下界:  
非单调:对于Mish来说是很重要的因素．我们保证了在负值区域有梯度值，它能够使网络学习的更好  
连续性:处处可微，可以有效的优化和泛化.  

尽管如此，我测试了许多激活函数，它们也满足了其中的许多想法，但大多数都无法执行。这里的主要区别可能是Mish函数在曲线上几乎所有点上的平滑度(可微)。

这种通过Mish激活曲线平滑性来推送信息的能力如下图所示，在本文的一个简单测试中，越来越多的层被添加到一个测试神经网络中，而没有一个统一的函数。随着层深的增加，ReLU精度迅速下降，其次是Swish。相比之下，Mish能更好地保持准确性，这可能是因为它能更好地传播信息  
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-16%2020-59-27.png)  
更平滑的激活功能允许信息更深入地流动……注意，随着层数的增加，ReLU快速下降。


### 13.常用的损失函数有哪些？分别适用于什么场景？**

$\text{Focal loss}=-{\alpha}_t*(1-p_t)^\gamma \log(p_t)$  

交叉熵损失函数:$-\sum_{i}^{n} {y^i}\log(P_i)$

smoothl1  

MSE  

### 14. 为什么常用交叉熵来作为分类问题的损失函数

从数学上来理解就是，为了让学到的模型分布更接近真实数据的分布，我们需要最小化模型数据分布与训练数据之间的差异，相对熵(又称KL 散度)来衡量这两个分布的差异

交叉熵 $H(p,q)=-\sum_{i=1}^n p(x_i)\text{log}(q(x_i))$

相对熵 $D_{KL}(p||q)=\sum_{i=1}^n p(x_i)\text{log}\frac{p(x_i)}{q(x_i)}=\sum_{i=1}^n p(x_i)\text{log}p(x_i)-\sum_{i=1}^n p(x_i)\text{log}q(x_i)$

相对熵=交叉熵-信息熵

因为训练数据的分布是固定的，所以信息熵固定，因此最小化 KL 散度等价于最小化交叉熵，而且交叉熵计算更简单

### 15.梯度下降与拟牛顿法的异同？

梯度下降用于数据较大情况，缺点:每一步可能不是总向着最优解的方向  
拟牛顿:收敛速度更快，缺点:迭代时间长，需要计算一阶二阶导数  

## 深度学习基础

- **以一层隐层的神经网络，relu激活，MSE作为损失函数推导反向传播**

### 1. NN的权重参数能否初始化为0？

神经网络的权重w 的不同代表输入的向量有不同的特征，即权重越大的特征越重要，比如在人脸识别中，人脸的属性有眼睛，鼻子，嘴巴，眉毛，其中眼睛更能够影响人脸的识别，所以我们给与眼睛更大的权重。如果将权重初始化全为0，那么隐藏层的各个神经元的结果都是一样的，从而正向传播的结果是一样的，反向传播求得的梯度也是一样的，也就是说不管经过多少次迭代，更新的w(i)是相同的，这样就判断不了哪个特征比较重要了。因此，初始w不同，可以学到不同的特征，如果都是0或某个值，由于计算方式相同，可能达不到学习不同特征的目的

### 2.logistic regression初始化参数可以全用0吗?

Logistic回归没有隐藏层。 如果将权重初始化为零，则Logistic回归中的第一个示例x将输出零，但Logistic回归的导数取决于不是零的输入x（因为没有隐藏层）。 因此，在第二次迭代中，如果x不是常量向量，则权值遵循x的分布并且彼此不同。

- **attention机制**

- **sparse dense特征**



- **dropout和BN 在前向传播和后向传播阶段的区别?**


### CNN结构的特点

局部连接，权值共享，池化操作，多层次结构  

1.局部连接使网络可以提取数据的局部特征  
2.权值共享大大降低了网络的训练难度，**一个Filter只提取一个特征**，在整个图片（或者语音／文本） 中进行卷积  
3.池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。  

### 给定卷积核的尺寸，特征图大小计算方法？

假设给定： 1. 输入层宽度W，高度H 2. 卷积层尺寸F 3. 步幅为S 4. 填充边框为P 5. 滤波器数量K  
W_out = [(W−F+2P)/S]+1  
H_out = [(H-F+2P)/S]+1  
D_out = K  

### 卷积层参数量，全连接层参数量，卷积层计算量，全连接层计算量**

卷积神经网络可以学习到模型的空间层次结构。  
卷积层参数量:输入28x28x192(WxHxC,C代表通道数)，然后在3x3的卷积核，卷积通道数为128，那么卷积的参数有(3x3x192+1)x128,注意!卷积核的权值共享只在每个单独通道上有效，至于通道与通道间的对应的卷积核是独立不共享的，所以这里是192x128,+1是因为bias  

全连接层参数量:VGG-16最后一次卷积得到的feature map为7×7×512,全连接层是将feature map展开成一维向量1×4096

### 1\*1卷积层作用

1.降维(dimension reductionality):eg,一张500\*500且厚度depth为100 的图片在20个filter上做1\*1的卷积，那么结果的大小为500\*500\*20,降低参数数量,当然也可以升维，1x1卷积的作用是为了让网络根据需要能够更灵活的控制数据的depth的。  
2.加入非线性。卷积层之后经过激励层，1\*1的卷积在前一层的学习表示上添加了non-linear activation,提升网络的表达能力  
3.跨通道信息交互:使用1x1卷积核，实现降维和升维的操作其实就是channel间信息的线性组合变化，3x3，64channels的卷积核后面添加一个1x1，28channels的卷积核，就变成了3x3，28channels的卷积核，原来的64个channels就可以理解为跨通道线性组合变成了28channels，这就是通道间的信息交互  

### 增大感受野(receptive field)的方法

较大的卷积核尺寸的卷积操作，小卷积的多层叠加，空洞卷积，池化操作  

### 常用的池化操作有哪些?有什么特点?

maxpooling,avgpooling  

### 池化的作用

pool一般放在conv_layer之后,通过池化来降低卷积层输出的特征维度，有效减少网络参数的同时还可以防止过拟合现象,增加特征平移不变性。**池化类似于PCA有效的对数据降维的同时保留关键特征**.pooling可以提高网络对微小位移的容忍能力，提升模型尺度不变性,旋转不变性,抑制噪声降低信息冗余  

### 共享参数有什么优点

减少参数数量   

权值共享是指用相同的filter去扫一遍图像，相当于提一次特征，得到一个feature map，为什么用filter是因为假设图像具有局部相关性，即一个pixel和周围离的近的pixels的相关性大，远的pixels相关性小

### BatchNormalization

1.随着训练的进行，网络中的参数也随着梯度下降在不停更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另一方面，参数的变化导致每一层的输入分布会发生改变，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift.  

Internal Covariate Shift:在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的过程  
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-16%2015-22-40.png)  
 Internal Covariate Shift Problem:  
 1）上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低  
 2）网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度  
 对于激活函数梯度饱和问题，有两种解决思路。第一种就是变为非饱和性激活函数，例如ReLU可以在一定程度上解决训练进入梯度饱和区的问题。另一种思路是，我们可以让激活函数的输入分布保持在一个稳定状态来尽可能避免它们陷入梯度饱和区，这也就是Normalization的思路。  
 
2.如何减缓Internal Covariate Shift  
`Batch Normalization`被提出.  

在深度学习中，由于采用full batch的训练方式对内存要求较大，且每一轮训练时间过长；我们一般都会采用对数据做划分，用mini-batch对网络进行训练。因此，Batch Normalization也就在mini-batch的基础上进行计算  
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-16%2015-54-00.png)  
Batch Normalization引入了两个learnable parameters(gamma,beta)  

- **测试阶段如何使用Batch Normalization**
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-16%2016-22-49.png)  

- **Batch Normalization优势**  
(1)BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度,BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。
(2)BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定  
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-16%2016-44-26.png)  
(3)BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题,在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习gamma与beta,又让数据保留更多的原始信息。  
(4)BN具有一定的正则化效果  
在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果  
另外，原作者通过也证明了网络加入BN后，可以丢弃Dropout，模型也同样具有很好的泛化效果。  
可以防止过拟合，Batch Normalization调整了数据的分布，不考虑激活函数，它让每一层的输出归一化到了均值为0方差为1的分布，这保证了梯度的有效性，可以解决反向传播过程中的梯度问题,缓解了Internal Covariate Shift问题  

### 训练过程中模型不收敛，是否说明模型无效?是什么原因导致的模型不收敛?

不一定无效  
1.没有正确初始化权重．解决方法:"lecun"或"xavier(he_initialization)权重初始化几乎在所有情况下表现良好  
2.使用不正确的学习率  
3.没有对数据进行归一化:这是由于不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价  
4.没有对数据进行预处理  
没有对数据做归一化。忘了做数据预处理。忘了使用正则化。Batch Size设的太大。学习率设的不对。最后一层的激活函数用的不对。网络存在坏梯度。比如Relu对负值的梯度为0，反向传播时，0梯度就是不传播。参数初始化错误。网络太深。隐藏层神经元数量错误。

- **为什么用交叉熵做分类的loss?欧式loss,均方差loss可以来做分类loss吗?**


- **训练的时候一般学习率和batchsize都怎么调整?**

1. 请简单介绍一下什么是卷积以及它的原理

2. 池化在卷积神经网络里面有何作用，在引进池化后解决了什么问题？

3. 列举至少三种激活函数，分别阐述他们的优缺点

4. 介绍一下感受野的概念，以及如何计算感受野的大小

5. 什么是过拟合现象，在训练网络的过程中如何防止过拟合现象的发生？如果网络中出现了过拟合，你该怎么应对

6. 什么是欠拟合现象，网络欠拟合你该怎么办？

7. 当网络损失下降的很平稳，突然间损失开始出现剧烈波动时，请分析是什么原因导致的

8. 什么是梯度消失和梯度爆炸，分别阐述解决办法

9. 介绍一下L1、L2正则化的原理和作用

10. 归一化和标准化的方法有哪些？在神经网络中归一化和标准化的作用

11. 简述一下数据增样、图像增强的常用方法

12. 介绍一下经典的五种卷积神经网络（VGG、残差两个是重点，其他的看改进点就好）

13. 在很多卷积神经网络中都会用到1*1卷积核，它的作用是什么？

14. 介绍一下卷积和全连接的区别，为什么在图像领域卷积的效果要比全连接好？

15. 介绍一下目前主流的目标检测算法，并简单做个对比，你在项目中使用的是什么算法，为什么用它而不用其他的？

16. 什么是anchor框，它在检测算法里面有什么作用？

17. 你是按照什么比例来划分你的数据集，训练集、验证集和测试集在网络训练时分别起什么作用？为什么有了测试集后还需要用到验证集？

18. 你有没有关注最新检测框架的论文进展，你对未来的检测框架发展有什么看法？

19. 介绍一下YOLOV3、SSD、Faster-RCNN的思想和实现过程（你和面试官说你擅长用的哪种就介绍哪种）

20. Batch Normalization原理和作用？

21. 如果你的网络不收敛，请分析一下是什么情况导致的

22. 你是依据什么去评估你的网络性能？精确度、召回率或者mAP等

23. 如果让你去改进YOLOV3（Faster-RCNN、SSD），你要从哪方面入手

24. 如果网络不初始化权重，即初始权重为0，那么网络训练出来会是什么效果？

25. 网络的评估标准mAP是什么，它是如何计算的？

26.batch_size大小对模型训练的影响

27.YOLO和SSD以及Faster-RCNN的区别，大概意思就是问单阶段和两阶段的区别，为什么单阶段快，为什么双阶段精确度更高

28.除了以上的算法外，你还了解哪些检测算法

29.你认为目标检测未来要解决的问题有哪些，有哪几个发展方向

30.你在项目中遇到了哪些问题，你是如何解决的？
### 卷积层和池化层有什么区别?

卷积层有参数，池化层没有参数，经过卷积层节点矩阵深度会改变，池化层不会改变节点矩阵的深度，但它可以缩小节点矩阵的大小  

- CNN如何用于文本分类？

- resnet提出的背景和核心理论是？

- 空洞卷积是什么？有什么应用场景？

### RNN

- 简述RNN，LSTM，GRU的区别和联系

- 画出lstm的结构图，写出公式

- RNN的梯度消失问题？如何解决？

- lstm中是否可以用relu作为激活函数？

- lstm各个门分别使用什么激活函数？

- 简述seq2seq模型？

- seq2seq在解码时候有哪些方法？

- Attention机制是什么？



## 机器学习

### 基础

- 样本不均衡如何处理？
- 什么是生成模型什么是判别模型？

### 集成学习

- 集成学习的分类？有什么代表性的模型和方法？
- 如何从偏差和方差的角度解释bagging和boosting的原理？
- GBDT的原理？和Xgboost的区别联系？
- adaboost和gbdt的区别联系？

### 模型 

- 手推LR、Kmeans、SVM
- 简述ridge和lasson的区别和联系
- 树模型如何调参
- 树模型如何剪枝？
- 是否存一定存在参数，使得SVM的训练误差能到0
- 逻辑回归如何处理多分类？
- 决策树有哪些划分指标？区别与联系？
- 简述SVD和PCA的区别和联系？
- 如何使用梯度下降方法进行矩阵分解？
- LDA与PCA的区别与联系？

### 特征工程 

- 常用的特征筛选方法有哪些？
- 文本如何构造特征？
- 类别变量如何构造特征？
- 连续值变量如何构造特征？
- 哪些模型需要对特征进行归一化？
- 什么是组合特征？如何处理高维组合特征？
