### Batch Normalization  
随着训练的进行，网络中的参数也随着梯度下降在不停更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另一方面，参数的变化导致每一层的输入分布会发生改变，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift.  

Internal Covariate Shift:在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的过程  
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-16%2015-22-40.png)  
 Internal Covariate Shift Problem:  
 1）上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低  
 2）网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度  
 对于激活函数梯度饱和问题，有两种解决思路。第一种就是变为非饱和性激活函数，例如ReLU可以在一定程度上解决训练进入梯度饱和区的问题。另一种思路是，我们可以让激活函数的输入分布保持在一个稳定状态来尽可能避免它们陷入梯度饱和区，这也就是Normalization的思路。  
 
### 如何减缓Internal Covariate Shift  
ICS产生的原因是由于参数更新带来的网络中每一层输入值分布的改变，并且随着网络层数的加深而变得更加严重，因此我们可以通过固定每一层网络输入值的分布来对减缓ICS问题。  
(1)Whitening
它是机器学习里面常用的一种规范化数据分布的方法，主要是PCA白化与ZCA白化。白化是对输入数据分布进行变换，进而达到以下两个目的：

使得输入特征分布具有相同的均值与方差。其中PCA白化保证了所有特征分布均值为0，方差为1；而ZCA白化则保证了所有特征分布均值为0，方差相同；去除特征之间的相关性。
通过白化操作，我们可以减缓ICS的问题，进而固定了每一层网络输入分布，加速网络训练过程的收敛。  
但whitening存在两个问题:  
1.计算成本高  
2.由于改变了网络每一层的分布，因而改变了网络层中本身数据的表达能力。底层网络学习到的参数信息会被白化操作丢失掉。  

既然白化计算过程比较复杂，那我们就简化一点，比如我们可以尝试单独对每个特征进行`normalizaiton`就可以了，让每个特征都有均值为0，方差为1的分布就OK。

另一个问题，既然白化操作减弱了网络中每一层输入数据表达能力，那我就再加个线性变换操作，让这些数据再能够尽可能恢复本身的表达能力就好了。`Batch Normalization`被提出.  

在深度学习中，由于采用full batch的训练方式对内存要求较大，且每一轮训练时间过长；我们一般都会采用对数据做划分，用mini-batch对网络进行训练。因此，Batch Normalization也就在mini-batch的基础上进行计算  
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-16%2015-54-00.png)  
Batch Normalization引入了两个learnable parameters(gamma,beta)  

### 测试阶段如何使用Batch Normalization  
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-16%2016-22-49.png)  

### Batch Normalization优势  
**（1）BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度**
BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，**使得后一层网络不必不断去适应底层网络中输入的变化**，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。
**（2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定**  
![](https://github.com/weiweia92/pictures/blob/master/Screenshot%20from%202020-06-16%2016-44-26.png)  
**（3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题**  
在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习r与beta,又让数据保留更多的原始信息。  
**（4）BN具有一定的正则化效果**  
在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。
另外，原作者通过也证明了网络加入BN后，可以丢弃Dropout，模型也同样具有很好的泛化效果。
